{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "46389519",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "torch.manual_seed(13);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3c4227f",
   "metadata": {},
   "source": [
    "## Hierarchy\n",
    "\n",
    "Ontologies can be represented using a tree data structure, where the leaves (nodes at level 0) represent the most specific classes. A mapping can be defined between the nodes at level 0 and any other nodes at level l.\n",
    "\n",
    "One way to define such a mapping is to use a list of length C (where C is the number of finer classes or leaves), consisting of integer numbers. The integer value represents the node at level L, while the corresponding list-index represents the finer class. Multiple levels of hierarchy can be stacked to form a *\"Hierarchy Tensor\"* (an (L,C)-tensor). With this implementation, we can obtain the coarser class at level `l` of a finer class `c` by using `hierarchy[l, c]`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4c795f72",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[-4.0818, -0.2062,  3.1055, -4.8489, -4.8470,  1.0357],\n",
       "         [-2.6820,  3.6329,  4.8588, -3.0249, -4.1699, -0.7465],\n",
       "         [ 4.1487, -0.2010,  0.3479, -2.3053, -2.4698, -1.6100],\n",
       "         [ 3.3667, -3.7112,  4.6935, -0.5049, -0.9694,  3.2024]]),\n",
       " tensor([2, 1, 5, 3]))"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Hierarchy levels (L), Batch size (B), Number of classes (C)\n",
    "L, B, C = 2, 4, 6\n",
    "\n",
    "# outputs of the model (i.e. logits)\n",
    "outputs = torch.rand(B, C) * 10 - 5\n",
    "\n",
    "# targets (i.e ground labels)\n",
    "targets = torch.randint(C, (B,)).long()\n",
    "\n",
    "outputs, targets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81e1ab33",
   "metadata": {},
   "source": [
    "```\n",
    "      _________0_________     ------ level-2\n",
    "     /         |         \\\n",
    "  __0__      __1__        2   ------ level-1\n",
    " /     \\    /  |  \\       |\n",
    "1       4  2   0   5      3   ------ level-0\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "56b02830",
   "metadata": {},
   "outputs": [],
   "source": [
    "hierarchy = torch.Tensor(\n",
    "    [\n",
    "        [0, 1, 2, 3, 4, 5],  # level 0\n",
    "        [1, 0, 1, 2, 0, 1],  # level 1\n",
    "        # Add here other other levels ... \n",
    "        # but do not include the root level.\n",
    "    ]\n",
    ")\n",
    "\n",
    "assert hierarchy.shape == (L, C)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b081707",
   "metadata": {},
   "source": [
    "## Hierarchical Hot Encoding\n",
    "\n",
    "When it comes to losses and metrics, it's helpful to have a multi-hot encoding of the `targets` ((B,)-tensor) at level `l`. This can be achieved using the *\"Encoding Tensor\"* (an (L,C,C)-tensor): `encoding[l, targets]`, which is generated from the Hierarchy Tensor. Note that `encoding[0, targets]` is the same as the usual one-hot encoding.\n",
    "\n",
    "The encoding tensor has a data type of bool, but printing out the integer version can make it easier to understand."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cc122313",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[1, 0, 0, 0, 0, 0],\n",
       "         [0, 1, 0, 0, 0, 0],\n",
       "         [0, 0, 1, 0, 0, 0],\n",
       "         [0, 0, 0, 1, 0, 0],\n",
       "         [0, 0, 0, 0, 1, 0],\n",
       "         [0, 0, 0, 0, 0, 1]],\n",
       "\n",
       "        [[1, 0, 1, 0, 0, 1],\n",
       "         [0, 1, 0, 0, 1, 0],\n",
       "         [1, 0, 1, 0, 0, 1],\n",
       "         [0, 0, 0, 1, 0, 0],\n",
       "         [0, 1, 0, 0, 1, 0],\n",
       "         [1, 0, 1, 0, 0, 1]]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoding = torch.zeros((L, C, C)).bool()\n",
    "\n",
    "for l, level in enumerate(hierarchy):\n",
    "    for i, label1 in enumerate(level):\n",
    "        for j, label2 in enumerate(level):\n",
    "            if label1 == label2:\n",
    "                encoding[l, i, j] = True\n",
    "\n",
    "encoding.long()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e09e76ab",
   "metadata": {},
   "source": [
    "```\n",
    "      _________0_________     ------ level-2\n",
    "     /         |         \\\n",
    "  __0__      __1__        2   ------ level-1\n",
    " /     \\    /  |  \\       |\n",
    "1       4  2   0   5      3   ------ level-0\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "40ea20a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([2, 1, 5, 3]),\n",
       " tensor([[1, 0, 1, 0, 0, 1],\n",
       "         [0, 1, 0, 0, 1, 0],\n",
       "         [1, 0, 1, 0, 0, 1],\n",
       "         [0, 0, 0, 1, 0, 0]]))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "targets, encoding[1, targets].long()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0aa63645",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([2, 1, 5, 3]),\n",
       " tensor([[0, 0, 1, 0, 0, 0],\n",
       "         [0, 1, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 0, 1],\n",
       "         [0, 0, 0, 1, 0, 0]]))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "assert (F.one_hot(targets, C) == encoding[0, targets]).all()\n",
    "\n",
    "targets, encoding[0, targets].long()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6664a984",
   "metadata": {},
   "source": [
    "## Lower Common Anchestor (LCA)\n",
    "The *\"Lower Common Ancestor\"* (LCA) is a function that takes two leaves (c1 and c2) and returns the distance to their common ancestor. In this implementation, the LCA function is represented by a C x C symmetric matrix: `lca[c1, c2]`. The general version of LCA function takes two generic nodes as input.\n",
    "\n",
    "LCA can be used to define hierarchical metrics, such as the \"hierarchical distance of a mistake\" presented in the paper by \\[Ber+20\\]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "277d01ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 2, 1, 2, 2, 1],\n",
       "        [2, 0, 2, 2, 1, 2],\n",
       "        [1, 2, 0, 2, 2, 1],\n",
       "        [2, 2, 2, 0, 2, 2],\n",
       "        [2, 1, 2, 2, 0, 2],\n",
       "        [1, 2, 1, 2, 2, 0]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lca = torch.full((C, C), L)\n",
    "\n",
    "for level in hierarchy:\n",
    "    for row, coarse in zip(lca, level):\n",
    "        for index, value in enumerate(level):\n",
    "            if coarse == value:\n",
    "                row[index] -= 1\n",
    "lca"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10479fde",
   "metadata": {},
   "source": [
    "```\n",
    "          _________0_________     ------ level-2\n",
    "         /         |         \\\n",
    "      __0__      __1__        2   ------ level-1\n",
    "     /     \\    /  |  \\       |\n",
    "    1       4  2   0   5      3   ------ level-0\n",
    " \n",
    "   [2]     [2][0] [1] [1]    [2]  ------ lca(2, .) example\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "638e5de0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([2, 2, 0, 2]), tensor([2, 1, 5, 3]), tensor([0, 2, 1, 2]))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds = outputs.argmax(dim=-1)\n",
    "\n",
    "preds, targets, lca[preds, targets]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2f3a4072",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.6667)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example of lca metric: \"hierarchical distance of a mistake\" [Ber+20]\n",
    "misclassified = targets.numel() - (preds == targets).sum()\n",
    "distance = lca[preds, targets].sum().float()\n",
    "metric = distance / misclassified\n",
    "metric"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
