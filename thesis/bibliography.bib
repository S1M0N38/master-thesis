@article{EfficientnetRTanM2019,
  abstract = {Convolutional Neural Networks (ConvNets) are commonly developed at a fixed resource budget, and then scaled up for better accuracy if more resources are available. In this paper, we systematically study model scaling and identify that carefully balancing network depth, width, and resolution can lead to better performance. Based on this observation, we propose a new scaling method that uniformly scales all dimensions of depth/width/resolution using a simple yet highly effective compound coefficient. We demonstrate the effectiveness of this method on scaling up MobileNets and ResNet.   To go even further, we use neural architecture search to design a new baseline network and scale it up to obtain a family of models, called EfficientNets, which achieve much better accuracy and efficiency than previous ConvNets. In particular, our EfficientNet-B7 achieves state-of-the-art 84.3\% top-1 accuracy on ImageNet, while being 8.4x smaller and 6.1x faster on inference than the best existing ConvNet. Our EfficientNets also transfer well and achieve state-of-the-art accuracy on CIFAR-100 (91.7\%), Flowers (98.8\%), and 3 other transfer learning datasets, with an order of magnitude fewer parameters. Source code is at https://github.com/tensorflow/tpu/tree/master/models/official/efficientnet.},
  archiveprefix = {arXiv},
  author = {Tan, Mingxing and Le, Quoc V.},
  eprint = {1905.11946v5},
  file = {1905.11946v5.pdf},
  month = {5},
  note = {International Conference on Machine Learning, 2019},
  primaryclass = {cs.LG},
  title = {EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks},
  url = {http://arxiv.org/abs/1905.11946v5},
  year = {2019},
}

@article{EnsembleAdversTramer2017,
  abstract = {Adversarial examples are perturbed inputs designed to fool machine learning models. Adversarial training injects such examples into training data to increase robustness. To scale this technique to large datasets, perturbations are crafted using fast single-step methods that maximize a linear approximation of the model's loss. We show that this form of adversarial training converges to a degenerate global minimum, wherein small curvature artifacts near the data points obfuscate a linear approximation of the loss. The model thus learns to generate weak perturbations, rather than defend against strong ones. As a result, we find that adversarial training remains vulnerable to black-box attacks, where we transfer perturbations computed on undefended models, as well as to a powerful novel single-step attack that escapes the non-smooth vicinity of the input data via a small random step. We further introduce Ensemble Adversarial Training, a technique that augments training data with perturbations transferred from other models. On ImageNet, Ensemble Adversarial Training yields models with strong robustness to black-box attacks. In particular, our most robust model won the first round of the NIPS 2017 competition on Defenses against Adversarial Attacks. However, subsequent work found that more elaborate black-box attacks could significantly enhance transferability and reduce the accuracy of our models.},
  archiveprefix = {arXiv},
  author = {Tram\`{e}r, Florian and Kurakin, Alexey and Papernot, Nicolas and Goodfellow, Ian and Boneh, Dan and McDaniel, Patrick},
  eprint = {1705.07204v5},
  file = {1705.07204v5.pdf},
  month = {May},
  primaryclass = {stat.ML},
  title = {Ensemble Adversarial Training: Attacks and Defenses},
  url = {http://arxiv.org/abs/1705.07204v5},
  year = {2017},
}

@article{Efficientnetv2TanM2021,
  abstract = {This paper introduces EfficientNetV2, a new family of convolutional networks that have faster training speed and better parameter efficiency than previous models. To develop this family of models, we use a combination of training-aware neural architecture search and scaling, to jointly optimize training speed and parameter efficiency. The models were searched from the search space enriched with new ops such as Fused-MBConv. Our experiments show that EfficientNetV2 models train much faster than state-of-the-art models while being up to 6.8x smaller.   Our training can be further sped up by progressively increasing the image size during training, but it often causes a drop in accuracy. To compensate for this accuracy drop, we propose to adaptively adjust regularization (e.g., dropout and data augmentation) as well, such that we can achieve both fast training and good accuracy.   With progressive learning, our EfficientNetV2 significantly outperforms previous models on ImageNet and CIFAR/Cars/Flowers datasets. By pretraining on the same ImageNet21k, our EfficientNetV2 achieves 87.3\% top-1 accuracy on ImageNet ILSVRC2012, outperforming the recent ViT by 2.0\% accuracy while training 5x-11x faster using the same computing resources. Code will be available at https://github.com/google/automl/tree/master/efficientnetv2.},
  archiveprefix = {arXiv},
  author = {Tan, Mingxing and Le, Quoc V.},
  eprint = {2104.00298v3},
  file = {2104.00298v3.pdf},
  month = {4},
  note = {International Conference on Machine Learning, 2021},
  primaryclass = {cs.CV},
  title = {EfficientNetV2: Smaller Models and Faster Training},
  url = {http://arxiv.org/abs/2104.00298v3},
  year = {2021},
}

@inproceedings{LabelEmbeddingBengio2010,
  abstract = {Multi-class classification becomes challenging at test time when the number of classes is very large and testing against every possible class can become computationally infeasible. This problem can be alleviated by imposing (or learning) a structure over the set of classes. We propose an algorithm for learning a tree-structure of classifiers which, by optimizing the overall tree loss, provides superior accuracy to existing tree labeling methods. We also propose a method that learns to embed labels in a low dimensional space that is faster than non-embedding approaches and has superior accuracy to existing embedding approaches. Finally we combine the two ideas resulting in the label embedding tree that outperforms alternative methods including One-vs-Rest while being orders of magnitude faster.},
  address = {Red Hook, NY, USA},
  author = {Bengio, Samy and Weston, Jason and Grangier, David},
  booktitle = {Proceedings of the 23rd International Conference on Neural Information Processing Systems - Volume 1},
  location = {Vancouver, British Columbia, Canada},
  pages = {163\textendash{}171},
  publisher = {Curran Associates Inc.},
  series = {NIPS'10},
  title = {Label Embedding Trees for Large Multi-Class Tasks},
  year = {2010},
}

@inproceedings{DesigningNeuraMiller1989,
  abstract = {We present a genetic algorithm method that evolves neural network architectures for specific tasks. Each network architecture is represented as a connection constraint matrix mapped directly into a bit-string genotype. Modified standard genetic operators act on populations of these genotypes to produce network architectures with higher fitness is assessed by training particular network instantiations and recording their final performance error. Three applications of this method to simple network mapping tasks are discussed.},
  address = {San Francisco, CA, USA},
  author = {Miller, Geoffrey F. and Todd, Peter M. and Hegde, Shailesh U.},
  booktitle = {Proceedings of the 3rd International Conference on Genetic Algorithms},
  isbn = {1558600663},
  pages = {379\textendash{}384},
  publisher = {Morgan Kaufmann Publishers Inc.},
  title = {Designing Neural Networks Using Genetic Algorithms},
  year = {1989},
}

@article{ImprovingInterDong2017,
  abstract = {Interpretability of deep neural networks (DNNs) is essential since it enables users to understand the overall strengths and weaknesses of the models, conveys an understanding of how the models will behave in the future, and how to diagnose and correct potential problems. However, it is challenging to reason about what a DNN actually does due to its opaque or black-box nature. To address this issue, we propose a novel technique to improve the interpretability of DNNs by leveraging the rich semantic information embedded in human descriptions. By concentrating on the video captioning task, we first extract a set of semantically meaningful topics from the human descriptions that cover a wide range of visual concepts, and integrate them into the model with an interpretive loss. We then propose a prediction difference maximization algorithm to interpret the learned features of each neuron. Experimental results demonstrate its effectiveness in video captioning using the interpretable features, which can also be transferred to video action recognition. By clearly understanding the learned features, users can easily revise false predictions via a human-in-the-loop procedure.},
  archiveprefix = {arXiv},
  author = {Dong, Yinpeng and Su, Hang and Zhu, Jun and Zhang, Bo},
  eprint = {1703.04096v2},
  file = {1703.04096v2.pdf},
  month = {3},
  primaryclass = {cs.CV},
  title = {Improving Interpretability of Deep Neural Networks with Semantic   Information},
  url = {http://arxiv.org/abs/1703.04096v2},
  year = {2017},
}

@article{ImagenetLargeRussak2014,
  abstract = {The ImageNet Large Scale Visual Recognition Challenge is a benchmark in object category classification and detection on hundreds of object categories and millions of images. The challenge has been run annually from 2010 to present, attracting participation from more than fifty institutions.   This paper describes the creation of this benchmark dataset and the advances in object recognition that have been possible as a result. We discuss the challenges of collecting large-scale ground truth annotation, highlight key breakthroughs in categorical object recognition, provide a detailed analysis of the current state of the field of large-scale image classification and object detection, and compare the state-of-the-art computer vision accuracy with human accuracy. We conclude with lessons learned in the five years of the challenge, and propose future directions and improvements.},
  archiveprefix = {arXiv},
  author = {Russakovsky, Olga and Deng, Jia and Su, Hao and Krause, Jonathan and Satheesh, Sanjeev and Ma, Sean and Huang, Zhiheng and Karpathy, Andrej and Khosla, Aditya and Bernstein, Michael and Berg, Alexander C. and Fei-Fei, Li},
  eprint = {1409.0575v3},
  file = {1409.0575v3.pdf},
  month = {9},
  primaryclass = {cs.CV},
  title = {ImageNet Large Scale Visual Recognition Challenge},
  url = {http://arxiv.org/abs/1409.0575v3},
  year = {2014},
}

@article{MobilenetsEffHoward2017,
  abstract = {We present a class of efficient models called MobileNets for mobile and embedded vision applications. MobileNets are based on a streamlined architecture that uses depth-wise separable convolutions to build light weight deep neural networks. We introduce two simple global hyper-parameters that efficiently trade off between latency and accuracy. These hyper-parameters allow the model builder to choose the right sized model for their application based on the constraints of the problem. We present extensive experiments on resource and accuracy tradeoffs and show strong performance compared to other popular models on ImageNet classification. We then demonstrate the effectiveness of MobileNets across a wide range of applications and use cases including object detection, finegrain classification, face attributes and large scale geo-localization.},
  archiveprefix = {arXiv},
  author = {Howard, Andrew G. and Zhu, Menglong and Chen, Bo and Kalenichenko, Dmitry and Wang, Weijun and Weyand, Tobias and Andreetto, Marco and Adam, Hartwig},
  eprint = {1704.04861v1},
  file = {1704.04861v1.pdf},
  month = {4},
  primaryclass = {cs.CV},
  title = {MobileNets: Efficient Convolutional Neural Networks for Mobile Vision   Applications},
  url = {http://arxiv.org/abs/1704.04861v1},
  year = {2017},
}

@article{TemporalContexShao2020,
  abstract = {The current research focus on Content-Based Video Retrieval requires higher-level video representation describing the long-range semantic dependencies of relevant incidents, events, etc. However, existing methods commonly process the frames of a video as individual images or short clips, making the modeling of long-range semantic dependencies difficult. In this paper, we propose TCA (Temporal Context Aggregation for Video Retrieval), a video representation learning framework that incorporates long-range temporal information between frame-level features using the self-attention mechanism. To train it on video retrieval datasets, we propose a supervised contrastive learning method that performs automatic hard negative mining and utilizes the memory bank mechanism to increase the capacity of negative samples. Extensive experiments are conducted on multiple video retrieval tasks, such as CC\_WEB\_VIDEO, FIVR-200K, and EVVE. The proposed method shows a significant performance advantage (\textasciitilde{}17\% mAP on FIVR-200K) over state-of-the-art methods with video-level features, and deliver competitive results with 22x faster inference time comparing with frame-level features.},
  archiveprefix = {arXiv},
  author = {Shao, Jie and Wen, Xin and Zhao, Bingchen and Xue, Xiangyang},
  eprint = {2008.01334v2},
  file = {2008.01334v2.pdf},
  month = {8},
  primaryclass = {cs.CV},
  title = {Temporal Context Aggregation for Video Retrieval with Contrastive   Learning},
  url = {http://arxiv.org/abs/2008.01334v2},
  year = {2020},
}

@article{NeuralSpeechSLiNa2018,
  abstract = {Although end-to-end neural text-to-speech (TTS) methods (such as Tacotron2) are proposed and achieve state-of-the-art performance, they still suffer from two problems: 1) low efficiency during training and inference; 2) hard to model long dependency using current recurrent neural networks (RNNs). Inspired by the success of Transformer network in neural machine translation (NMT), in this paper, we introduce and adapt the multi-head attention mechanism to replace the RNN structures and also the original attention mechanism in Tacotron2. With the help of multi-head self-attention, the hidden states in the encoder and decoder are constructed in parallel, which improves the training efficiency. Meanwhile, any two inputs at different times are connected directly by self-attention mechanism, which solves the long range dependency problem effectively. Using phoneme sequences as input, our Transformer TTS network generates mel spectrograms, followed by a WaveNet vocoder to output the final audio results. Experiments are conducted to test the efficiency and performance of our new network. For the efficiency, our Transformer TTS network can speed up the training about 4.25 times faster compared with Tacotron2. For the performance, rigorous human tests show that our proposed model achieves state-of-the-art performance (outperforms Tacotron2 with a gap of 0.048) and is very close to human quality (4.39 vs 4.44 in MOS).},
  archiveprefix = {arXiv},
  author = {Li, Naihan and Liu, Shujie and Liu, Yanqing and Zhao, Sheng and Liu, Ming and Zhou, Ming},
  eprint = {1809.08895v3},
  file = {1809.08895v3.pdf},
  month = {9},
  primaryclass = {cs.CL},
  title = {Neural Speech Synthesis with Transformer Network},
  url = {http://arxiv.org/abs/1809.08895v3},
  year = {2018},
}

@article{DistributedRepMikolo2013,
  abstract = {The recently introduced continuous Skip-gram model is an efficient method for learning high-quality distributed vector representations that capture a large number of precise syntactic and semantic word relationships. In this paper we present several extensions that improve both the quality of the vectors and the training speed. By subsampling of the frequent words we obtain significant speedup and also learn more regular word representations. We also describe a simple alternative to the hierarchical softmax called negative sampling. An inherent limitation of word representations is their indifference to word order and their inability to represent idiomatic phrases. For example, the meanings of "Canada" and "Air" cannot be easily combined to obtain "Air Canada". Motivated by this example, we present a simple method for finding phrases in text, and show that learning good vector representations for millions of phrases is possible.},
  archiveprefix = {arXiv},
  author = {Mikolov, Tomas and Sutskever, Ilya and Chen, Kai and Corrado, Greg and Dean, Jeffrey},
  eprint = {1310.4546v1},
  file = {1310.4546v1.pdf},
  month = {10},
  primaryclass = {cs.CL},
  title = {Distributed Representations of Words and Phrases and their   Compositionality},
  url = {http://arxiv.org/abs/1310.4546v1},
  year = {2013},
}

@article{ASurveyOnVisHanK2023,
  author = {Han, Kai and Wang, Yunhe and Chen, Hanting and Chen, Xinghao and Guo, Jianyuan and Liu, Zhenhua and Tang, Yehui and Xiao, An and Xu, Chunjing and Xu, Yixing and Yang, Zhaohui and Zhang, Yiman and Tao, Dacheng},
  doi = {10.1109/tpami.2022.3152247},
  issue = {1},
  journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  month = {1},
  pages = {87--110},
  publisher = {Institute of Electrical and Electronics Engineers (IEEE)},
  title = {A Survey on Vision Transformer},
  url = {http://dx.doi.org/10.1109/tpami.2022.3152247},
  volume = {45},
  year = {2023},
}

@article{AttentionIsAlVaswan2017,
  abstract = {The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.},
  archiveprefix = {arXiv},
  author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N. and Kaiser, Lukasz and Polosukhin, Illia},
  eprint = {1706.03762v7},
  file = {1706.03762v7.pdf},
  month = {6},
  primaryclass = {cs.CL},
  title = {Attention Is All You Need},
  url = {http://arxiv.org/abs/1706.03762v7},
  year = {2017},
}

@article{VideoTransformNeimar2021,
  abstract = {This paper presents VTN, a transformer-based framework for video recognition. Inspired by recent developments in vision transformers, we ditch the standard approach in video action recognition that relies on 3D ConvNets and introduce a method that classifies actions by attending to the entire video sequence information. Our approach is generic and builds on top of any given 2D spatial network. In terms of wall runtime, it trains \textdollar{}16.1\textbackslash{}times\textdollar{} faster and runs \textdollar{}5.1\textbackslash{}times\textdollar{} faster during inference while maintaining competitive accuracy compared to other state-of-the-art methods. It enables whole video analysis, via a single end-to-end pass, while requiring \textdollar{}1.5\textbackslash{}times\textdollar{} fewer GFLOPs. We report competitive results on Kinetics-400 and present an ablation study of VTN properties and the trade-off between accuracy and inference speed. We hope our approach will serve as a new baseline and start a fresh line of research in the video recognition domain. Code and models are available at: https://github.com/bomri/SlowFast/blob/master/projects/vtn/README.md},
  archiveprefix = {arXiv},
  author = {Neimark, Daniel and Bar, Omri and Zohar, Maya and Asselmann, Dotan},
  eprint = {2102.00719v3},
  file = {2102.00719v3.pdf},
  month = {2},
  primaryclass = {cs.CV},
  title = {Video Transformer Network},
  url = {http://arxiv.org/abs/2102.00719v3},
  year = {2021},
}

@article{SoftmaxWithCrPeters2017,
  author = {Petersen, Matt},
  journal = {mattpetersen.github.io},
  title = {Softmax with cross-entropy},
  url = {https://mattpetersen.github.io/softmax-with-cross-entropy},
  year = {2017},
}

@article{TheShapeOfLeVierin2021,
  abstract = {Learning curves provide insight into the dependence of a learner's generalization performance on the training set size. This important tool can be used for model selection, to predict the effect of more training data, and to reduce the computational complexity of model training and hyperparameter tuning. This review recounts the origins of the term, provides a formal definition of the learning curve, and briefly covers basics such as its estimation. Our main contribution is a comprehensive overview of the literature regarding the shape of learning curves. We discuss empirical and theoretical evidence that supports well-behaved curves that often have the shape of a power law or an exponential. We consider the learning curves of Gaussian processes, the complex shapes they can display, and the factors influencing them. We draw specific attention to examples of learning curves that are ill-behaved, showing worse learning performance with more training data. To wrap up, we point out various open problems that warrant deeper empirical and theoretical investigation. All in all, our review underscores that learning curves are surprisingly diverse and no universal model can be identified.},
  archiveprefix = {arXiv},
  author = {Viering, Tom and Loog, Marco},
  eprint = {2103.10948v2},
  file = {2103.10948v2.pdf},
  month = {3},
  primaryclass = {cs.LG},
  title = {The Shape of Learning Curves: a Review},
  url = {http://arxiv.org/abs/2103.10948v2},
  year = {2021},
}

@article{GeneticCnnXieL2017,
  abstract = {The deep Convolutional Neural Network (CNN) is the state-of-the-art solution for large-scale visual recognition. Following basic principles such as increasing the depth and constructing highway connections, researchers have manually designed a lot of fixed network structures and verified their effectiveness.   In this paper, we discuss the possibility of learning deep network structures automatically. Note that the number of possible network structures increases exponentially with the number of layers in the network, which inspires us to adopt the genetic algorithm to efficiently traverse this large search space. We first propose an encoding method to represent each network structure in a fixed-length binary string, and initialize the genetic algorithm by generating a set of randomized individuals. In each generation, we define standard genetic operations, e.g., selection, mutation and crossover, to eliminate weak individuals and then generate more competitive ones. The competitiveness of each individual is defined as its recognition accuracy, which is obtained via training the network from scratch and evaluating it on a validation set. We run the genetic process on two small datasets, i.e., MNIST and CIFAR10, demonstrating its ability to evolve and find high-quality structures which are little studied before. These structures are also transferrable to the large-scale ILSVRC2012 dataset.},
  archiveprefix = {arXiv},
  author = {Xie, Lingxi and Yuille, Alan},
  eprint = {1703.01513v1},
  file = {1703.01513v1.pdf},
  month = {3},
  primaryclass = {cs.CV},
  title = {Genetic CNN},
  url = {http://arxiv.org/abs/1703.01513v1},
  year = {2017},
}

@article{DenselyConnectHuang2016,
  abstract = {Recent work has shown that convolutional networks can be substantially deeper, more accurate, and efficient to train if they contain shorter connections between layers close to the input and those close to the output. In this paper, we embrace this observation and introduce the Dense Convolutional Network (DenseNet), which connects each layer to every other layer in a feed-forward fashion. Whereas traditional convolutional networks with L layers have L connections - one between each layer and its subsequent layer - our network has L(L+1)/2 direct connections. For each layer, the feature-maps of all preceding layers are used as inputs, and its own feature-maps are used as inputs into all subsequent layers. DenseNets have several compelling advantages: they alleviate the vanishing-gradient problem, strengthen feature propagation, encourage feature reuse, and substantially reduce the number of parameters. We evaluate our proposed architecture on four highly competitive object recognition benchmark tasks (CIFAR-10, CIFAR-100, SVHN, and ImageNet). DenseNets obtain significant improvements over the state-of-the-art on most of them, whilst requiring less computation to achieve high performance. Code and pre-trained models are available at https://github.com/liuzhuang13/DenseNet .},
  archiveprefix = {arXiv},
  author = {Huang, Gao and Liu, Zhuang and van der Maaten, Laurens and Weinberger, Kilian Q.},
  eprint = {1608.06993v5},
  file = {1608.06993v5.pdf},
  month = {8},
  primaryclass = {cs.CV},
  title = {Densely Connected Convolutional Networks},
  url = {http://arxiv.org/abs/1608.06993v5},
  year = {2016},
}

@article{DeepRootsImpIoanno2016,
  abstract = {We propose a new method for creating computationally efficient and compact convolutional neural networks (CNNs) using a novel sparse connection structure that resembles a tree root. This allows a significant reduction in computational cost and number of parameters compared to state-of-the-art deep CNNs, without compromising accuracy, by exploiting the sparsity of inter-layer filter dependencies. We validate our approach by using it to train more efficient variants of state-of-the-art CNN architectures, evaluated on the CIFAR10 and ILSVRC datasets. Our results show similar or higher accuracy than the baseline architectures with much less computation, as measured by CPU and GPU timings. For example, for ResNet 50, our model has 40\% fewer parameters, 45\% fewer floating point operations, and is 31\% (12\%) faster on a CPU (GPU). For the deeper ResNet 200 our model has 25\% fewer floating point operations and 44\% fewer parameters, while maintaining state-of-the-art accuracy. For GoogLeNet, our model has 7\% fewer parameters and is 21\% (16\%) faster on a CPU (GPU).},
  archiveprefix = {arXiv},
  author = {Ioannou, Yani and Robertson, Duncan and Cipolla, Roberto and Criminisi, Antonio},
  doi = {10.1109/CVPR.2017.633},
  eprint = {1605.06489v3},
  file = {1605.06489v3.pdf},
  month = {5},
  primaryclass = {cs.NE},
  title = {Deep Roots: Improving CNN Efficiency with Hierarchical Filter Groups},
  url = {http://arxiv.org/abs/1605.06489v3},
  year = {2016},
}

@article{AdversariallyRGoldbl2019,
  abstract = {Knowledge distillation is effective for producing small, high-performance neural networks for classification, but these small networks are vulnerable to adversarial attacks. This paper studies how adversarial robustness transfers from teacher to student during knowledge distillation. We find that a large amount of robustness may be inherited by the student even when distilled on only clean images. Second, we introduce Adversarially Robust Distillation (ARD) for distilling robustness onto student networks. In addition to producing small models with high test accuracy like conventional distillation, ARD also passes the superior robustness of large networks onto the student. In our experiments, we find that ARD student models decisively outperform adversarially trained networks of identical architecture in terms of robust accuracy, surpassing state-of-the-art methods on standard robustness benchmarks. Finally, we adapt recent fast adversarial training methods to ARD for accelerated robust distillation.},
  archiveprefix = {arXiv},
  author = {Goldblum, Micah and Fowl, Liam and Feizi, Soheil and Goldstein, Tom},
  doi = {10.1609/aaai.v34i04.5816},
  eprint = {1905.09747v2},
  file = {1905.09747v2.pdf},
  month = {May},
  primaryclass = {cs.LG},
  title = {Adversarially Robust Distillation},
  url = {http://arxiv.org/abs/1905.09747v2},
  year = {2019},
}

@inproceedings{ScalableRecognNister2006,
  author = {Nister, D. and Stewenius, H.},
  booktitle = {2006 IEEE Computer Society Conference on Computer Vision and Pattern Recognition - Volume 2 (CVPR'06)},
  doi = {10.1109/cvpr.2006.264},
  journal = {2006 IEEE Computer Society Conference on Computer Vision and Pattern Recognition - Volume 2 (CVPR\textbackslash{}textquotesingle 06)},
  publisher = {IEEE},
  title = {Scalable Recognition with a Vocabulary Tree},
  url = {http://dx.doi.org/10.1109/cvpr.2006.264},
  venue = {New York, NY, USA},
  year = {2006},
}

@article{DefensiveDistiCarlin2016,
  abstract = {We show that defensive distillation is not secure: it is no more resistant to targeted misclassification attacks than unprotected neural networks.},
  archiveprefix = {arXiv},
  author = {Carlini, Nicholas and Wagner, David},
  eprint = {1607.04311v1},
  file = {1607.04311v1.pdf},
  month = {Jul},
  primaryclass = {cs.CR},
  title = {Defensive Distillation is Not Robust to Adversarial Examples},
  url = {http://arxiv.org/abs/1607.04311v1},
  year = {2016},
}

@patent{ComputingNumerMikolo2013,
  abstract = {Methods, systems, and apparatus, including computer programs encoded on computer storage media, for computing numeric representations of words. One of the methods includes obtaining a set of training data, wherein the set of training data comprises sequences of words; training a classifier and an embedding function on the set of training data, wherein training the embedding function comprises obtained trained values of the embedding function parameters; processing each word in the vocabulary using the embedding function in accordance with the trained values of the embedding function parameters to generate a respective numerical representation of each word in the vocabulary in the high-dimensional space; and associating each word in the vocabulary with the respective numeric representation of the word in the high-dimensional space.},
  author = {Mikolov, Tomas and Chen, Kai and Corrado, Gregory S. and Dean, Jeffrey A.},
  holder = {GOOGLE INC.},
  month = {03},
  number = {US9037464B1},
  title = {Computing numeric representations of words in a high-dimensional space},
  url = {https://patents.google.com/patent/US9037464B1/en},
  year = {2013},
}

@article{AdversarialDefMustaf2019,
  abstract = {Deep neural networks are vulnerable to adversarial attacks, which can fool them by adding minuscule perturbations to the input images. The robustness of existing defenses suffers greatly under white-box attack settings, where an adversary has full knowledge about the network and can iterate several times to find strong perturbations. We observe that the main reason for the existence of such perturbations is the close proximity of different class samples in the learned feature space. This allows model decisions to be totally changed by adding an imperceptible perturbation in the inputs. To counter this, we propose to class-wise disentangle the intermediate feature representations of deep networks. Specifically, we force the features for each class to lie inside a convex polytope that is maximally separated from the polytopes of other classes. In this manner, the network is forced to learn distinct and distant decision regions for each class. We observe that this simple constraint on the features greatly enhances the robustness of learned models, even against the strongest white-box attacks, without degrading the classification performance on clean images. We report extensive evaluations in both black-box and white-box attack scenarios and show significant gains in comparison to state-of-the art defenses.},
  archiveprefix = {arXiv},
  author = {Mustafa, Aamir and Khan, Salman and Hayat, Munawar and Goecke, Roland and Shen, Jianbing and Shao, Ling},
  eprint = {1904.00887v4},
  file = {1904.00887v4.pdf},
  month = {4},
  primaryclass = {cs.CV},
  title = {Adversarial Defense by Restricting the Hidden Space of Deep Neural   Networks},
  url = {http://arxiv.org/abs/1904.00887v4},
  year = {2019},
}

@article{LabelEmbeddingSunX2017,
  abstract = {We propose a method, called Label Embedding Network, which can learn label representation (label embedding) during the training process of deep networks. With the proposed method, the label embedding is adaptively and automatically learned through back propagation. The original one-hot represented loss function is converted into a new loss function with soft distributions, such that the originally unrelated labels have continuous interactions with each other during the training process. As a result, the trained model can achieve substantially higher accuracy and with faster convergence speed. Experimental results based on competitive tasks demonstrate the effectiveness of the proposed method, and the learned label embedding is reasonable and interpretable. The proposed method achieves comparable or even better results than the state-of-the-art systems. The source code is available at \textbackslash{}url\lbrace{}https://github.com/lancopku/LabelEmb\rbrace{}.},
  archiveprefix = {arXiv},
  author = {Sun, Xu and Wei, Bingzhen and Ren, Xuancheng and Ma, Shuming},
  eprint = {1710.10393v1},
  file = {1710.10393v1.pdf},
  month = {10},
  primaryclass = {cs.LG},
  title = {Label Embedding Network: Learning Label Representation for Soft Training   of Deep Networks},
  url = {http://arxiv.org/abs/1710.10393v1},
  year = {2017},
}

@article{VivitAVideoArnab2021,
  abstract = {We present pure-transformer based models for video classification, drawing upon the recent success of such models in image classification. Our model extracts spatio-temporal tokens from the input video, which are then encoded by a series of transformer layers. In order to handle the long sequences of tokens encountered in video, we propose several, efficient variants of our model which factorise the spatial- and temporal-dimensions of the input. Although transformer-based models are known to only be effective when large training datasets are available, we show how we can effectively regularise the model during training and leverage pretrained image models to be able to train on comparatively small datasets. We conduct thorough ablation studies, and achieve state-of-the-art results on multiple video classification benchmarks including Kinetics 400 and 600, Epic Kitchens, Something-Something v2 and Moments in Time, outperforming prior methods based on deep 3D convolutional networks. To facilitate further research, we release code at https://github.com/google-research/scenic/tree/main/scenic/projects/vivit},
  archiveprefix = {arXiv},
  author = {Arnab, Anurag and Dehghani, Mostafa and Heigold, Georg and Sun, Chen and Lu\v{c}i\'{c}, Mario and Schmid, Cordelia},
  eprint = {2103.15691v2},
  file = {2103.15691v2.pdf},
  month = {3},
  primaryclass = {cs.CV},
  title = {ViViT: A Video Vision Transformer},
  url = {http://arxiv.org/abs/2103.15691v2},
  year = {2021},
}

@article{Yolo9000BetteRedmon2016,
  abstract = {We introduce YOLO9000, a state-of-the-art, real-time object detection system that can detect over 9000 object categories. First we propose various improvements to the YOLO detection method, both novel and drawn from prior work. The improved model, YOLOv2, is state-of-the-art on standard detection tasks like PASCAL VOC and COCO. At 67 FPS, YOLOv2 gets 76.8 mAP on VOC 2007. At 40 FPS, YOLOv2 gets 78.6 mAP, outperforming state-of-the-art methods like Faster RCNN with ResNet and SSD while still running significantly faster. Finally we propose a method to jointly train on object detection and classification. Using this method we train YOLO9000 simultaneously on the COCO detection dataset and the ImageNet classification dataset. Our joint training allows YOLO9000 to predict detections for object classes that don't have labelled detection data. We validate our approach on the ImageNet detection task. YOLO9000 gets 19.7 mAP on the ImageNet detection validation set despite only having detection data for 44 of the 200 classes. On the 156 classes not in COCO, YOLO9000 gets 16.0 mAP. But YOLO can detect more than just 200 classes; it predicts detections for more than 9000 different object categories. And it still runs in real-time.},
  archiveprefix = {arXiv},
  author = {Redmon, Joseph and Farhadi, Ali},
  eprint = {1612.08242v1},
  file = {1612.08242v1.pdf},
  month = {12},
  primaryclass = {cs.CV},
  title = {YOLO9000: Better, Faster, Stronger},
  url = {http://arxiv.org/abs/1612.08242v1},
  year = {2016},
}

@article{DeepResidualLHeKa2015,
  abstract = {Deeper neural networks are more difficult to train. We present a residual learning framework to ease the training of networks that are substantially deeper than those used previously. We explicitly reformulate the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions. We provide comprehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth. On the ImageNet dataset we evaluate residual nets with a depth of up to 152 layers---8x deeper than VGG nets but still having lower complexity. An ensemble of these residual nets achieves 3.57\% error on the ImageNet test set. This result won the 1st place on the ILSVRC 2015 classification task. We also present analysis on CIFAR-10 with 100 and 1000 layers.   The depth of representations is of central importance for many visual recognition tasks. Solely due to our extremely deep representations, we obtain a 28\% relative improvement on the COCO object detection dataset. Deep residual nets are foundations of our submissions to ILSVRC \& COCO 2015 competitions, where we also won the 1st places on the tasks of ImageNet detection, ImageNet localization, COCO detection, and COCO segmentation.},
  archiveprefix = {arXiv},
  author = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
  eprint = {1512.03385v1},
  file = {1512.03385v1.pdf},
  month = {12},
  primaryclass = {cs.CV},
  title = {Deep Residual Learning for Image Recognition},
  url = {http://arxiv.org/abs/1512.03385v1},
  year = {2015},
}

@article{ComplementObjeChen2019,
  abstract = {Learning with a primary objective, such as softmax cross entropy for classification and sequence generation, has been the norm for training deep neural networks for years. Although being a widely-adopted approach, using cross entropy as the primary objective exploits mostly the information from the ground-truth class for maximizing data likelihood, and largely ignores information from the complement (incorrect) classes. We argue that, in addition to the primary objective, training also using a complement objective that leverages information from the complement classes can be effective in improving model performance. This motivates us to study a new training paradigm that maximizes the likelihood of the groundtruth class while neutralizing the probabilities of the complement classes. We conduct extensive experiments on multiple tasks ranging from computer vision to natural language understanding. The experimental results confirm that, compared to the conventional training with just one primary objective, training also with the complement objective further improves the performance of the state-of-the-art models across all tasks. In addition to the accuracy improvement, we also show that models trained with both primary and complement objectives are more robust to single-step adversarial attacks.},
  archiveprefix = {arXiv},
  author = {Chen, Hao-Yun and Wang, Pei-Hsin and Liu, Chun-Hao and Chang, Shih-Chieh and Pan, Jia-Yu and Chen, Yu-Ting and Wei, Wei and Juan, Da-Cheng},
  eprint = {1903.01182v2},
  file = {1903.01182v2.pdf},
  month = {3},
  primaryclass = {cs.LG},
  title = {Complement Objective Training},
  url = {http://arxiv.org/abs/1903.01182v2},
  year = {2019},
}

@article{DeepLearningOBarz2019,
  abstract = {Two things seem to be indisputable in the contemporary deep learning discourse: 1. The categorical cross-entropy loss after softmax activation is the method of choice for classification. 2. Training a CNN classifier from scratch on small datasets does not work well. In contrast to this, we show that the cosine loss function provides significantly better performance than cross-entropy on datasets with only a handful of samples per class. For example, the accuracy achieved on the CUB-200-2011 dataset without pre-training is by 30\% higher than with the cross-entropy loss. Further experiments on other popular datasets confirm our findings. Moreover, we demonstrate that integrating prior knowledge in the form of class hierarchies is straightforward with the cosine loss and improves classification performance further.},
  archiveprefix = {arXiv},
  author = {Barz, Bj\"{o}rn and Denzler, Joachim},
  eprint = {1901.09054v2},
  file = {1901.09054v2.pdf},
  month = {1},
  note = {2020 IEEE Winter Conference on Applications of Computer Vision   (WACV), Snowmass Village, CO, USA, 2020},
  primaryclass = {cs.LG},
  title = {Deep Learning on Small Datasets without Pre-Training using Cosine Loss},
  url = {http://arxiv.org/abs/1901.09054v2},
  year = {2019},
}

@inproceedings{DeviseADeepFrome2013,
  abstract = {Modern visual recognition systems are often limited in their ability to scale to large numbers of object categories. This limitation is in part due to the increasing difficulty of acquiring sufficient training data in the form of labeled images as the number of object categories grows. One remedy is to leverage data from other sources - such as text data - both to train visual models and to constrain their predictions. In this paper we present a new deep visual-semantic embedding model trained to identify visual objects using both labeled image data as well as semantic information gleaned from unannotated text. We demonstrate that this model matches state-of-the-art performance on the 1000-class ImageNet object recognition challenge while making more semantically reasonable errors, and also show that the semantic information can be exploited to make predictions about tens of thousands of image labels not observed during training. Semantic knowledge improves such zero-shot predictions achieving hit rates of up to 18\% across thousands of novel labels never seen by the visual model.},
  address = {Red Hook, NY, USA},
  author = {Frome, Andrea and Corrado, Greg S. and Shlens, Jonathon and Bengio, Samy and Dean, Jeffrey and Ranzato, Marc'Aurelio and Mikolov, Tomas},
  booktitle = {Proceedings of the 26th International Conference on Neural Information Processing Systems - Volume 2},
  location = {Lake Tahoe, Nevada},
  pages = {2121\textendash{}2129},
  publisher = {Curran Associates Inc.},
  series = {NIPS'13},
  title = {DeViSE: A Deep Visual-Semantic Embedding Model},
  year = {2013},
}

@article{MnasnetPlatfoTanM2018,
  abstract = {Designing convolutional neural networks (CNN) for mobile devices is challenging because mobile models need to be small and fast, yet still accurate. Although significant efforts have been dedicated to design and improve mobile CNNs on all dimensions, it is very difficult to manually balance these trade-offs when there are so many architectural possibilities to consider. In this paper, we propose an automated mobile neural architecture search (MNAS) approach, which explicitly incorporate model latency into the main objective so that the search can identify a model that achieves a good trade-off between accuracy and latency. Unlike previous work, where latency is considered via another, often inaccurate proxy (e.g., FLOPS), our approach directly measures real-world inference latency by executing the model on mobile phones. To further strike the right balance between flexibility and search space size, we propose a novel factorized hierarchical search space that encourages layer diversity throughout the network. Experimental results show that our approach consistently outperforms state-of-the-art mobile CNN models across multiple vision tasks. On the ImageNet classification task, our MnasNet achieves 75.2\% top-1 accuracy with 78ms latency on a Pixel phone, which is 1.8x faster than MobileNetV2 [29] with 0.5\% higher accuracy and 2.3x faster than NASNet [36] with 1.2\% higher accuracy. Our MnasNet also achieves better mAP quality than MobileNets for COCO object detection. Code is at https://github.com/tensorflow/tpu/tree/master/models/official/mnasnet},
  archiveprefix = {arXiv},
  author = {Tan, Mingxing and Chen, Bo and Pang, Ruoming and Vasudevan, Vijay and Sandler, Mark and Howard, Andrew and Le, Quoc V.},
  eprint = {1807.11626v3},
  file = {1807.11626v3.pdf},
  month = {7},
  note = {CVPR 2019},
  primaryclass = {cs.CV},
  title = {MnasNet: Platform-Aware Neural Architecture Search for Mobile},
  url = {http://arxiv.org/abs/1807.11626v3},
  year = {2018},
}

@article{DualPathNetwoChen2017,
  abstract = {In this work, we present a simple, highly efficient and modularized Dual Path Network (DPN) for image classification which presents a new topology of connection paths internally. By revealing the equivalence of the state-of-the-art Residual Network (ResNet) and Densely Convolutional Network (DenseNet) within the HORNN framework, we find that ResNet enables feature re-usage while DenseNet enables new features exploration which are both important for learning good representations. To enjoy the benefits from both path topologies, our proposed Dual Path Network shares common features while maintaining the flexibility to explore new features through dual path architectures. Extensive experiments on three benchmark datasets, ImagNet-1k, Places365 and PASCAL VOC, clearly demonstrate superior performance of the proposed DPN over state-of-the-arts. In particular, on the ImagNet-1k dataset, a shallow DPN surpasses the best ResNeXt-101(64x4d) with 26\% smaller model size, 25\% less computational cost and 8\% lower memory consumption, and a deeper DPN (DPN-131) further pushes the state-of-the-art single model performance with about 2 times faster training speed. Experiments on the Places365 large-scale scene dataset, PASCAL VOC detection dataset, and PASCAL VOC segmentation dataset also demonstrate its consistently better performance than DenseNet, ResNet and the latest ResNeXt model over various applications.},
  archiveprefix = {arXiv},
  author = {Chen, Yunpeng and Li, Jianan and Xiao, Huaxin and Jin, Xiaojie and Yan, Shuicheng and Feng, Jiashi},
  eprint = {1707.01629v2},
  file = {1707.01629v2.pdf},
  month = {7},
  primaryclass = {cs.CV},
  title = {Dual Path Networks},
  url = {http://arxiv.org/abs/1707.01629v2},
  year = {2017},
}

@article{IntriguingPropSzeged2013,
  abstract = {Deep neural networks are highly expressive models that have recently achieved state of the art performance on speech and visual recognition tasks. While their expressiveness is the reason they succeed, it also causes them to learn uninterpretable solutions that could have counter-intuitive properties. In this paper we report two such properties.   First, we find that there is no distinction between individual high level units and random linear combinations of high level units, according to various methods of unit analysis. It suggests that it is the space, rather than the individual units, that contains of the semantic information in the high layers of neural networks.   Second, we find that deep neural networks learn input-output mappings that are fairly discontinuous to a significant extend. We can cause the network to misclassify an image by applying a certain imperceptible perturbation, which is found by maximizing the network's prediction error. In addition, the specific nature of these perturbations is not a random artifact of learning: the same perturbation can cause a different network, that was trained on a different subset of the dataset, to misclassify the same input.},
  archiveprefix = {arXiv},
  author = {Szegedy, Christian and Zaremba, Wojciech and Sutskever, Ilya and Bruna, Joan and Erhan, Dumitru and Goodfellow, Ian and Fergus, Rob},
  eprint = {1312.6199v4},
  file = {1312.6199v4.pdf},
  month = {Dec},
  primaryclass = {cs.CV},
  title = {Intriguing properties of neural networks},
  url = {http://arxiv.org/abs/1312.6199v4},
  year = {2013},
}

@inproceedings{LearningToMakWuHu2016,
  abstract = {We propose a visual food recognition framework that integrates the inherent semantic relationships among fine-grained classes. Our method learns semantics-aware features by formulating a multi-task loss function on top of a convolutional neural network (CNN) architecture. It then refines the CNN predictions using a random walk based smoothing procedure, which further exploits the rich semantic information. We evaluate our algorithm on a large "food-in-the-wild" benchmark, as well as a challenging dataset of restaurant food dishes with very few training images. The proposed method achieves higher classification accuracy than a baseline which directly fine-tunes a deep learning network on the target dataset. Furthermore, we analyze the consistency of the learned model with the inherent semantic relationships among food categories. Results show that the proposed approach provides more semantically meaningful results than the baseline method, even in cases of mispredictions.},
  address = {New York, NY, USA},
  author = {Wu, Hui and Merler, Michele and Uceda-Sosa, Rosario and Smith, John R.},
  booktitle = {Proceedings of the 24th ACM International Conference on Multimedia},
  doi = {10.1145/2964284.2967205},
  isbn = {9781450336031},
  keywords = {food recognition, multi-task learning},
  location = {Amsterdam, The Netherlands},
  pages = {172\textendash{}176},
  publisher = {Association for Computing Machinery},
  series = {MM '16},
  title = {Learning to Make Better Mistakes: Semantics-Aware Visual Food Recognition},
  url = {https://doi.org/10.1145/2964284.2967205},
  year = {2016},
}

@article{SqueezeAndExcHuJi2017,
  abstract = {The central building block of convolutional neural networks (CNNs) is the convolution operator, which enables networks to construct informative features by fusing both spatial and channel-wise information within local receptive fields at each layer. A broad range of prior research has investigated the spatial component of this relationship, seeking to strengthen the representational power of a CNN by enhancing the quality of spatial encodings throughout its feature hierarchy. In this work, we focus instead on the channel relationship and propose a novel architectural unit, which we term the "Squeeze-and-Excitation" (SE) block, that adaptively recalibrates channel-wise feature responses by explicitly modelling interdependencies between channels. We show that these blocks can be stacked together to form SENet architectures that generalise extremely effectively across different datasets. We further demonstrate that SE blocks bring significant improvements in performance for existing state-of-the-art CNNs at slight additional computational cost. Squeeze-and-Excitation Networks formed the foundation of our ILSVRC 2017 classification submission which won first place and reduced the top-5 error to 2.251\%, surpassing the winning entry of 2016 by a relative improvement of \textasciitilde{}25\%. Models and code are available at https://github.com/hujie-frank/SENet.},
  archiveprefix = {arXiv},
  author = {Hu, Jie and Shen, Li and Albanie, Samuel and Sun, Gang and Wu, Enhua},
  eprint = {1709.01507v4},
  file = {1709.01507v4.pdf},
  month = {9},
  primaryclass = {cs.CV},
  title = {Squeeze-and-Excitation Networks},
  url = {http://arxiv.org/abs/1709.01507v4},
  year = {2017},
}

@article{TowardsDeepLeMadry2017,
  abstract = {Recent work has demonstrated that deep neural networks are vulnerable to adversarial examples---inputs that are almost indistinguishable from natural data and yet classified incorrectly by the network. In fact, some of the latest findings suggest that the existence of adversarial attacks may be an inherent weakness of deep learning models. To address this problem, we study the adversarial robustness of neural networks through the lens of robust optimization. This approach provides us with a broad and unifying view on much of the prior work on this topic. Its principled nature also enables us to identify methods for both training and attacking neural networks that are reliable and, in a certain sense, universal. In particular, they specify a concrete security guarantee that would protect against any adversary. These methods let us train networks with significantly improved resistance to a wide range of adversarial attacks. They also suggest the notion of security against a first-order adversary as a natural and broad security guarantee. We believe that robustness against such well-defined classes of adversaries is an important stepping stone towards fully resistant deep learning models. Code and pre-trained models are available at https://github.com/MadryLab/mnist\_challenge and https://github.com/MadryLab/cifar10\_challenge.},
  archiveprefix = {arXiv},
  author = {Madry, Aleksander and Makelov, Aleksandar and Schmidt, Ludwig and Tsipras, Dimitris and Vladu, Adrian},
  eprint = {1706.06083v4},
  file = {1706.06083v4.pdf},
  month = {Jun},
  primaryclass = {stat.ML},
  title = {Towards Deep Learning Models Resistant to Adversarial Attacks},
  url = {http://arxiv.org/abs/1706.06083v4},
  year = {2017},
}

@article{BeyondOneHotPerott2023,
  abstract = {Images are loaded with semantic information that pertains to real-world ontologies: dog breeds share mammalian similarities, food pictures are often depicted in domestic environments, and so on. However, when training machine learning models for image classification, the relative similarities amongst object classes are commonly paired with one-hot-encoded labels. According to this logic, if an image is labelled as 'spoon', then 'tea-spoon' and 'shark' are equally wrong in terms of training loss. To overcome this limitation, we explore the integration of additional goals that reflect ontological and semantic knowledge, improving model interpretability and trustworthiness. We suggest a generic approach that allows to derive an additional loss term starting from any kind of semantic information about the classification label. First, we show how to apply our approach to ontologies and word embeddings, and discuss how the resulting information can drive a supervised learning process. Second, we use our semantically enriched loss to train image classifiers, and analyse the trade-offs between accuracy, mistake severity, and learned internal representations. Finally, we discuss how this approach can be further exploited in terms of explainability and adversarial robustness. Code repository: https://github.com/S1M0N38/semantic-encodings},
  archiveprefix = {arXiv},
  author = {Perotti, Alan and Bertolotto, Simone and Pastor, Eliana and Panisson, Andr\'{e}},
  eprint = {2308.00607v1},
  file = {2308.00607v1.pdf},
  month = {8},
  primaryclass = {cs.CV},
  title = {Beyond One-Hot-Encoding: Injecting Semantics to Drive Image Classifiers},
  url = {http://arxiv.org/abs/2308.00607v1},
  year = {2023},
}

@article{BeyondWordEmbIncitt2023,
  author = {Incitti, Francesca and Urli, Federico and Snidaro, Lauro},
  doi = {10.1016/j.inffus.2022.08.024},
  journal = {Information Fusion},
  language = {en},
  month = {1},
  pages = {418--436},
  publisher = {Elsevier BV},
  title = {Beyond word embeddings: A survey},
  url = {http://dx.doi.org/10.1016/j.inffus.2022.08.024},
  volume = {89},
  year = {2023},
}

@inproceedings{AnIntroductionSoysal2020,
  author = {Soysal, Omurhan A. and Serdar Guzel, Mehmet},
  booktitle = {2020 International Congress on Human-Computer Interaction, Optimization and Robotic Applications (HORA)},
  doi = {10.1109/hora49412.2020.9152859},
  journal = {2020 International Congress on Human-Computer Interaction, Optimization and Robotic Applications (HORA)},
  month = {6},
  publisher = {IEEE},
  title = {An Introduction to Zero-Shot Learning: An Essential Review},
  url = {http://dx.doi.org/10.1109/hora49412.2020.9152859},
  venue = {Ankara, Turkey},
  year = {2020},
}

@inproceedings{LookAndThinkCaoC2015,
  author = {Cao, Chunshui and Liu, Xianming and Yang, Yi and Yu, Yinan and Wang, Jiang and Wang, Zilei and Huang, Yongzhen and Wang, Liang and Huang, Chang and Xu, Wei and Ramanan, Deva and Huang, Thomas S.},
  booktitle = {2015 IEEE International Conference on Computer Vision (ICCV)},
  doi = {10.1109/iccv.2015.338},
  journal = {2015 IEEE International Conference on Computer Vision (ICCV)},
  month = {12},
  publisher = {IEEE},
  title = {Look and Think Twice: Capturing Top-Down Visual Attention with Feedback Convolutional Neural Networks},
  url = {http://dx.doi.org/10.1109/iccv.2015.338},
  venue = {Santiago, Chile},
  year = {2015},
}

@inproceedings{SemanticHierarMarsza2007,
  author = {Marszalek, Marcin and Schmid, Cordelia},
  booktitle = {2007 IEEE Conference on Computer Vision and Pattern Recognition},
  doi = {10.1109/cvpr.2007.383272},
  journal = {2007 IEEE Conference on Computer Vision and Pattern Recognition},
  month = {6},
  publisher = {IEEE},
  title = {Semantic Hierarchies for Visual Object Recognition},
  url = {http://dx.doi.org/10.1109/cvpr.2007.383272},
  venue = {Minneapolis, MN, USA},
  year = {2007},
}

@article{AdversarialMacKuraki2016,
  abstract = {Adversarial examples are malicious inputs designed to fool machine learning models. They often transfer from one model to another, allowing attackers to mount black box attacks without knowledge of the target model's parameters. Adversarial training is the process of explicitly training a model on adversarial examples, in order to make it more robust to attack or to reduce its test error on clean inputs. So far, adversarial training has primarily been applied to small problems. In this research, we apply adversarial training to ImageNet. Our contributions include: (1) recommendations for how to succesfully scale adversarial training to large models and datasets, (2) the observation that adversarial training confers robustness to single-step attack methods, (3) the finding that multi-step attack methods are somewhat less transferable than single-step attack methods, so single-step attacks are the best for mounting black-box attacks, and (4) resolution of a "label leaking" effect that causes adversarially trained models to perform better on adversarial examples than on clean examples, because the adversarial example construction process uses the true label and the model can learn to exploit regularities in the construction process.},
  archiveprefix = {arXiv},
  author = {Kurakin, Alexey and Goodfellow, Ian and Bengio, Samy},
  eprint = {1611.01236v2},
  file = {1611.01236v2.pdf},
  month = {Nov},
  primaryclass = {cs.CV},
  title = {Adversarial Machine Learning at Scale},
  url = {http://arxiv.org/abs/1611.01236v2},
  year = {2016},
}

@article{LargeScaleEvoReal2017,
  abstract = {Neural networks have proven effective at solving difficult problems but designing their architectures can be challenging, even for image classification problems alone. Our goal is to minimize human participation, so we employ evolutionary algorithms to discover such networks automatically. Despite significant computational requirements, we show that it is now possible to evolve models with accuracies within the range of those published in the last year. Specifically, we employ simple evolutionary techniques at unprecedented scales to discover models for the CIFAR-10 and CIFAR-100 datasets, starting from trivial initial conditions and reaching accuracies of 94.6\% (95.6\% for ensemble) and 77.0\%, respectively. To do this, we use novel and intuitive mutation operators that navigate large search spaces; we stress that no human participation is required once evolution starts and that the output is a fully-trained model. Throughout this work, we place special emphasis on the repeatability of results, the variability in the outcomes and the computational requirements.},
  archiveprefix = {arXiv},
  author = {Real, Esteban and Moore, Sherry and Selle, Andrew and Saxena, Saurabh and Suematsu, Yutaka Leon and Tan, Jie and Le, Quoc and Kurakin, Alex},
  eprint = {1703.01041v2},
  file = {1703.01041v2.pdf},
  month = {3},
  primaryclass = {cs.NE},
  title = {Large-Scale Evolution of Image Classifiers},
  url = {http://arxiv.org/abs/1703.01041v2},
  year = {2017},
}

@article{MakingBetterMBertin2019,
  abstract = {Deep neural networks have improved image classification dramatically over the past decade, but have done so by focusing on performance measures that treat all classes other than the ground truth as equally wrong. This has led to a situation in which mistakes are less likely to be made than before, but are equally likely to be absurd or catastrophic when they do occur. Past works have recognised and tried to address this issue of mistake severity, often by using graph distances in class hierarchies, but this has largely been neglected since the advent of the current deep learning era in computer vision. In this paper, we aim to renew interest in this problem by reviewing past approaches and proposing two simple modifications of the cross-entropy loss which outperform the prior art under several metrics on two large datasets with complex class hierarchies: tieredImageNet and iNaturalist'19.},
  archiveprefix = {arXiv},
  author = {Bertinetto, Luca and Mueller, Romain and Tertikas, Konstantinos and Samangooei, Sina and Lord, Nicholas A.},
  eprint = {1912.09393v2},
  file = {1912.09393v2.pdf},
  month = {12},
  primaryclass = {cs.CV},
  title = {Making Better Mistakes: Leveraging Class Hierarchies with Deep Networks},
  url = {http://arxiv.org/abs/1912.09393v2},
  year = {2019},
}

@article{DefenseAgainstLiao2017,
  abstract = {Neural networks are vulnerable to adversarial examples, which poses a threat to their application in security sensitive systems. We propose high-level representation guided denoiser (HGD) as a defense for image classification. Standard denoiser suffers from the error amplification effect, in which small residual adversarial noise is progressively amplified and leads to wrong classifications. HGD overcomes this problem by using a loss function defined as the difference between the target model's outputs activated by the clean image and denoised image. Compared with ensemble adversarial training which is the state-of-the-art defending method on large images, HGD has three advantages. First, with HGD as a defense, the target model is more robust to either white-box or black-box adversarial attacks. Second, HGD can be trained on a small subset of the images and generalizes well to other images and unseen classes. Third, HGD can be transferred to defend models other than the one guiding it. In NIPS competition on defense against adversarial attacks, our HGD solution won the first place and outperformed other models by a large margin.},
  archiveprefix = {arXiv},
  author = {Liao, Fangzhou and Liang, Ming and Dong, Yinpeng and Pang, Tianyu and Hu, Xiaolin and Zhu, Jun},
  eprint = {1712.02976v2},
  file = {1712.02976v2.pdf},
  month = {Dec},
  note = {CVPR 2018},
  primaryclass = {cs.CV},
  title = {Defense against Adversarial Attacks Using High-Level Representation   Guided Denoiser},
  url = {http://arxiv.org/abs/1712.02976v2},
  year = {2017},
}

@article{ASurveyOnMulYinS2023,
  abstract = {Multimodal Large Language Model (MLLM) recently has been a new rising research hotspot, which uses powerful Large Language Models (LLMs) as a brain to perform multimodal tasks. The surprising emergent capabilities of MLLM, such as writing stories based on images and OCR-free math reasoning, are rare in traditional methods, suggesting a potential path to artificial general intelligence. In this paper, we aim to trace and summarize the recent progress of MLLM. First of all, we present the formulation of MLLM and delineate its related concepts. Then, we discuss the key techniques and applications, including Multimodal Instruction Tuning (M-IT), Multimodal In-Context Learning (M-ICL), Multimodal Chain of Thought (M-CoT), and LLM-Aided Visual Reasoning (LAVR). Finally, we discuss existing challenges and point out promising research directions. In light of the fact that the era of MLLM has only just begun, we will keep updating this survey and hope it can inspire more research. An associated GitHub link collecting the latest papers is available at https://github.com/BradyFU/Awesome-Multimodal-Large-Language-Models.},
  archiveprefix = {arXiv},
  author = {Yin, Shukang and Fu, Chaoyou and Zhao, Sirui and Li, Ke and Sun, Xing and Xu, Tong and Chen, Enhong},
  eprint = {2306.13549v1},
  file = {2306.13549v1.pdf},
  month = {6},
  primaryclass = {cs.CV},
  title = {A Survey on Multimodal Large Language Models},
  url = {http://arxiv.org/abs/2306.13549v1},
  year = {2023},
}

@article{YourFlamingoChang2020,
  abstract = {Whether what you see in Figure 1 is a "flamingo" or a "bird", is the question we ask in this paper. While fine-grained visual classification (FGVC) strives to arrive at the former, for the majority of us non-experts just "bird" would probably suffice. The real question is therefore -- how can we tailor for different fine-grained definitions under divergent levels of expertise. For that, we re-envisage the traditional setting of FGVC, from single-label classification, to that of top-down traversal of a pre-defined coarse-to-fine label hierarchy -- so that our answer becomes "bird"-->"Phoenicopteriformes"-->"Phoenicopteridae"-->"flamingo". To approach this new problem, we first conduct a comprehensive human study where we confirm that most participants prefer multi-granularity labels, regardless whether they consider themselves experts. We then discover the key intuition that: coarse-level label prediction exacerbates fine-grained feature learning, yet fine-level feature betters the learning of coarse-level classifier. This discovery enables us to design a very simple albeit surprisingly effective solution to our new problem, where we (i) leverage level-specific classification heads to disentangle coarse-level features with fine-grained ones, and (ii) allow finer-grained features to participate in coarser-grained label predictions, which in turn helps with better disentanglement. Experiments show that our method achieves superior performance in the new FGVC setting, and performs better than state-of-the-art on traditional single-label FGVC problem as well. Thanks to its simplicity, our method can be easily implemented on top of any existing FGVC frameworks and is parameter-free.},
  archiveprefix = {arXiv},
  author = {Chang, Dongliang and Pang, Kaiyue and Zheng, Yixiao and Ma, Zhanyu and Song, Yi-Zhe and Guo, Jun},
  eprint = {2011.09040v3},
  file = {2011.09040v3.pdf},
  month = {11},
  primaryclass = {cs.CV},
  title = {Your "Flamingo" is My "Bird": Fine-Grained, or Not},
  url = {http://arxiv.org/abs/2011.09040v3},
  year = {2020},
}

@article{YouOnlyLookORedmon2015,
  abstract = {We present YOLO, a new approach to object detection. Prior work on object detection repurposes classifiers to perform detection. Instead, we frame object detection as a regression problem to spatially separated bounding boxes and associated class probabilities. A single neural network predicts bounding boxes and class probabilities directly from full images in one evaluation. Since the whole detection pipeline is a single network, it can be optimized end-to-end directly on detection performance.   Our unified architecture is extremely fast. Our base YOLO model processes images in real-time at 45 frames per second. A smaller version of the network, Fast YOLO, processes an astounding 155 frames per second while still achieving double the mAP of other real-time detectors. Compared to state-of-the-art detection systems, YOLO makes more localization errors but is far less likely to predict false detections where nothing exists. Finally, YOLO learns very general representations of objects. It outperforms all other detection methods, including DPM and R-CNN, by a wide margin when generalizing from natural images to artwork on both the Picasso Dataset and the People-Art Dataset.},
  archiveprefix = {arXiv},
  author = {Redmon, Joseph and Divvala, Santosh and Girshick, Ross and Farhadi, Ali},
  eprint = {1506.02640v5},
  file = {1506.02640v5.pdf},
  month = {6},
  primaryclass = {cs.CV},
  title = {You Only Look Once: Unified, Real-Time Object Detection},
  url = {http://arxiv.org/abs/1506.02640v5},
  year = {2015},
}

@article{ScaCnnSpatiaChen2016,
  abstract = {Visual attention has been successfully applied in structural prediction tasks such as visual captioning and question answering. Existing visual attention models are generally spatial, i.e., the attention is modeled as spatial probabilities that re-weight the last conv-layer feature map of a CNN encoding an input image. However, we argue that such spatial attention does not necessarily conform to the attention mechanism --- a dynamic feature extractor that combines contextual fixations over time, as CNN features are naturally spatial, channel-wise and multi-layer. In this paper, we introduce a novel convolutional neural network dubbed SCA-CNN that incorporates Spatial and Channel-wise Attentions in a CNN. In the task of image captioning, SCA-CNN dynamically modulates the sentence generation context in multi-layer feature maps, encoding where (i.e., attentive spatial locations at multiple layers) and what (i.e., attentive channels) the visual attention is. We evaluate the proposed SCA-CNN architecture on three benchmark image captioning datasets: Flickr8K, Flickr30K, and MSCOCO. It is consistently observed that SCA-CNN significantly outperforms state-of-the-art visual attention-based image captioning methods.},
  archiveprefix = {arXiv},
  author = {Chen, Long and Zhang, Hanwang and Xiao, Jun and Nie, Liqiang and Shao, Jian and Liu, Wei and Chua, Tat-Seng},
  eprint = {1611.05594v2},
  file = {1611.05594v2.pdf},
  month = {11},
  primaryclass = {cs.CV},
  title = {SCA-CNN: Spatial and Channel-wise Attention in Convolutional Networks   for Image Captioning},
  url = {http://arxiv.org/abs/1611.05594v2},
  year = {2016},
}

@article{EndToEndObjeCarion2020,
  abstract = {We present a new method that views object detection as a direct set prediction problem. Our approach streamlines the detection pipeline, effectively removing the need for many hand-designed components like a non-maximum suppression procedure or anchor generation that explicitly encode our prior knowledge about the task. The main ingredients of the new framework, called DEtection TRansformer or DETR, are a set-based global loss that forces unique predictions via bipartite matching, and a transformer encoder-decoder architecture. Given a fixed small set of learned object queries, DETR reasons about the relations of the objects and the global image context to directly output the final set of predictions in parallel. The new model is conceptually simple and does not require a specialized library, unlike many other modern detectors. DETR demonstrates accuracy and run-time performance on par with the well-established and highly-optimized Faster RCNN baseline on the challenging COCO object detection dataset. Moreover, DETR can be easily generalized to produce panoptic segmentation in a unified manner. We show that it significantly outperforms competitive baselines. Training code and pretrained models are available at https://github.com/facebookresearch/detr.},
  archiveprefix = {arXiv},
  author = {Carion, Nicolas and Massa, Francisco and Synnaeve, Gabriel and Usunier, Nicolas and Kirillov, Alexander and Zagoruyko, Sergey},
  eprint = {2005.12872v3},
  file = {2005.12872v3.pdf},
  month = {5},
  primaryclass = {cs.CV},
  title = {End-to-End Object Detection with Transformers},
  url = {http://arxiv.org/abs/2005.12872v3},
  year = {2020},
}

@inproceedings{EvaluatingKnowRohrba2011,
  abstract = {While knowledge transfer (KT) between object classes has been accepted as a promising route towards scalable recognition, most experimental KT studies are surprisingly limited in the number of object classes considered. To support claims of KT w.r.t. scalability we thus advocate to evaluate KT in a large-scale setting. To this end, we provide an extensive evaluation of three popular approaches to KT on a recently proposed large-scale data set, the ImageNet Large Scale Visual Recognition Competition 2010 data set. In a first setting they are directly compared to one-vs-all classification often neglected in KT papers and in a second setting we evaluate their ability to enable zero-shot learning. While none of the KT methods can improve over one-vs-all classification they prove valuable for zero-shot learning, especially hierarchical and direct similarity based KT. We also propose and describe several extensions of the evaluated approaches that are necessary for this large-scale study.},
  author = {Rohrbach, Marcus and Stark, Michael and Schiele, Bernt},
  booktitle = {2011 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
  doi = {10.1109/cvpr.2011.5995627},
  journal = {CVPR 2011},
  month = {6},
  publisher = {IEEE},
  title = {Evaluating knowledge transfer and zero-shot learning in a large-scale setting},
  url = {http://dx.doi.org/10.1109/cvpr.2011.5995627},
  venue = {Colorado Springs, CO, USA},
  year = {2011},
}

@article{HierarchyBasedBarz2018,
  abstract = {Deep neural networks trained for classification have been found to learn powerful image representations, which are also often used for other tasks such as comparing images w.r.t. their visual similarity. However, visual similarity does not imply semantic similarity. In order to learn semantically discriminative features, we propose to map images onto class embeddings whose pair-wise dot products correspond to a measure of semantic similarity between classes. Such an embedding does not only improve image retrieval results, but could also facilitate integrating semantics for other tasks, e.g., novelty detection or few-shot learning. We introduce a deterministic algorithm for computing the class centroids directly based on prior world-knowledge encoded in a hierarchy of classes such as WordNet. Experiments on CIFAR-100, NABirds, and ImageNet show that our learned semantic image embeddings improve the semantic consistency of image retrieval results by a large margin.},
  archiveprefix = {arXiv},
  author = {Barz, Bj\"{o}rn and Denzler, Joachim},
  doi = {10.1109/WACV.2019.00073},
  eprint = {1809.09924v4},
  file = {1809.09924v4.pdf},
  month = {9},
  note = {2019 IEEE Winter Conference on Applications of Computer Vision   (WACV), Waikoloa Village, HI, USA, 2019, pp. 638-647},
  primaryclass = {cs.CV},
  title = {Hierarchy-based Image Embeddings for Semantic Image Retrieval},
  url = {http://arxiv.org/abs/1809.09924v4},
  year = {2018},
}

@article{AStudyOfTheDziuga2016,
  abstract = {Neural network image classifiers are known to be vulnerable to adversarial images, i.e., natural images which have been modified by an adversarial perturbation specifically designed to be imperceptible to humans yet fool the classifier. Not only can adversarial images be generated easily, but these images will often be adversarial for networks trained on disjoint subsets of data or with different architectures. Adversarial images represent a potential security risk as well as a serious machine learning challenge---it is clear that vulnerable neural networks perceive images very differently from humans. Noting that virtually every image classification data set is composed of JPG images, we evaluate the effect of JPG compression on the classification of adversarial images. For Fast-Gradient-Sign perturbations of small magnitude, we found that JPG compression often reverses the drop in classification accuracy to a large extent, but not always. As the magnitude of the perturbations increases, JPG recompression alone is insufficient to reverse the effect.},
  archiveprefix = {arXiv},
  author = {Dziugaite, Gintare Karolina and Ghahramani, Zoubin and Roy, Daniel M.},
  eprint = {1608.00853v1},
  file = {1608.00853v1.pdf},
  month = {Aug},
  primaryclass = {cs.CV},
  title = {A study of the effect of JPG compression on adversarial images},
  url = {http://arxiv.org/abs/1608.00853v1},
  year = {2016},
}

@inproceedings{LearningMultipKrizhe2009,
  abstract = {Groups at MIT and NYU have collected a dataset of millions of tiny colour images from the web. It is, in principle, an excellent dataset for unsupervised training of deep generative models, but previous researchers who have tried this have found it dicult to learn a good set of lters from the images. We show how to train a multi-layer generative model that learns to extract meaningful features which resemble those found in the human visual cortex. Using a novel parallelization algorithm to distribute the work among multiple machines connected on a network, we show how training such a model can be done in reasonable time. A second problematic aspect of the tiny images dataset is that there are no reliable class labels which makes it hard to use for object recognition experiments. We created two sets of reliable labels. The CIFAR-10 set has 6000 examples of each of 10 classes and the CIFAR-100 set has 600 examples of each of 100 non-overlapping classes. Using these labels, we show that object recognition is signicantly improved by pre-training a layer of features on a large set of unlabeled tiny images.},
  author = {Krizhevsky, Alex},
  title = {Learning Multiple Layers of Features from Tiny Images},
  url = {https://api.semanticscholar.org/CorpusID:18268744},
  year = {2009},
}

@article{GradientBasedLecun1998,
  author = {Lecun, Y. and Bottou, L. and Bengio, Y. and Haffner, P.},
  doi = {10.1109/5.726791},
  issue = {11},
  journal = {Proceedings of the IEEE},
  pages = {2278--2324},
  publisher = {Institute of Electrical and Electronics Engineers (IEEE)},
  title = {Gradient-based learning applied to document recognition},
  url = {http://dx.doi.org/10.1109/5.726791},
  volume = {86},
  year = {1998},
}

@inproceedings{UnderstandingOLiuY2010,
  abstract = {Clustering validation has long been recognized as one of the vital issues essential to the success of clustering applications. In general, clustering validation can be categorized into two classes, external clustering validation and internal clustering validation. In this paper, we focus on internal clustering validation and present a detailed study of 11 widely used internal clustering validation measures for crisp clustering. From five conventional aspects of clustering, we investigate their validation properties. Experiment results show that S\_Dbw is the only internal validation measure which performs well in all five aspects, while other measures have certain limitations in different application scenarios.},
  author = {Liu, Yanchi and Li, Zhongmou and Xiong, Hui and Gao, Xuedong and Wu, Junjie},
  booktitle = {2010 IEEE 10th International Conference on Data Mining (ICDM)},
  doi = {10.1109/icdm.2010.35},
  journal = {2010 IEEE International Conference on Data Mining},
  month = {12},
  publisher = {IEEE},
  title = {Understanding of Internal Clustering Validation Measures},
  url = {http://dx.doi.org/10.1109/icdm.2010.35},
  venue = {Sydney, Australia},
  year = {2010},
}

@inproceedings{ImagenetALarDeng2009,
  author = {Deng, Jia and Dong, Wei and Socher, Richard and Li, Li-Jia and Kai Li, None and Li Fei-Fei, None},
  booktitle = {2009 IEEE Computer Society Conference on Computer Vision and Pattern Recognition Workshops (CVPR Workshops)},
  doi = {10.1109/cvpr.2009.5206848},
  journal = {2009 IEEE Conference on Computer Vision and Pattern Recognition},
  month = {6},
  publisher = {IEEE},
  title = {ImageNet: A large-scale hierarchical image database},
  url = {http://dx.doi.org/10.1109/cvpr.2009.5206848},
  venue = {Miami, FL},
  year = {2009},
}

@inproceedings{LearningAndUsGriffi2008,
  author = {Griffin, Gregory and Perona, Pietro},
  booktitle = {2008 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
  doi = {10.1109/cvpr.2008.4587410},
  journal = {2008 IEEE Conference on Computer Vision and Pattern Recognition},
  month = {6},
  publisher = {IEEE},
  title = {Learning and using taxonomies for fast visual categorization},
  url = {http://dx.doi.org/10.1109/cvpr.2008.4587410},
  venue = {Anchorage, AK, USA},
  year = {2008},
}

@inproceedings{GloveGlobalVPennin2014,
  author = {Pennington, Jeffrey and Socher, Richard and Manning, Christopher},
  booktitle = {Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)},
  doi = {10.3115/v1/d14-1162},
  journal = {Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)},
  month = {10},
  publisher = {Association for Computational Linguistics},
  title = {Glove: Global Vectors for Word Representation},
  url = {http://dx.doi.org/10.3115/v1/d14-1162},
  venue = {Doha, Qatar},
  year = {2014},
}

@article{ImageTransformParmar2018,
  abstract = {Image generation has been successfully cast as an autoregressive sequence generation or transformation problem. Recent work has shown that self-attention is an effective way of modeling textual sequences. In this work, we generalize a recently proposed model architecture based on self-attention, the Transformer, to a sequence modeling formulation of image generation with a tractable likelihood. By restricting the self-attention mechanism to attend to local neighborhoods we significantly increase the size of images the model can process in practice, despite maintaining significantly larger receptive fields per layer than typical convolutional neural networks. While conceptually simple, our generative models significantly outperform the current state of the art in image generation on ImageNet, improving the best published negative log-likelihood on ImageNet from 3.83 to 3.77. We also present results on image super-resolution with a large magnification ratio, applying an encoder-decoder configuration of our architecture. In a human evaluation study, we find that images generated by our super-resolution model fool human observers three times more often than the previous state of the art.},
  archiveprefix = {arXiv},
  author = {Parmar, Niki and Vaswani, Ashish and Uszkoreit, Jakob and Kaiser, \L{}ukasz and Shazeer, Noam and Ku, Alexander and Tran, Dustin},
  eprint = {1802.05751v3},
  file = {1802.05751v3.pdf},
  month = {2},
  primaryclass = {cs.CV},
  title = {Image Transformer},
  url = {http://arxiv.org/abs/1802.05751v3},
  year = {2018},
}

@article{EfficientEstimMikolo2013,
  abstract = {We propose two novel model architectures for computing continuous vector representations of words from very large data sets. The quality of these representations is measured in a word similarity task, and the results are compared to the previously best performing techniques based on different types of neural networks. We observe large improvements in accuracy at much lower computational cost, i.e. it takes less than a day to learn high quality word vectors from a 1.6 billion words data set. Furthermore, we show that these vectors provide state-of-the-art performance on our test set for measuring syntactic and semantic word similarities.},
  archiveprefix = {arXiv},
  author = {Mikolov, Tomas and Chen, Kai and Corrado, Greg and Dean, Jeffrey},
  eprint = {1301.3781v3},
  file = {1301.3781v3.pdf},
  month = {1},
  primaryclass = {cs.CL},
  title = {Efficient Estimation of Word Representations in Vector Space},
  url = {http://arxiv.org/abs/1301.3781v3},
  year = {2013},
}

@article{SemanticImagePasini2022,
  author = {Pasini, Andrea and Giobergia, Flavio and Pastor, Eliana and Baralis, Elena},
  doi = {10.1109/access.2022.3229654},
  journal = {IEEE Access},
  pages = {131747--131764},
  publisher = {Institute of Electrical and Electronics Engineers (IEEE)},
  title = {Semantic Image Collection Summarization With Frequent Subgraph Mining},
  url = {http://dx.doi.org/10.1109/access.2022.3229654},
  volume = {10},
  year = {2022},
}

@article{SqueezenetAleIandol2016,
  abstract = {Recent research on deep neural networks has focused primarily on improving accuracy. For a given accuracy level, it is typically possible to identify multiple DNN architectures that achieve that accuracy level. With equivalent accuracy, smaller DNN architectures offer at least three advantages: (1) Smaller DNNs require less communication across servers during distributed training. (2) Smaller DNNs require less bandwidth to export a new model from the cloud to an autonomous car. (3) Smaller DNNs are more feasible to deploy on FPGAs and other hardware with limited memory. To provide all of these advantages, we propose a small DNN architecture called SqueezeNet. SqueezeNet achieves AlexNet-level accuracy on ImageNet with 50x fewer parameters. Additionally, with model compression techniques we are able to compress SqueezeNet to less than 0.5MB (510x smaller than AlexNet).   The SqueezeNet architecture is available for download here: https://github.com/DeepScale/SqueezeNet},
  archiveprefix = {arXiv},
  author = {Iandola, Forrest N. and Han, Song and Moskewicz, Matthew W. and Ashraf, Khalid and Dally, William J. and Keutzer, Kurt},
  eprint = {1602.07360v4},
  file = {1602.07360v4.pdf},
  month = {2},
  primaryclass = {cs.CV},
  title = {SqueezeNet: AlexNet-level accuracy with 50x fewer parameters and <0.5MB   model size},
  url = {http://arxiv.org/abs/1602.07360v4},
  year = {2016},
}

@article{ImagenetClassiKrizhe2017,
  abstract = {We trained a large, deep convolutional neural network to classify the 1.2 million high-resolution images in the ImageNet LSVRC-2010 contest into the 1000 different classes. On the test data, we achieved top-1 and top-5 error rates of 37.5\% and 17.0\%, respectively, which is considerably better than the previous state-of-the-art. The neural network, which has 60 million parameters and 650,000 neurons, consists of five convolutional layers, some of which are followed by max-pooling layers, and three fully connected layers with a final 1000-way softmax. To make training faster, we used non-saturating neurons and a very efficient GPU implementation of the convolution operation. To reduce overfitting in the fully connected layers we employed a recently developed regularization method called "dropout" that proved to be very effective. We also entered a variant of this model in the ILSVRC-2012 competition and achieved a winning top-5 test error rate of 15.3\%, compared to 26.2\% achieved by the second-best entry.},
  author = {Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey E.},
  doi = {10.1145/3065386},
  issue = {6},
  journal = {Communications of the ACM},
  language = {en},
  month = {5},
  pages = {84--90},
  publisher = {Association for Computing Machinery (ACM)},
  title = {ImageNet classification with deep convolutional neural networks},
  url = {http://dx.doi.org/10.1145/3065386},
  volume = {60},
  year = {2017},
}

@article{LabelEmbeddingAkata2015,
  abstract = {Attributes act as intermediate representations that enable parameter sharing between classes, a must when training data is scarce. We propose to view attribute-based image classification as a label-embedding problem: each class is embedded in the space of attribute vectors. We introduce a function that measures the compatibility between an image and a label embedding. The parameters of this function are learned on a training set of labeled samples to ensure that, given an image, the correct classes rank higher than the incorrect ones. Results on the Animals With Attributes and Caltech-UCSD-Birds datasets show that the proposed framework outperforms the standard Direct Attribute Prediction baseline in a zero-shot learning scenario. Label embedding enjoys a built-in ability to leverage alternative sources of information instead of or in addition to attributes, such as e.g. class hierarchies or textual descriptions. Moreover, label embedding encompasses the whole range of learning settings from zero-shot learning to regular learning with a large number of labeled examples.},
  archiveprefix = {arXiv},
  author = {Akata, Zeynep and Perronnin, Florent and Harchaoui, Zaid and Schmid, Cordelia},
  doi = {10.1109/TPAMI.2015.2487986},
  eprint = {1503.08677v2},
  file = {1503.08677v2.pdf},
  month = {3},
  primaryclass = {cs.CV},
  title = {Label-Embedding for Image Classification},
  url = {http://arxiv.org/abs/1503.08677v2},
  year = {2015},
}

@article{FeatureSqueeziXuWe2017,
  abstract = {Although deep neural networks (DNNs) have achieved great success in many tasks, they can often be fooled by \textbackslash{}emph\lbrace{}adversarial examples\rbrace{} that are generated by adding small but purposeful distortions to natural examples. Previous studies to defend against adversarial examples mostly focused on refining the DNN models, but have either shown limited success or required expensive computation. We propose a new strategy, \textbackslash{}emph\lbrace{}feature squeezing\rbrace{}, that can be used to harden DNN models by detecting adversarial examples. Feature squeezing reduces the search space available to an adversary by coalescing samples that correspond to many different feature vectors in the original space into a single sample. By comparing a DNN model's prediction on the original input with that on squeezed inputs, feature squeezing detects adversarial examples with high accuracy and few false positives. This paper explores two feature squeezing methods: reducing the color bit depth of each pixel and spatial smoothing. These simple strategies are inexpensive and complementary to other defenses, and can be combined in a joint detection framework to achieve high detection rates against state-of-the-art attacks.},
  archiveprefix = {arXiv},
  author = {Xu, Weilin and Evans, David and Qi, Yanjun},
  doi = {10.14722/ndss.2018.23198},
  eprint = {1704.01155v2},
  file = {1704.01155v2.pdf},
  month = {Apr},
  primaryclass = {cs.CV},
  title = {Feature Squeezing: Detecting Adversarial Examples in Deep Neural   Networks},
  url = {http://arxiv.org/abs/1704.01155v2},
  year = {2017},
}

@article{WordnetMi1995,
  abstract = {Because meaningful sentences are composed of meaningful words, any system that hopes to process natural languages as people do must have information about words and their meanings. This information is traditionally provided through dictionaries, and machine-readable dictionaries are now widely available. But dictionary entries evolved for the convenience of human readers, not for machines. WordNet provides a more effective combination of traditional lexicographic information and modern computing. WordNet is an online lexical database designed for use under program control. English nouns, verbs, adjectives, and adverbs are organized into sets of synonyms, each representing a lexicalized concept. Semantic relations link the synonym sets},
  author = {Miller, George A.},
  doi = {10.1145/219717.219748},
  journal = {Communications of the ACM},
  language = {EN},
  month = {11},
  pages = {39\textendash{}41},
  title = {WordNet: a lexical database for English},
  url = {https://dl.acm.org/doi/10.1145/219717.219748},
  year = {1995},
}

@article{AnImageIsWorDosovi2020,
  abstract = {While the Transformer architecture has become the de-facto standard for natural language processing tasks, its applications to computer vision remain limited. In vision, attention is either applied in conjunction with convolutional networks, or used to replace certain components of convolutional networks while keeping their overall structure in place. We show that this reliance on CNNs is not necessary and a pure transformer applied directly to sequences of image patches can perform very well on image classification tasks. When pre-trained on large amounts of data and transferred to multiple mid-sized or small image recognition benchmarks (ImageNet, CIFAR-100, VTAB, etc.), Vision Transformer (ViT) attains excellent results compared to state-of-the-art convolutional networks while requiring substantially fewer computational resources to train.},
  archiveprefix = {arXiv},
  author = {Dosovitskiy, Alexey and Beyer, Lucas and Kolesnikov, Alexander and Weissenborn, Dirk and Zhai, Xiaohua and Unterthiner, Thomas and Dehghani, Mostafa and Minderer, Matthias and Heigold, Georg and Gelly, Sylvain and Uszkoreit, Jakob and Houlsby, Neil},
  eprint = {2010.11929v2},
  file = {2010.11929v2.pdf},
  month = {10},
  primaryclass = {cs.CV},
  title = {An Image is Worth 16x16 Words: Transformers for Image Recognition at   Scale},
  url = {http://arxiv.org/abs/2010.11929v2},
  year = {2020},
}

@article{ExplainingAndGoodfe2014,
  abstract = {Several machine learning models, including neural networks, consistently misclassify adversarial examples---inputs formed by applying small but intentionally worst-case perturbations to examples from the dataset, such that the perturbed input results in the model outputting an incorrect answer with high confidence. Early attempts at explaining this phenomenon focused on nonlinearity and overfitting. We argue instead that the primary cause of neural networks' vulnerability to adversarial perturbation is their linear nature. This explanation is supported by new quantitative results while giving the first explanation of the most intriguing fact about them: their generalization across architectures and training sets. Moreover, this view yields a simple and fast method of generating adversarial examples. Using this approach to provide examples for adversarial training, we reduce the test set error of a maxout network on the MNIST dataset.},
  archiveprefix = {arXiv},
  author = {Goodfellow, Ian J. and Shlens, Jonathon and Szegedy, Christian},
  eprint = {1412.6572v3},
  file = {1412.6572v3.pdf},
  month = {12},
  primaryclass = {stat.ML},
  title = {Explaining and Harnessing Adversarial Examples},
  url = {http://arxiv.org/abs/1412.6572v3},
  year = {2014},
}

@inproceedings{LearningHierarVerma2012,
  abstract = {Categories in multi-class data are often part of an underlying semantic taxonomy. Recent work in object classification has found interesting ways to use this taxonomy structure to develop better recognition algorithms. Here we propose a novel framework to learn similarity metrics using the class taxonomy. We show that a nearest neighbor classifier using the learned metrics gets improved performance over the best discriminative methods. Moreover, by incorporating the taxonomy, our learned metrics can also help in some taxonomy specific applications. We show that the metrics can help determine the correct placement of a new category that was not part of the original taxonomy, and can provide effective classification amongst categories local to specific subtrees of the taxonomy.},
  author = {Verma, Nakul and Mahajan, Dhruv and Sellamanickam, Sundararajan and Nair, Vinod},
  booktitle = {2012 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
  doi = {10.1109/cvpr.2012.6247938},
  journal = {2012 IEEE Conference on Computer Vision and Pattern Recognition},
  month = {6},
  publisher = {IEEE},
  title = {Learning hierarchical similarity metrics},
  url = {http://dx.doi.org/10.1109/cvpr.2012.6247938},
  venue = {Providence, RI},
  year = {2012},
}

@article{IdentityMappinHeKa2016,
  abstract = {Deep residual networks have emerged as a family of extremely deep architectures showing compelling accuracy and nice convergence behaviors. In this paper, we analyze the propagation formulations behind the residual building blocks, which suggest that the forward and backward signals can be directly propagated from one block to any other block, when using identity mappings as the skip connections and after-addition activation. A series of ablation experiments support the importance of these identity mappings. This motivates us to propose a new residual unit, which makes training easier and improves generalization. We report improved results using a 1001-layer ResNet on CIFAR-10 (4.62\% error) and CIFAR-100, and a 200-layer ResNet on ImageNet. Code is available at: https://github.com/KaimingHe/resnet-1k-layers},
  archiveprefix = {arXiv},
  author = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
  eprint = {1603.05027v3},
  file = {1603.05027v3.pdf},
  month = {3},
  primaryclass = {cs.CV},
  title = {Identity Mappings in Deep Residual Networks},
  url = {http://arxiv.org/abs/1603.05027v3},
  year = {2016},
}

@article{TheInaturalistHorn2017,
  abstract = {Existing image classification datasets used in computer vision tend to have a uniform distribution of images across object categories. In contrast, the natural world is heavily imbalanced, as some species are more abundant and easier to photograph than others. To encourage further progress in challenging real world conditions we present the iNaturalist species classification and detection dataset, consisting of 859,000 images from over 5,000 different species of plants and animals. It features visually similar species, captured in a wide variety of situations, from all over the world. Images were collected with different camera types, have varying image quality, feature a large class imbalance, and have been verified by multiple citizen scientists. We discuss the collection of the dataset and present extensive baseline experiments using state-of-the-art computer vision classification and detection models. Results show that current non-ensemble based methods achieve only 67\% top one classification accuracy, illustrating the difficulty of the dataset. Specifically, we observe poor results for classes with small numbers of training examples suggesting more attention is needed in low-shot learning.},
  archiveprefix = {arXiv},
  author = {Horn, Grant Van and Aodha, Oisin Mac and Song, Yang and Cui, Yin and Sun, Chen and Shepard, Alex and Adam, Hartwig and Perona, Pietro and Belongie, Serge},
  eprint = {1707.06642v2},
  file = {1707.06642v2.pdf},
  month = {7},
  primaryclass = {cs.CV},
  title = {The iNaturalist Species Classification and Detection Dataset},
  url = {http://arxiv.org/abs/1707.06642v2},
  year = {2017},
}

@article{EvolvingNeuralStanle2002,
  abstract = {An important question in neuroevolution is how to gain an advantage from evolving neural network topologies along with weights. We present a method, NeuroEvolution of Augmenting Topologies (NEAT), which outperforms the best fixed-topology method on a challenging benchmark reinforcement learning task. We claim that the increased efficiency is due to (1) employing a principled method of crossover of different topologies, (2) protecting structural innovation using speciation, and (3) incrementally growing from minimal structure. We test this claim through a series of ablation studies that demonstrate that each component is necessary to the system as a whole and to each other. What results is signicantly faster learning. NEAT is also an important contribution to GAs because it shows how it is possible for evolution to both optimize and complexify solutions simultaneously, offering the possibility of evolving increasingly complex solutions over generations, and strengthening the analogy with biological evolution.},
  author = {Stanley, Kenneth O. and Miikkulainen, Risto},
  doi = {10.1162/106365602320169811},
  issue = {2},
  journal = {Evolutionary Computation},
  language = {en},
  month = {6},
  pages = {99--127},
  publisher = {MIT Press - Journals},
  title = {Evolving Neural Networks through Augmenting Topologies},
  url = {http://dx.doi.org/10.1162/106365602320169811},
  volume = {10},
  year = {2002},
}

@article{LearningHierarGarg2022,
  abstract = {Label hierarchies are often available apriori as part of biological taxonomy or language datasets WordNet. Several works exploit these to learn hierarchy aware features in order to improve the classifier to make semantically meaningful mistakes while maintaining or reducing the overall error. In this paper, we propose a novel approach for learning Hierarchy Aware Features (HAF) that leverages classifiers at each level of the hierarchy that are constrained to generate predictions consistent with the label hierarchy. The classifiers are trained by minimizing a Jensen-Shannon Divergence with target soft labels obtained from the fine-grained classifiers. Additionally, we employ a simple geometric loss that constrains the feature space geometry to capture the semantic structure of the label space. HAF is a training time approach that improves the mistakes while maintaining top-1 error, thereby, addressing the problem of cross-entropy loss that treats all mistakes as equal. We evaluate HAF on three hierarchical datasets and achieve state-of-the-art results on the iNaturalist-19 and CIFAR-100 datasets. The source code is available at https://github.com/07Agarg/HAF},
  archiveprefix = {arXiv},
  author = {Garg, Ashima and Sani, Depanshu and Anand, Saket},
  eprint = {2207.12646v1},
  file = {2207.12646v1.pdf},
  month = {7},
  primaryclass = {cs.CV},
  title = {Learning Hierarchy Aware Features for Reducing Mistake Severity},
  url = {http://arxiv.org/abs/2207.12646v1},
  year = {2022},
}

@article{GoingDeeperWiSzeged2014,
  abstract = {We propose a deep convolutional neural network architecture codenamed "Inception", which was responsible for setting the new state of the art for classification and detection in the ImageNet Large-Scale Visual Recognition Challenge 2014 (ILSVRC 2014). The main hallmark of this architecture is the improved utilization of the computing resources inside the network. This was achieved by a carefully crafted design that allows for increasing the depth and width of the network while keeping the computational budget constant. To optimize quality, the architectural decisions were based on the Hebbian principle and the intuition of multi-scale processing. One particular incarnation used in our submission for ILSVRC 2014 is called GoogLeNet, a 22 layers deep network, the quality of which is assessed in the context of classification and detection.},
  archiveprefix = {arXiv},
  author = {Szegedy, Christian and Liu, Wei and Jia, Yangqing and Sermanet, Pierre and Reed, Scott and Anguelov, Dragomir and Erhan, Dumitru and Vanhoucke, Vincent and Rabinovich, Andrew},
  eprint = {1409.4842v1},
  file = {1409.4842v1.pdf},
  month = {9},
  primaryclass = {cs.CV},
  title = {Going Deeper with Convolutions},
  url = {http://arxiv.org/abs/1409.4842v1},
  year = {2014},
}

@article{LatentEmbeddinXian2016,
  abstract = {We present a novel latent embedding model for learning a compatibility function between image and class embeddings, in the context of zero-shot classification. The proposed method augments the state-of-the-art bilinear compatibility model by incorporating latent variables. Instead of learning a single bilinear map, it learns a collection of maps with the selection, of which map to use, being a latent variable for the current image-class pair. We train the model with a ranking based objective function which penalizes incorrect rankings of the true class for a given image. We empirically demonstrate that our model improves the state-of-the-art for various class embeddings consistently on three challenging publicly available datasets for the zero-shot setting. Moreover, our method leads to visually highly interpretable results with clear clusters of different fine-grained object properties that correspond to different latent variable maps.},
  archiveprefix = {arXiv},
  author = {Xian, Yongqin and Akata, Zeynep and Sharma, Gaurav and Nguyen, Quynh and Hein, Matthias and Schiele, Bernt},
  eprint = {1603.08895v2},
  file = {1603.08895v2.pdf},
  month = {3},
  primaryclass = {cs.CV},
  title = {Latent Embeddings for Zero-shot Classification},
  url = {http://arxiv.org/abs/1603.08895v2},
  year = {2016},
}

@article{TowardsEvaluatCarlin2016,
  abstract = {Neural networks provide state-of-the-art results for most machine learning tasks. Unfortunately, neural networks are vulnerable to adversarial examples: given an input \textdollar{}x\textdollar{} and any target classification \textdollar{}t\textdollar{}, it is possible to find a new input \textdollar{}x'\textdollar{} that is similar to \textdollar{}x\textdollar{} but classified as \textdollar{}t\textdollar{}. This makes it difficult to apply neural networks in security-critical areas. Defensive distillation is a recently proposed approach that can take an arbitrary neural network, and increase its robustness, reducing the success rate of current attacks' ability to find adversarial examples from \textdollar{}95\textbackslash{}\%\textdollar{} to \textdollar{}0.5\textbackslash{}\%\textdollar{}.   In this paper, we demonstrate that defensive distillation does not significantly increase the robustness of neural networks by introducing three new attack algorithms that are successful on both distilled and undistilled neural networks with \textdollar{}100\textbackslash{}\%\textdollar{} probability. Our attacks are tailored to three distance metrics used previously in the literature, and when compared to previous adversarial example generation algorithms, our attacks are often much more effective (and never worse). Furthermore, we propose using high-confidence adversarial examples in a simple transferability test we show can also be used to break defensive distillation. We hope our attacks will be used as a benchmark in future defense attempts to create neural networks that resist adversarial examples.},
  archiveprefix = {arXiv},
  author = {Carlini, Nicholas and Wagner, David},
  eprint = {1608.04644v2},
  file = {1608.04644v2.pdf},
  month = {Aug},
  primaryclass = {cs.CR},
  title = {Towards Evaluating the Robustness of Neural Networks},
  url = {http://arxiv.org/abs/1608.04644v2},
  year = {2016},
}

@article{DoConvolutionaAlsall2017,
  abstract = {Convolutional Neural Networks (CNNs) currently achieve state-of-the-art accuracy in image classification. With a growing number of classes, the accuracy usually drops as the possibilities of confusion increase. Interestingly, the class confusion patterns follow a hierarchical structure over the classes. We present visual-analytics methods to reveal and analyze this hierarchy of similar classes in relation with CNN-internal data. We found that this hierarchy not only dictates the confusion patterns between the classes, it furthermore dictates the learning behavior of CNNs. In particular, the early layers in these networks develop feature detectors that can separate high-level groups of classes quite well, even after a few training epochs. In contrast, the latter layers require substantially more epochs to develop specialized feature detectors that can separate individual classes. We demonstrate how these insights are key to significant improvement in accuracy by designing hierarchy-aware CNNs that accelerate model convergence and alleviate overfitting. We further demonstrate how our methods help in identifying various quality issues in the training data.},
  archiveprefix = {arXiv},
  author = {Alsallakh, Bilal and Jourabloo, Amin and Ye, Mao and Liu, Xiaoming and Ren, Liu},
  doi = {10.1109/TVCG.2017.2744683},
  eprint = {1710.06501v1},
  file = {1710.06501v1.pdf},
  month = {10},
  note = {IEEE Transactions on Visualization and Computer Graphics, Volume:   24, Issue: 1 (2018)},
  primaryclass = {cs.CV},
  title = {Do Convolutional Neural Networks Learn Class Hierarchy?},
  url = {http://arxiv.org/abs/1710.06501v1},
  year = {2017},
}

@article{RegularizedEvoReal2018,
  abstract = {The effort devoted to hand-crafting neural network image classifiers has motivated the use of architecture search to discover them automatically. Although evolutionary algorithms have been repeatedly applied to neural network topologies, the image classifiers thus discovered have remained inferior to human-crafted ones. Here, we evolve an image classifier---AmoebaNet-A---that surpasses hand-designs for the first time. To do this, we modify the tournament selection evolutionary algorithm by introducing an age property to favor the younger genotypes. Matching size, AmoebaNet-A has comparable accuracy to current state-of-the-art ImageNet models discovered with more complex architecture-search methods. Scaled to larger size, AmoebaNet-A sets a new state-of-the-art 83.9\% / 96.6\% top-5 ImageNet accuracy. In a controlled comparison against a well known reinforcement learning algorithm, we give evidence that evolution can obtain results faster with the same hardware, especially at the earlier stages of the search. This is relevant when fewer compute resources are available. Evolution is, thus, a simple method to effectively discover high-quality architectures.},
  archiveprefix = {arXiv},
  author = {Real, Esteban and Aggarwal, Alok and Huang, Yanping and Le, Quoc V},
  eprint = {1802.01548v7},
  file = {1802.01548v7.pdf},
  month = {2},
  primaryclass = {cs.NE},
  title = {Regularized Evolution for Image Classifier Architecture Search},
  url = {http://arxiv.org/abs/1802.01548v7},
  year = {2018},
}

@article{PoincareEmbeddNickel2017,
  abstract = {Representation learning has become an invaluable approach for learning from symbolic data such as text and graphs. However, while complex symbolic datasets often exhibit a latent hierarchical structure, state-of-the-art methods typically learn embeddings in Euclidean vector spaces, which do not account for this property. For this purpose, we introduce a new approach for learning hierarchical representations of symbolic data by embedding them into hyperbolic space -- or more precisely into an n-dimensional Poincar\textbackslash{}'e ball. Due to the underlying hyperbolic geometry, this allows us to learn parsimonious representations of symbolic data by simultaneously capturing hierarchy and similarity. We introduce an efficient algorithm to learn the embeddings based on Riemannian optimization and show experimentally that Poincar\textbackslash{}'e embeddings outperform Euclidean embeddings significantly on data with latent hierarchies, both in terms of representation capacity and in terms of generalization ability.},
  archiveprefix = {arXiv},
  author = {Nickel, Maximilian and Kiela, Douwe},
  eprint = {1705.08039v2},
  file = {1705.08039v2.pdf},
  month = {5},
  primaryclass = {cs.AI},
  title = {Poincar\'{e} Embeddings for Learning Hierarchical Representations},
  url = {http://arxiv.org/abs/1705.08039v2},
  year = {2017},
}

@article{IntegratingDomBrust2018,
  abstract = {One of the most prominent problems in machine learning in the age of deep learning is the availability of sufficiently large annotated datasets. For specific domains, e.g. animal species, a long-tail distribution means that some classes are observed and annotated insufficiently. Additional labels can be prohibitively expensive, e.g. because domain experts need to be involved. However, there is more information available that is to the best of our knowledge not exploited accordingly. In this paper, we propose to make use of preexisting class hierarchies like WordNet to integrate additional domain knowledge into classification. We encode the properties of such a class hierarchy into a probabilistic model. From there, we derive a novel label encoding and a corresponding loss function. On the ImageNet and NABirds datasets our method offers a relative improvement of 10.4\% and 9.6\% in accuracy over the baseline respectively. After less than a third of training time, it is already able to match the baseline's fine-grained recognition performance. Both results show that our suggested method is efficient and effective.},
  archiveprefix = {arXiv},
  author = {Brust, Clemens-Alexander and Denzler, Joachim},
  eprint = {1811.07125v2},
  file = {1811.07125v2.pdf},
  month = {11},
  primaryclass = {cs.CV},
  title = {Integrating domain knowledge: using hierarchies to improve deep   classifiers},
  url = {http://arxiv.org/abs/1811.07125v2},
  year = {2018},
}

@article{TheLimitationsPapern2015,
  abstract = {Deep learning takes advantage of large datasets and computationally efficient training algorithms to outperform other approaches at various machine learning tasks. However, imperfections in the training phase of deep neural networks make them vulnerable to adversarial samples: inputs crafted by adversaries with the intent of causing deep neural networks to misclassify. In this work, we formalize the space of adversaries against deep neural networks (DNNs) and introduce a novel class of algorithms to craft adversarial samples based on a precise understanding of the mapping between inputs and outputs of DNNs. In an application to computer vision, we show that our algorithms can reliably produce samples correctly classified by human subjects but misclassified in specific targets by a DNN with a 97\% adversarial success rate while only modifying on average 4.02\% of the input features per sample. We then evaluate the vulnerability of different sample classes to adversarial perturbations by defining a hardness measure. Finally, we describe preliminary work outlining defenses against adversarial samples by defining a predictive measure of distance between a benign input and a target classification.},
  archiveprefix = {arXiv},
  author = {Papernot, Nicolas and McDaniel, Patrick and Jha, Somesh and Fredrikson, Matt and Celik, Z. Berkay and Swami, Ananthram},
  eprint = {1511.07528v1},
  file = {1511.07528v1.pdf},
  month = {Nov},
  primaryclass = {cs.CR},
  title = {The Limitations of Deep Learning in Adversarial Settings},
  url = {http://arxiv.org/abs/1511.07528v1},
  year = {2015},
}

@article{EvaluationOfOAkata2014,
  abstract = {Image classification has advanced significantly in recent years with the availability of large-scale image sets. However, fine-grained classification remains a major challenge due to the annotation cost of large numbers of fine-grained categories. This project shows that compelling classification performance can be achieved on such categories even without labeled training data. Given image and class embeddings, we learn a compatibility function such that matching embeddings are assigned a higher score than mismatching ones; zero-shot classification of an image proceeds by finding the label yielding the highest joint compatibility score. We use state-of-the-art image features and focus on different supervised attributes and unsupervised output embeddings either derived from hierarchies or learned from unlabeled text corpora. We establish a substantially improved state-of-the-art on the Animals with Attributes and Caltech-UCSD Birds datasets. Most encouragingly, we demonstrate that purely unsupervised output embeddings (learned from Wikipedia and improved with fine-grained text) achieve compelling results, even outperforming the previous supervised state-of-the-art. By combining different output embeddings, we further improve results.},
  archiveprefix = {arXiv},
  author = {Akata, Zeynep and Reed, Scott and Walter, Daniel and Lee, Honglak and Schiele, Bernt},
  doi = {10.1109/CVPR.2015.7298911},
  eprint = {1409.8403v2},
  file = {1409.8403v2.pdf},
  month = {9},
  primaryclass = {cs.CV},
  title = {Evaluation of Output Embeddings for Fine-Grained Image Classification},
  url = {http://arxiv.org/abs/1409.8403v2},
  year = {2014},
}

@article{LearningWithHChen2019,
  abstract = {Label hierarchies widely exist in many vision-related problems, ranging from explicit label hierarchies existed in image classification to latent label hierarchies existed in semantic segmentation. Nevertheless, state-of-the-art methods often deploy cross-entropy loss that implicitly assumes class labels to be exclusive and thus independence from each other. Motivated by the fact that classes from the same parental category usually share certain similarity, we design a new training diagram called Hierarchical Complement Objective Training (HCOT) that leverages the information from label hierarchy. HCOT maximizes the probability of the ground truth class, and at the same time, neutralizes the probabilities of rest of the classes in a hierarchical fashion, making the model take advantage of the label hierarchy explicitly. The proposed HCOT is evaluated on both image classification and semantic segmentation tasks. Experimental results confirm that HCOT outperforms state-of-the-art models in CIFAR-100, ImageNet-2012, and PASCAL-Context. The study further demonstrates that HCOT can be applied on tasks with latent label hierarchies, which is a common characteristic in many machine learning tasks.},
  archiveprefix = {arXiv},
  author = {Chen, Hao-Yun and Tsai, Li-Huang and Chang, Shih-Chieh and Pan, Jia-Yu and Chen, Yu-Ting and Wei, Wei and Juan, Da-Cheng},
  eprint = {1911.07257v1},
  file = {1911.07257v1.pdf},
  month = {11},
  primaryclass = {cs.CV},
  title = {Learning with Hierarchical Complement Objective},
  url = {http://arxiv.org/abs/1911.07257v1},
  year = {2019},
}

@article{HyperbolicEntaGanea2018,
  abstract = {Learning graph representations via low-dimensional embeddings that preserve relevant network properties is an important class of problems in machine learning. We here present a novel method to embed directed acyclic graphs. Following prior work, we first advocate for using hyperbolic spaces which provably model tree-like structures better than Euclidean geometry. Second, we view hierarchical relations as partial orders defined using a family of nested geodesically convex cones. We prove that these entailment cones admit an optimal shape with a closed form expression both in the Euclidean and hyperbolic spaces, and they canonically define the embedding learning process. Experiments show significant improvements of our method over strong recent baselines both in terms of representational capacity and generalization.},
  archiveprefix = {arXiv},
  author = {Ganea, Octavian-Eugen and B\'{e}cigneul, Gary and Hofmann, Thomas},
  eprint = {1804.01882v3},
  file = {1804.01882v3.pdf},
  month = {4},
  primaryclass = {cs.LG},
  title = {Hyperbolic Entailment Cones for Learning Hierarchical Embeddings},
  url = {http://arxiv.org/abs/1804.01882v3},
  year = {2018},
}

@article{ShowAttendAnXuKe2015,
  abstract = {Inspired by recent work in machine translation and object detection, we introduce an attention based model that automatically learns to describe the content of images. We describe how we can train this model in a deterministic manner using standard backpropagation techniques and stochastically by maximizing a variational lower bound. We also show through visualization how the model is able to automatically learn to fix its gaze on salient objects while generating the corresponding words in the output sequence. We validate the use of attention with state-of-the-art performance on three benchmark datasets: Flickr8k, Flickr30k and MS COCO.},
  archiveprefix = {arXiv},
  author = {Xu, Kelvin and Ba, Jimmy and Kiros, Ryan and Cho, Kyunghyun and Courville, Aaron and Salakhutdinov, Ruslan and Zemel, Richard and Bengio, Yoshua},
  eprint = {1502.03044v3},
  file = {1502.03044v3.pdf},
  month = {2},
  primaryclass = {cs.LG},
  title = {Show, Attend and Tell: Neural Image Caption Generation with Visual   Attention},
  url = {http://arxiv.org/abs/1502.03044v3},
  year = {2015},
}

@inproceedings{ExploitingObjeZweig2007,
  author = {Zweig, Alon and Weinshall, Daphna},
  booktitle = {2007 IEEE 11th International Conference on Computer Vision},
  doi = {10.1109/iccv.2007.4409064},
  journal = {2007 IEEE 11th International Conference on Computer Vision},
  month = {10},
  publisher = {IEEE},
  title = {Exploiting Object Hierarchy: Combining Models from Different Category Levels},
  url = {http://dx.doi.org/10.1109/iccv.2007.4409064},
  venue = {Rio de Janeiro, Brazil},
  year = {2007},
}

@article{VeryDeepConvoSimony2014,
  abstract = {In this work we investigate the effect of the convolutional network depth on its accuracy in the large-scale image recognition setting. Our main contribution is a thorough evaluation of networks of increasing depth using an architecture with very small (3x3) convolution filters, which shows that a significant improvement on the prior-art configurations can be achieved by pushing the depth to 16-19 weight layers. These findings were the basis of our ImageNet Challenge 2014 submission, where our team secured the first and the second places in the localisation and classification tracks respectively. We also show that our representations generalise well to other datasets, where they achieve state-of-the-art results. We have made our two best-performing ConvNet models publicly available to facilitate further research on the use of deep visual representations in computer vision.},
  archiveprefix = {arXiv},
  author = {Simonyan, Karen and Zisserman, Andrew},
  eprint = {1409.1556v6},
  file = {1409.1556v6.pdf},
  month = {9},
  primaryclass = {cs.CV},
  title = {Very Deep Convolutional Networks for Large-Scale Image Recognition},
  url = {http://arxiv.org/abs/1409.1556v6},
  year = {2014},
}

@inproceedings{SpeechTransforDong2018,
  author = {Dong, Linhao and Xu, Shuang and Xu, Bo},
  booktitle = {ICASSP 2018 - 2018 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
  doi = {10.1109/icassp.2018.8462506},
  journal = {2018 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
  month = {4},
  publisher = {IEEE},
  title = {Speech-Transformer: A No-Recurrence Sequence-to-Sequence Model for Speech Recognition},
  url = {http://dx.doi.org/10.1109/icassp.2018.8462506},
  venue = {Calgary, AB},
  year = {2018},
}

@article{MaximalJacobiaWiyatn2018,
  abstract = {The Jacobian-based Saliency Map Attack is a family of adversarial attack methods for fooling classification models, such as deep neural networks for image classification tasks. By saturating a few pixels in a given image to their maximum or minimum values, JSMA can cause the model to misclassify the resulting adversarial image as a specified erroneous target class. We propose two variants of JSMA, one which removes the requirement to specify a target class, and another that additionally does not need to specify whether to only increase or decrease pixel intensities. Our experiments highlight the competitive speeds and qualities of these variants when applied to datasets of hand-written digits and natural scenes.},
  archiveprefix = {arXiv},
  author = {Wiyatno, Rey and Xu, Anqi},
  eprint = {1808.07945v1},
  file = {1808.07945v1.pdf},
  month = {Aug},
  primaryclass = {cs.LG},
  title = {Maximal Jacobian-based Saliency Map Attack},
  url = {http://arxiv.org/abs/1808.07945v1},
  year = {2018},
}

@article{UnderstandingSMirand2017,
  author = {Miranda, Lester James},
  journal = {ljvmiranda921.github.io},
  title = {Understanding softmax and the negative log-likelihood"},
  url = {https://ljvmiranda921.github.io/notebook/2017/08/13/softmax-and-the-negative-log-likelihood/},
  year = {2017},
}

@article{CounteringAdveGuoC2017,
  abstract = {This paper investigates strategies that defend against adversarial-example attacks on image-classification systems by transforming the inputs before feeding them to the system. Specifically, we study applying image transformations such as bit-depth reduction, JPEG compression, total variance minimization, and image quilting before feeding the image to a convolutional network classifier. Our experiments on ImageNet show that total variance minimization and image quilting are very effective defenses in practice, in particular, when the network is trained on transformed images. The strength of those defenses lies in their non-differentiable nature and their inherent randomness, which makes it difficult for an adversary to circumvent the defenses. Our best defense eliminates 60\% of strong gray-box and 90\% of strong black-box attacks by a variety of major attack methods},
  archiveprefix = {arXiv},
  author = {Guo, Chuan and Rana, Mayank and Cisse, Moustapha and van der Maaten, Laurens},
  eprint = {1711.00117v3},
  file = {1711.00117v3.pdf},
  month = {Oct},
  primaryclass = {cs.CV},
  title = {Countering Adversarial Images using Input Transformations},
  url = {http://arxiv.org/abs/1711.00117v3},
  year = {2017},
}

@article{AdversarialExaKuraki2016,
  abstract = {Most existing machine learning classifiers are highly vulnerable to adversarial examples. An adversarial example is a sample of input data which has been modified very slightly in a way that is intended to cause a machine learning classifier to misclassify it. In many cases, these modifications can be so subtle that a human observer does not even notice the modification at all, yet the classifier still makes a mistake. Adversarial examples pose security concerns because they could be used to perform an attack on machine learning systems, even if the adversary has no access to the underlying model. Up to now, all previous work have assumed a threat model in which the adversary can feed data directly into the machine learning classifier. This is not always the case for systems operating in the physical world, for example those which are using signals from cameras and other sensors as an input. This paper shows that even in such physical world scenarios, machine learning systems are vulnerable to adversarial examples. We demonstrate this by feeding adversarial images obtained from cell-phone camera to an ImageNet Inception classifier and measuring the classification accuracy of the system. We find that a large fraction of adversarial examples are classified incorrectly even when perceived through the camera.},
  archiveprefix = {arXiv},
  author = {Kurakin, Alexey and Goodfellow, Ian and Bengio, Samy},
  eprint = {1607.02533v4},
  file = {1607.02533v4.pdf},
  month = {Jul},
  primaryclass = {cs.CV},
  title = {Adversarial examples in the physical world},
  url = {http://arxiv.org/abs/1607.02533v4},
  year = {2016},
}

@inproceedings{LabelEmbeddingAkata2013,
  abstract = {Attributes are an intermediate representation, which enables parameter sharing between classes, a must when training data is scarce. We propose to view attribute-based image classification as a label-embedding problem: each class is embedded in the space of attribute vectors. We introduce a function which measures the compatibility between an image and a label embedding. The parameters of this function are learned on a training set of labeled samples to ensure that, given an image, the correct classes rank higher than the incorrect ones. Results on the Animals With Attributes and Caltech-UCSD-Birds datasets show that the proposed framework outperforms the standard Direct Attribute Prediction baseline in a zero-shot learning scenario. The label embedding framework offers other advantages such as the ability to leverage alternative sources of information in addition to attributes (e.g. class hierarchies) or to transition smoothly from zero-shot learning to learning with large quantities of data.},
  author = {Akata, Zeynep and Perronnin, Florent and Harchaoui, Zaid and Schmid, Cordelia},
  booktitle = {2013 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
  doi = {10.1109/cvpr.2013.111},
  journal = {2013 IEEE Conference on Computer Vision and Pattern Recognition},
  month = {6},
  publisher = {IEEE},
  title = {Label-Embedding for Attribute-Based Classification},
  url = {http://dx.doi.org/10.1109/cvpr.2013.111},
  venue = {Portland, OR, USA},
  year = {2013},
}

@article{ProgressiveNeuLiuC2017,
  abstract = {We propose a new method for learning the structure of convolutional neural networks (CNNs) that is more efficient than recent state-of-the-art methods based on reinforcement learning and evolutionary algorithms. Our approach uses a sequential model-based optimization (SMBO) strategy, in which we search for structures in order of increasing complexity, while simultaneously learning a surrogate model to guide the search through structure space. Direct comparison under the same search space shows that our method is up to 5 times more efficient than the RL method of Zoph et al. (2018) in terms of number of models evaluated, and 8 times faster in terms of total compute. The structures we discover in this way achieve state of the art classification accuracies on CIFAR-10 and ImageNet.},
  archiveprefix = {arXiv},
  author = {Liu, Chenxi and Zoph, Barret and Neumann, Maxim and Shlens, Jonathon and Hua, Wei and Li, Li-Jia and Fei-Fei, Li and Yuille, Alan and Huang, Jonathan and Murphy, Kevin},
  eprint = {1712.00559v3},
  file = {1712.00559v3.pdf},
  month = {12},
  primaryclass = {cs.CV},
  title = {Progressive Neural Architecture Search},
  url = {http://arxiv.org/abs/1712.00559v3},
  year = {2017},
}

@article{SpatialTransfoJaderb2015,
  abstract = {Convolutional Neural Networks define an exceptionally powerful class of models, but are still limited by the lack of ability to be spatially invariant to the input data in a computationally and parameter efficient manner. In this work we introduce a new learnable module, the Spatial Transformer, which explicitly allows the spatial manipulation of data within the network. This differentiable module can be inserted into existing convolutional architectures, giving neural networks the ability to actively spatially transform feature maps, conditional on the feature map itself, without any extra training supervision or modification to the optimisation process. We show that the use of spatial transformers results in models which learn invariance to translation, scale, rotation and more generic warping, resulting in state-of-the-art performance on several benchmarks, and for a number of classes of transformations.},
  archiveprefix = {arXiv},
  author = {Jaderberg, Max and Simonyan, Karen and Zisserman, Andrew and Kavukcuoglu, Koray},
  eprint = {1506.02025v3},
  file = {1506.02025v3.pdf},
  month = {6},
  primaryclass = {cs.CV},
  title = {Spatial Transformer Networks},
  url = {http://arxiv.org/abs/1506.02025v3},
  year = {2015},
}

@article{ConformerConvGulati2020,
  abstract = {Recently Transformer and Convolution neural network (CNN) based models have shown promising results in Automatic Speech Recognition (ASR), outperforming Recurrent neural networks (RNNs). Transformer models are good at capturing content-based global interactions, while CNNs exploit local features effectively. In this work, we achieve the best of both worlds by studying how to combine convolution neural networks and transformers to model both local and global dependencies of an audio sequence in a parameter-efficient way. To this regard, we propose the convolution-augmented transformer for speech recognition, named Conformer. Conformer significantly outperforms the previous Transformer and CNN based models achieving state-of-the-art accuracies. On the widely used LibriSpeech benchmark, our model achieves WER of 2.1\%/4.3\% without using a language model and 1.9\%/3.9\% with an external language model on test/testother. We also observe competitive performance of 2.7\%/6.3\% with a small model of only 10M parameters.},
  archiveprefix = {arXiv},
  author = {Gulati, Anmol and Qin, James and Chiu, Chung-Cheng and Parmar, Niki and Zhang, Yu and Yu, Jiahui and Han, Wei and Wang, Shibo and Zhang, Zhengdong and Wu, Yonghui and Pang, Ruoming},
  eprint = {2005.08100v1},
  file = {2005.08100v1.pdf},
  month = {5},
  primaryclass = {eess.AS},
  title = {Conformer: Convolution-augmented Transformer for Speech Recognition},
  url = {http://arxiv.org/abs/2005.08100v1},
  year = {2020},
}

@inproceedings{WhatDoesClassDeng2010,
  abstract = {Image classification is a critical task for both humans and computers. One of the challenges lies in the large scale of the semantic space. In particular, humans can recognize tens of thousands of object classes and scenes. No computer vision algorithm today has been tested at this scale. This paper presents a study of large scale categorization including a series of challenging experiments on classification with more than 10, 000 image classes. We find that a) computational issues become crucial in algorithm design; b) conventional wisdom from a couple of hundred image categories on relative performance of different classifiers does not necessarily hold when the number of categories increases; c) there is a surprisingly strong relationship between the structure of WordNet (developed for studying language) and the difficulty of visual categorization; d) classification can be improved by exploiting the semantic hierarchy. Toward the future goal of developing automatic vision algorithms to recognize tens of thousands or even millions of image categories, we make a series of observations and arguments about dataset scale, category density, and image hierarchy.},
  address = {Berlin, Heidelberg},
  author = {Deng, Jia and Berg, Alexander C. and Li, Kai and Fei-Fei, Li},
  booktitle = {Proceedings of the 11th European Conference on Computer Vision: Part V},
  doi = {10.1007/978-3-642-15555-0\_6},
  isbn = {3642155545},
  location = {Heraklion, Crete, Greece},
  pages = {71\textendash{}84},
  publisher = {Springer-Verlag},
  series = {ECCV'10},
  title = {What Does Classifying More than 10,000 Image Categories Tell Us?},
  year = {2010},
}

@article{TransformersInKhan2021,
  abstract = {Astounding results from Transformer models on natural language tasks have intrigued the vision community to study their application to computer vision problems. Among their salient benefits, Transformers enable modeling long dependencies between input sequence elements and support parallel processing of sequence as compared to recurrent networks e.g., Long short-term memory (LSTM). Different from convolutional networks, Transformers require minimal inductive biases for their design and are naturally suited as set-functions. Furthermore, the straightforward design of Transformers allows processing multiple modalities (e.g., images, videos, text and speech) using similar processing blocks and demonstrates excellent scalability to very large capacity networks and huge datasets. These strengths have led to exciting progress on a number of vision tasks using Transformer networks. This survey aims to provide a comprehensive overview of the Transformer models in the computer vision discipline. We start with an introduction to fundamental concepts behind the success of Transformers i.e., self-attention, large-scale pre-training, and bidirectional encoding. We then cover extensive applications of transformers in vision including popular recognition tasks (e.g., image classification, object detection, action recognition, and segmentation), generative modeling, multi-modal tasks (e.g., visual-question answering, visual reasoning, and visual grounding), video processing (e.g., activity recognition, video forecasting), low-level vision (e.g., image super-resolution, image enhancement, and colorization) and 3D analysis (e.g., point cloud classification and segmentation). We compare the respective advantages and limitations of popular techniques both in terms of architectural design and their experimental value. Finally, we provide an analysis on open research directions and possible future works.},
  archiveprefix = {arXiv},
  author = {Khan, Salman and Naseer, Muzammal and Hayat, Munawar and Zamir, Syed Waqas and Khan, Fahad Shahbaz and Shah, Mubarak},
  doi = {10.1145/3505244},
  eprint = {2101.01169v5},
  file = {2101.01169v5.pdf},
  month = {1},
  primaryclass = {cs.CV},
  title = {Transformers in Vision: A Survey},
  url = {http://arxiv.org/abs/2101.01169v5},
  year = {2021},
}

@inbook{DeepLearningGoodfe2016,
  author = {Goodfellow, Ian and Bengio, Yoshua and Courville, Aaron},
  booktitle = {Deep Learning},
  chapter = {6},
  pages = {180--184},
  publisher = {MIT Press},
  title = {Deep Learning},
  url = {http://www.deeplearningbook.org},
  year = {2016},
}

@inproceedings{HyperbolicVisuLiuS2020,
  author = {Liu, Shaoteng and Chen, Jingjing and Pan, Liangming and Ngo, Chong-Wah and Chua, Tat-Seng and Jiang, Yu-Gang},
  booktitle = {2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
  doi = {10.1109/cvpr42600.2020.00929},
  journal = {2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
  month = {6},
  publisher = {IEEE},
  title = {Hyperbolic Visual Embedding Learning for Zero-Shot Recognition},
  url = {http://dx.doi.org/10.1109/cvpr42600.2020.00929},
  venue = {Seattle, WA, USA},
  year = {2020},
}

@article{ImprovingAdverChen2019,
  abstract = {Adversarial robustness has emerged as an important topic in deep learning as carefully crafted attack samples can significantly disturb the performance of a model. Many recent methods have proposed to improve adversarial robustness by utilizing adversarial training or model distillation, which adds additional procedures to model training. In this paper, we propose a new training paradigm called Guided Complement Entropy (GCE) that is capable of achieving "adversarial defense for free," which involves no additional procedures in the process of improving adversarial robustness. In addition to maximizing model probabilities on the ground-truth class like cross-entropy, we neutralize its probabilities on the incorrect classes along with a "guided" term to balance between these two terms. We show in the experiments that our method achieves better model robustness with even better performance compared to the commonly used cross-entropy training objective. We also show that our method can be used orthogonal to adversarial training across well-known methods with noticeable robustness gain. To the best of our knowledge, our approach is the first one that improves model robustness without compromising performance.},
  archiveprefix = {arXiv},
  author = {Chen, Hao-Yun and Liang, Jhao-Hong and Chang, Shih-Chieh and Pan, Jia-Yu and Chen, Yu-Ting and Wei, Wei and Juan, Da-Cheng},
  eprint = {1903.09799v3},
  file = {1903.09799v3.pdf},
  month = {3},
  primaryclass = {cs.LG},
  title = {Improving Adversarial Robustness via Guided Complement Entropy},
  url = {http://arxiv.org/abs/1903.09799v3},
  year = {2019},
}

@article{DistillationAsPapern2015,
  abstract = {Deep learning algorithms have been shown to perform extremely well on many classical machine learning problems. However, recent studies have shown that deep learning, like other machine learning techniques, is vulnerable to adversarial samples: inputs crafted to force a deep neural network (DNN) to provide adversary-selected outputs. Such attacks can seriously undermine the security of the system supported by the DNN, sometimes with devastating consequences. For example, autonomous vehicles can be crashed, illicit or illegal content can bypass content filters, or biometric authentication systems can be manipulated to allow improper access. In this work, we introduce a defensive mechanism called defensive distillation to reduce the effectiveness of adversarial samples on DNNs. We analytically investigate the generalizability and robustness properties granted by the use of defensive distillation when training DNNs. We also empirically study the effectiveness of our defense mechanisms on two DNNs placed in adversarial settings. The study shows that defensive distillation can reduce effectiveness of sample creation from 95\% to less than 0.5\% on a studied DNN. Such dramatic gains can be explained by the fact that distillation leads gradients used in adversarial sample creation to be reduced by a factor of 10\^{}30. We also find that distillation increases the average minimum number of features that need to be modified to create adversarial samples by about 800\% on one of the DNNs we tested.},
  archiveprefix = {arXiv},
  author = {Papernot, Nicolas and McDaniel, Patrick and Wu, Xi and Jha, Somesh and Swami, Ananthram},
  eprint = {1511.04508v2},
  file = {1511.04508v2.pdf},
  month = {Nov},
  primaryclass = {cs.CR},
  title = {Distillation as a Defense to Adversarial Perturbations against Deep   Neural Networks},
  url = {http://arxiv.org/abs/1511.04508v2},
  year = {2015},
}