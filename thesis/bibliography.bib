@article{1903.01182v2,
  abstract = {Learning with a primary objective, such as softmax cross entropy for classification and sequence generation, has been the norm for training deep neural networks for years. Although being a widely-adopted approach, using cross entropy as the primary objective exploits mostly the information from the ground-truth class for maximizing data likelihood, and largely ignores information from the complement (incorrect) classes. We argue that, in addition to the primary objective, training also using a complement objective that leverages information from the complement classes can be effective in improving model performance. This motivates us to study a new training paradigm that maximizes the likelihood of the groundtruth class while neutralizing the probabilities of the complement classes. We conduct extensive experiments on multiple tasks ranging from computer vision to natural language understanding. The experimental results confirm that, compared to the conventional training with just one primary objective, training also with the complement objective further improves the performance of the state-of-the-art models across all tasks. In addition to the accuracy improvement, we also show that models trained with both primary and complement objectives are more robust to single-step adversarial attacks.},
  archiveprefix = {arXiv},
  author = {Chen, Hao-Yun and Wang, Pei-Hsin and Liu, Chun-Hao and Chang, Shih-Chieh and Pan, Jia-Yu and Chen, Yu-Ting and Wei, Wei and Juan, Da-Cheng},
  eprint = {1903.01182v2},
  file = {1903.01182v2.pdf},
  month = {Mar},
  primaryclass = {cs.LG},
  title = {Complement Objective Training},
  url = {http://arxiv.org/abs/1903.01182v2},
  year = {2019},
}

@article{TheShapeOfLeVierin2021,
  abstract = {Learning curves provide insight into the dependence of a learner's generalization performance on the training set size. This important tool can be used for model selection, to predict the effect of more training data, and to reduce the computational complexity of model training and hyperparameter tuning. This review recounts the origins of the term, provides a formal definition of the learning curve, and briefly covers basics such as its estimation. Our main contribution is a comprehensive overview of the literature regarding the shape of learning curves. We discuss empirical and theoretical evidence that supports well-behaved curves that often have the shape of a power law or an exponential. We consider the learning curves of Gaussian processes, the complex shapes they can display, and the factors influencing them. We draw specific attention to examples of learning curves that are ill-behaved, showing worse learning performance with more training data. To wrap up, we point out various open problems that warrant deeper empirical and theoretical investigation. All in all, our review underscores that learning curves are surprisingly diverse and no universal model can be identified.},
  archiveprefix = {arXiv},
  author = {Viering, Tom and Loog, Marco},
  eprint = {2103.10948v2},
  file = {2103.10948v2.pdf},
  month = {Mar},
  primaryclass = {cs.LG},
  title = {The Shape of Learning Curves: a Review},
  url = {http://arxiv.org/abs/2103.10948v2},
  year = {2021},
}

@article{DeepResidualLHeKa2015,
  abstract = {Deeper neural networks are more difficult to train. We present a residual learning framework to ease the training of networks that are substantially deeper than those used previously. We explicitly reformulate the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions. We provide comprehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth. On the ImageNet dataset we evaluate residual nets with a depth of up to 152 layers---8x deeper than VGG nets but still having lower complexity. An ensemble of these residual nets achieves 3.57\% error on the ImageNet test set. This result won the 1st place on the ILSVRC 2015 classification task. We also present analysis on CIFAR-10 with 100 and 1000 layers.   The depth of representations is of central importance for many visual recognition tasks. Solely due to our extremely deep representations, we obtain a 28\% relative improvement on the COCO object detection dataset. Deep residual nets are foundations of our submissions to ILSVRC \& COCO 2015 competitions, where we also won the 1st places on the tasks of ImageNet detection, ImageNet localization, COCO detection, and COCO segmentation.},
  archiveprefix = {arXiv},
  author = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
  eprint = {1512.03385v1},
  file = {1512.03385v1.pdf},
  month = {Dec},
  primaryclass = {cs.CV},
  title = {Deep Residual Learning for Image Recognition},
  url = {http://arxiv.org/abs/1512.03385v1},
  year = {2015},
}

@article{1412.6572v3,
  abstract = {Several machine learning models, including neural networks, consistently misclassify adversarial examples---inputs formed by applying small but intentionally worst-case perturbations to examples from the dataset, such that the perturbed input results in the model outputting an incorrect answer with high confidence. Early attempts at explaining this phenomenon focused on nonlinearity and overfitting. We argue instead that the primary cause of neural networks' vulnerability to adversarial perturbation is their linear nature. This explanation is supported by new quantitative results while giving the first explanation of the most intriguing fact about them: their generalization across architectures and training sets. Moreover, this view yields a simple and fast method of generating adversarial examples. Using this approach to provide examples for adversarial training, we reduce the test set error of a maxout network on the MNIST dataset.},
  archiveprefix = {arXiv},
  author = {Goodfellow, Ian J. and Shlens, Jonathon and Szegedy, Christian},
  eprint = {1412.6572v3},
  file = {1412.6572v3.pdf},
  month = {Dec},
  primaryclass = {stat.ML},
  title = {Explaining and Harnessing Adversarial Examples},
  url = {http://arxiv.org/abs/1412.6572v3},
  year = {2014},
}

@article{DeepLearningOBarz2019,
  abstract = {Two things seem to be indisputable in the contemporary deep learning discourse: 1. The categorical cross-entropy loss after softmax activation is the method of choice for classification. 2. Training a CNN classifier from scratch on small datasets does not work well. In contrast to this, we show that the cosine loss function provides significantly better performance than cross-entropy on datasets with only a handful of samples per class. For example, the accuracy achieved on the CUB-200-2011 dataset without pre-training is by 30\% higher than with the cross-entropy loss. Further experiments on other popular datasets confirm our findings. Moreover, we demonstrate that integrating prior knowledge in the form of class hierarchies is straightforward with the cosine loss and improves classification performance further.},
  archiveprefix = {arXiv},
  author = {Barz, Bj\"{o}rn and Denzler, Joachim},
  eprint = {1901.09054v2},
  file = {1901.09054v2.pdf},
  month = {Jan},
  note = {2020 IEEE Winter Conference on Applications of Computer Vision   (WACV), Snowmass Village, CO, USA, 2020},
  primaryclass = {cs.LG},
  title = {Deep Learning on Small Datasets without Pre-Training using Cosine Loss},
  url = {http://arxiv.org/abs/1901.09054v2},
  year = {2019},
}

@inproceedings{DeviseADeepFrome2013,
  abstract = {Modern visual recognition systems are often limited in their ability to scale to large numbers of object categories. This limitation is in part due to the increasing difficulty of acquiring sufficient training data in the form of labeled images as the number of object categories grows. One remedy is to leverage data from other sources - such as text data - both to train visual models and to constrain their predictions. In this paper we present a new deep visual-semantic embedding model trained to identify visual objects using both labeled image data as well as semantic information gleaned from unannotated text. We demonstrate that this model matches state-of-the-art performance on the 1000-class ImageNet object recognition challenge while making more semantically reasonable errors, and also show that the semantic information can be exploited to make predictions about tens of thousands of image labels not observed during training. Semantic knowledge improves such zero-shot predictions achieving hit rates of up to 18\% across thousands of novel labels never seen by the visual model.},
  address = {Red Hook, NY, USA},
  author = {Frome, Andrea and Corrado, Greg S. and Shlens, Jonathon and Bengio, Samy and Dean, Jeffrey and Ranzato, Marc'Aurelio and Mikolov, Tomas},
  booktitle = {Proceedings of the 26th International Conference on Neural Information Processing Systems - Volume 2},
  location = {Lake Tahoe, Nevada},
  pages = {2121\textendash{}2129},
  publisher = {Curran Associates Inc.},
  series = {NIPS'13},
  title = {DeViSE: A Deep Visual-Semantic Embedding Model},
  year = {2013},
}

@article{1911.07257v1,
  abstract = {Label hierarchies widely exist in many vision-related problems, ranging from explicit label hierarchies existed in image classification to latent label hierarchies existed in semantic segmentation. Nevertheless, state-of-the-art methods often deploy cross-entropy loss that implicitly assumes class labels to be exclusive and thus independence from each other. Motivated by the fact that classes from the same parental category usually share certain similarity, we design a new training diagram called Hierarchical Complement Objective Training (HCOT) that leverages the information from label hierarchy. HCOT maximizes the probability of the ground truth class, and at the same time, neutralizes the probabilities of rest of the classes in a hierarchical fashion, making the model take advantage of the label hierarchy explicitly. The proposed HCOT is evaluated on both image classification and semantic segmentation tasks. Experimental results confirm that HCOT outperforms state-of-the-art models in CIFAR-100, ImageNet-2012, and PASCAL-Context. The study further demonstrates that HCOT can be applied on tasks with latent label hierarchies, which is a common characteristic in many machine learning tasks.},
  archiveprefix = {arXiv},
  author = {Chen, Hao-Yun and Tsai, Li-Huang and Chang, Shih-Chieh and Pan, Jia-Yu and Chen, Yu-Ting and Wei, Wei and Juan, Da-Cheng},
  eprint = {1911.07257v1},
  file = {1911.07257v1.pdf},
  month = {Nov},
  primaryclass = {cs.CV},
  title = {Learning with Hierarchical Complement Objective},
  url = {http://arxiv.org/abs/1911.07257v1},
  year = {2019},
}

@article{BeyondOneHotPerott2023,
  abstract = {Images are loaded with semantic information that pertains to real-world ontologies: dog breeds share mammalian similarities, food pictures are often depicted in domestic environments, and so on. However, when training machine learning models for image classification, the relative similarities amongst object classes are commonly paired with one-hot-encoded labels. According to this logic, if an image is labelled as 'spoon', then 'tea-spoon' and 'shark' are equally wrong in terms of training loss. To overcome this limitation, we explore the integration of additional goals that reflect ontological and semantic knowledge, improving model interpretability and trustworthiness. We suggest a generic approach that allows to derive an additional loss term starting from any kind of semantic information about the classification label. First, we show how to apply our approach to ontologies and word embeddings, and discuss how the resulting information can drive a supervised learning process. Second, we use our semantically enriched loss to train image classifiers, and analyse the trade-offs between accuracy, mistake severity, and learned internal representations. Finally, we discuss how this approach can be further exploited in terms of explainability and adversarial robustness. Code repository: https://github.com/S1M0N38/semantic-encodings},
  archiveprefix = {arXiv},
  author = {Perotti, Alan and Bertolotto, Simone and Pastor, Eliana and Panisson, Andr\'{e}},
  eprint = {2308.00607v1},
  file = {2308.00607v1.pdf},
  month = {Aug},
  primaryclass = {cs.CV},
  title = {Beyond One-Hot-Encoding: Injecting Semantics to Drive Image Classifiers},
  url = {http://arxiv.org/abs/2308.00607v1},
  year = {2023},
}

@article{HierarchyBasedBarz2018,
  abstract = {Deep neural networks trained for classification have been found to learn powerful image representations, which are also often used for other tasks such as comparing images w.r.t. their visual similarity. However, visual similarity does not imply semantic similarity. In order to learn semantically discriminative features, we propose to map images onto class embeddings whose pair-wise dot products correspond to a measure of semantic similarity between classes. Such an embedding does not only improve image retrieval results, but could also facilitate integrating semantics for other tasks, e.g., novelty detection or few-shot learning. We introduce a deterministic algorithm for computing the class centroids directly based on prior world-knowledge encoded in a hierarchy of classes such as WordNet. Experiments on CIFAR-100, NABirds, and ImageNet show that our learned semantic image embeddings improve the semantic consistency of image retrieval results by a large margin.},
  archiveprefix = {arXiv},
  author = {Barz, Bj\"{o}rn and Denzler, Joachim},
  doi = {10.1109/WACV.2019.00073},
  eprint = {1809.09924v4},
  file = {1809.09924v4.pdf},
  month = {Sep},
  note = {2019 IEEE Winter Conference on Applications of Computer Vision   (WACV), Waikoloa Village, HI, USA, 2019, pp. 638-647},
  primaryclass = {cs.CV},
  title = {Hierarchy-based Image Embeddings for Semantic Image Retrieval},
  url = {http://arxiv.org/abs/1809.09924v4},
  year = {2018},
}

@article{GradientBasedLecun1998,
  author = {Lecun, Y. and Bottou, L. and Bengio, Y. and Haffner, P.},
  doi = {10.1109/5.726791},
  issue = {11},
  journal = {Proceedings of the IEEE},
  pages = {2278--2324},
  publisher = {Institute of Electrical and Electronics Engineers (IEEE)},
  title = {Gradient-based learning applied to document recognition},
  url = {http://dx.doi.org/10.1109/5.726791},
  volume = {86},
  year = {1998},
}

@inproceedings{UnderstandingOLiuY2010,
  abstract = {Clustering validation has long been recognized as one of the vital issues essential to the success of clustering applications. In general, clustering validation can be categorized into two classes, external clustering validation and internal clustering validation. In this paper, we focus on internal clustering validation and present a detailed study of 11 widely used internal clustering validation measures for crisp clustering. From five conventional aspects of clustering, we investigate their validation properties. Experiment results show that S\_Dbw is the only internal validation measure which performs well in all five aspects, while other measures have certain limitations in different application scenarios.},
  author = {Liu, Yanchi and Li, Zhongmou and Xiong, Hui and Gao, Xuedong and Wu, Junjie},
  booktitle = {2010 IEEE 10th International Conference on Data Mining (ICDM)},
  doi = {10.1109/icdm.2010.35},
  journal = {2010 IEEE International Conference on Data Mining},
  month = {12},
  publisher = {IEEE},
  title = {Understanding of Internal Clustering Validation Measures},
  url = {http://dx.doi.org/10.1109/icdm.2010.35},
  venue = {Sydney, Australia},
  year = {2010},
}

@inproceedings{ImagenetALarDeng2009,
  author = {Deng, Jia and Dong, Wei and Socher, Richard and Li, Li-Jia and Kai Li, None and Li Fei-Fei, None},
  booktitle = {2009 IEEE Computer Society Conference on Computer Vision and Pattern Recognition Workshops (CVPR Workshops)},
  doi = {10.1109/cvpr.2009.5206848},
  journal = {2009 IEEE Conference on Computer Vision and Pattern Recognition},
  month = {6},
  publisher = {IEEE},
  title = {ImageNet: A large-scale hierarchical image database},
  url = {http://dx.doi.org/10.1109/cvpr.2009.5206848},
  venue = {Miami, FL},
  year = {2009},
}

@article{ImagenetClassiKrizhe2017,
  abstract = {We trained a large, deep convolutional neural network to classify the 1.2 million high-resolution images in the ImageNet LSVRC-2010 contest into the 1000 different classes. On the test data, we achieved top-1 and top-5 error rates of 37.5\% and 17.0\%, respectively, which is considerably better than the previous state-of-the-art. The neural network, which has 60 million parameters and 650,000 neurons, consists of five convolutional layers, some of which are followed by max-pooling layers, and three fully connected layers with a final 1000-way softmax. To make training faster, we used non-saturating neurons and a very efficient GPU implementation of the convolution operation. To reduce overfitting in the fully connected layers we employed a recently developed regularization method called "dropout" that proved to be very effective. We also entered a variant of this model in the ILSVRC-2012 competition and achieved a winning top-5 test error rate of 15.3\%, compared to 26.2\% achieved by the second-best entry.},
  author = {Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey E.},
  doi = {10.1145/3065386},
  issue = {6},
  journal = {Communications of the ACM},
  language = {en},
  month = {5},
  pages = {84--90},
  publisher = {Association for Computing Machinery (ACM)},
  title = {ImageNet classification with deep convolutional neural networks},
  url = {http://dx.doi.org/10.1145/3065386},
  volume = {60},
  year = {2017},
}

@article{LearningHierarGarg2022,
  abstract = {Label hierarchies are often available apriori as part of biological taxonomy or language datasets WordNet. Several works exploit these to learn hierarchy aware features in order to improve the classifier to make semantically meaningful mistakes while maintaining or reducing the overall error. In this paper, we propose a novel approach for learning Hierarchy Aware Features (HAF) that leverages classifiers at each level of the hierarchy that are constrained to generate predictions consistent with the label hierarchy. The classifiers are trained by minimizing a Jensen-Shannon Divergence with target soft labels obtained from the fine-grained classifiers. Additionally, we employ a simple geometric loss that constrains the feature space geometry to capture the semantic structure of the label space. HAF is a training time approach that improves the mistakes while maintaining top-1 error, thereby, addressing the problem of cross-entropy loss that treats all mistakes as equal. We evaluate HAF on three hierarchical datasets and achieve state-of-the-art results on the iNaturalist-19 and CIFAR-100 datasets. The source code is available at https://github.com/07Agarg/HAF},
  archiveprefix = {arXiv},
  author = {Garg, Ashima and Sani, Depanshu and Anand, Saket},
  eprint = {2207.12646v1},
  file = {2207.12646v1.pdf},
  month = {Jul},
  primaryclass = {cs.CV},
  title = {Learning Hierarchy Aware Features for Reducing Mistake Severity},
  url = {http://arxiv.org/abs/2207.12646v1},
  year = {2022},
}

@article{GoingDeeperWiSzeged2014,
  abstract = {We propose a deep convolutional neural network architecture codenamed "Inception", which was responsible for setting the new state of the art for classification and detection in the ImageNet Large-Scale Visual Recognition Challenge 2014 (ILSVRC 2014). The main hallmark of this architecture is the improved utilization of the computing resources inside the network. This was achieved by a carefully crafted design that allows for increasing the depth and width of the network while keeping the computational budget constant. To optimize quality, the architectural decisions were based on the Hebbian principle and the intuition of multi-scale processing. One particular incarnation used in our submission for ILSVRC 2014 is called GoogLeNet, a 22 layers deep network, the quality of which is assessed in the context of classification and detection.},
  archiveprefix = {arXiv},
  author = {Szegedy, Christian and Liu, Wei and Jia, Yangqing and Sermanet, Pierre and Reed, Scott and Anguelov, Dragomir and Erhan, Dumitru and Vanhoucke, Vincent and Rabinovich, Andrew},
  eprint = {1409.4842v1},
  file = {1409.4842v1.pdf},
  month = {Sep},
  primaryclass = {cs.CV},
  title = {Going Deeper with Convolutions},
  url = {http://arxiv.org/abs/1409.4842v1},
  year = {2014},
}

@article{1912.09393v2,
  abstract = {Deep neural networks have improved image classification dramatically over the past decade, but have done so by focusing on performance measures that treat all classes other than the ground truth as equally wrong. This has led to a situation in which mistakes are less likely to be made than before, but are equally likely to be absurd or catastrophic when they do occur. Past works have recognised and tried to address this issue of mistake severity, often by using graph distances in class hierarchies, but this has largely been neglected since the advent of the current deep learning era in computer vision. In this paper, we aim to renew interest in this problem by reviewing past approaches and proposing two simple modifications of the cross-entropy loss which outperform the prior art under several metrics on two large datasets with complex class hierarchies: tieredImageNet and iNaturalist'19.},
  archiveprefix = {arXiv},
  author = {Bertinetto, Luca and Mueller, Romain and Tertikas, Konstantinos and Samangooei, Sina and Lord, Nicholas A.},
  eprint = {1912.09393v2},
  file = {1912.09393v2.pdf},
  month = {Dec},
  primaryclass = {cs.CV},
  title = {Making Better Mistakes: Leveraging Class Hierarchies with Deep Networks},
  url = {http://arxiv.org/abs/1912.09393v2},
  year = {2019},
}

@article{1904.00887v4,
  abstract = {Deep neural networks are vulnerable to adversarial attacks, which can fool them by adding minuscule perturbations to the input images. The robustness of existing defenses suffers greatly under white-box attack settings, where an adversary has full knowledge about the network and can iterate several times to find strong perturbations. We observe that the main reason for the existence of such perturbations is the close proximity of different class samples in the learned feature space. This allows model decisions to be totally changed by adding an imperceptible perturbation in the inputs. To counter this, we propose to class-wise disentangle the intermediate feature representations of deep networks. Specifically, we force the features for each class to lie inside a convex polytope that is maximally separated from the polytopes of other classes. In this manner, the network is forced to learn distinct and distant decision regions for each class. We observe that this simple constraint on the features greatly enhances the robustness of learned models, even against the strongest white-box attacks, without degrading the classification performance on clean images. We report extensive evaluations in both black-box and white-box attack scenarios and show significant gains in comparison to state-of-the art defenses.},
  archiveprefix = {arXiv},
  author = {Mustafa, Aamir and Khan, Salman and Hayat, Munawar and Goecke, Roland and Shen, Jianbing and Shao, Ling},
  eprint = {1904.00887v4},
  file = {1904.00887v4.pdf},
  month = {Apr},
  primaryclass = {cs.CV},
  title = {Adversarial Defense by Restricting the Hidden Space of Deep Neural Networks},
  url = {http://arxiv.org/abs/1904.00887v4},
  year = {2019},
}

@article{VeryDeepConvoSimony2014,
  abstract = {In this work we investigate the effect of the convolutional network depth on its accuracy in the large-scale image recognition setting. Our main contribution is a thorough evaluation of networks of increasing depth using an architecture with very small (3x3) convolution filters, which shows that a significant improvement on the prior-art configurations can be achieved by pushing the depth to 16-19 weight layers. These findings were the basis of our ImageNet Challenge 2014 submission, where our team secured the first and the second places in the localisation and classification tracks respectively. We also show that our representations generalise well to other datasets, where they achieve state-of-the-art results. We have made our two best-performing ConvNet models publicly available to facilitate further research on the use of deep visual representations in computer vision.},
  archiveprefix = {arXiv},
  author = {Simonyan, Karen and Zisserman, Andrew},
  eprint = {1409.1556v6},
  file = {1409.1556v6.pdf},
  month = {Sep},
  primaryclass = {cs.CV},
  title = {Very Deep Convolutional Networks for Large-Scale Image Recognition},
  url = {http://arxiv.org/abs/1409.1556v6},
  year = {2014},
}

@article{1903.09799v3,
  abstract = {Adversarial robustness has emerged as an important topic in deep learning as carefully crafted attack samples can significantly disturb the performance of a model. Many recent methods have proposed to improve adversarial robustness by utilizing adversarial training or model distillation, which adds additional procedures to model training. In this paper, we propose a new training paradigm called Guided Complement Entropy (GCE) that is capable of achieving "adversarial defense for free," which involves no additional procedures in the process of improving adversarial robustness. In addition to maximizing model probabilities on the ground-truth class like cross-entropy, we neutralize its probabilities on the incorrect classes along with a "guided" term to balance between these two terms. We show in the experiments that our method achieves better model robustness with even better performance compared to the commonly used cross-entropy training objective. We also show that our method can be used orthogonal to adversarial training across well-known methods with noticeable robustness gain. To the best of our knowledge, our approach is the first one that improves model robustness without compromising performance.},
  archiveprefix = {arXiv},
  author = {Chen, Hao-Yun and Liang, Jhao-Hong and Chang, Shih-Chieh and Pan, Jia-Yu and Chen, Yu-Ting and Wei, Wei and Juan, Da-Cheng},
  eprint = {1903.09799v3},
  file = {1903.09799v3.pdf},
  month = {Mar},
  primaryclass = {cs.LG},
  title = {Improving Adversarial Robustness via Guided Complement Entropy},
  url = {http://arxiv.org/abs/1903.09799v3},
  year = {2019},
}

@article{EfficientnetRTanM2019,
  abstract = {Convolutional Neural Networks (ConvNets) are commonly developed at a fixed resource budget, and then scaled up for better accuracy if more resources are available. In this paper, we systematically study model scaling and identify that carefully balancing network depth, width, and resolution can lead to better performance. Based on this observation, we propose a new scaling method that uniformly scales all dimensions of depth/width/resolution using a simple yet highly effective compound coefficient. We demonstrate the effectiveness of this method on scaling up MobileNets and ResNet.   To go even further, we use neural architecture search to design a new baseline network and scale it up to obtain a family of models, called EfficientNets, which achieve much better accuracy and efficiency than previous ConvNets. In particular, our EfficientNet-B7 achieves state-of-the-art 84.3\% top-1 accuracy on ImageNet, while being 8.4x smaller and 6.1x faster on inference than the best existing ConvNet. Our EfficientNets also transfer well and achieve state-of-the-art accuracy on CIFAR-100 (91.7\%), Flowers (98.8\%), and 3 other transfer learning datasets, with an order of magnitude fewer parameters. Source code is at https://github.com/tensorflow/tpu/tree/master/models/official/efficientnet.},
  archiveprefix = {arXiv},
  author = {Tan, Mingxing and Le, Quoc V.},
  eprint = {1905.11946v5},
  file = {1905.11946v5.pdf},
  month = {May},
  note = {International Conference on Machine Learning, 2019},
  primaryclass = {cs.LG},
  title = {EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks},
  url = {http://arxiv.org/abs/1905.11946v5},
  year = {2019},
}

@article{Efficientnetv2TanM2021,
  abstract = {This paper introduces EfficientNetV2, a new family of convolutional networks that have faster training speed and better parameter efficiency than previous models. To develop this family of models, we use a combination of training-aware neural architecture search and scaling, to jointly optimize training speed and parameter efficiency. The models were searched from the search space enriched with new ops such as Fused-MBConv. Our experiments show that EfficientNetV2 models train much faster than state-of-the-art models while being up to 6.8x smaller.   Our training can be further sped up by progressively increasing the image size during training, but it often causes a drop in accuracy. To compensate for this accuracy drop, we propose to adaptively adjust regularization (e.g., dropout and data augmentation) as well, such that we can achieve both fast training and good accuracy.   With progressive learning, our EfficientNetV2 significantly outperforms previous models on ImageNet and CIFAR/Cars/Flowers datasets. By pretraining on the same ImageNet21k, our EfficientNetV2 achieves 87.3\% top-1 accuracy on ImageNet ILSVRC2012, outperforming the recent ViT by 2.0\% accuracy while training 5x-11x faster using the same computing resources. Code will be available at https://github.com/google/automl/tree/master/efficientnetv2.},
  archiveprefix = {arXiv},
  author = {Tan, Mingxing and Le, Quoc V.},
  eprint = {2104.00298v3},
  file = {2104.00298v3.pdf},
  month = {Apr},
  note = {International Conference on Machine Learning, 2021},
  primaryclass = {cs.CV},
  title = {EfficientNetV2: Smaller Models and Faster Training},
  url = {http://arxiv.org/abs/2104.00298v3},
  year = {2021},
}