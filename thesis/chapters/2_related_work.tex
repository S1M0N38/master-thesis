\chapter{Related Work}
\label{ch:related-work}

\section{Image Classification with Neural Network}
\label{sec:image-classification-with-neural-network}

One of the first instances where Neural Network were employed for image
classification was in 1998 by Lecun et al.~\cite{GradientBasedLecun1998}. Their
model, LeNet-5, was able to identify handwritten digit with higher accuracy
than previous methods which relied on manual features extraction such edges
detection and hard-coded pattern recognition. A raw image of a digit is passed
as input to LeNet and the model spits out a number from 0 to 9. Some
foundational ideas were at the core of the architecture (convolutional
operations, gradient based optimization, pooling layers) stood the test of time
and proved to be highly successful.

Fast forwarding to 2012. AlexNet~\cite{ImagenetClassiKrizhe2017}, a Convolution
Neural Network (CNN) similar in spirit to LeNet-5, annihilated competitions at
ImageNet Large Scale Visual Recognition Challenge~\cite{ImagenetALarDeng2009}
(ILSVRC) with a whopping 10.8 \% gap on the runner-up in top-5 error metric.
This result proved the potential of CNNs to process bigger color images (224 ×
224 × 3) and this sparked interest in the field which abandoned the idea of
manual features extraction in favour of the end-to-end approach.

Researchers explored various moving part of CNNs architecture: depth of the
model, optimal kernel size, number of filters, pooling operations.
\cite{VeryDeepConvoSimony2014,GoingDeeperWiSzeged2014} focus on increasing the
depth of the models and \cite{GoingDeeperWiSzeged2014} was able to do so by the
introduction of a highly efficient Inception module which were able to increase
the depth while keeping the FLOPs under control. Efficiency at inference time is
crucial for embedding models into resource constrained device such smartphone or
IOTs~\cite{SqueezenetAleIandol2016, MnasnetPlatfoTanM2018,
MobilenetsEffHoward2017}.

A problem that arose with the increase in depth was the so-called ``vanishing
gradient''. The gradient of the loss function with respect to the model's
parameters in the early layers tends to zero and this affect the ability of the
optimizer to update the weights towards a local minimum. Residual connections,
introduced in ResNet~\cite{DeepResidualLHeKa2015, IdentityMappinHeKa2016}, are
shortcuts in the network architecture that circumvent the problem allowing the
gradient to flow backwards effectively, so deeper models can be trained.
Connections between layers, i.e. the topology of the network, was further
explored by~\cite{DualPathNetwoChen2017, DenselyConnectHuang2016} and prior
using evolutionary methods by~\cite{DesigningNeuraMiller1989,
EvolvingNeuralStanle2002}.

Neural Architecture Search was employed to search for the width and the depth
of a model given a basic building block~\cite{ProgressiveNeuLiuC2017,
GeneticCnnXieL2017, LargeScaleEvoReal2017, RegularizedEvoReal2018}. Using
different building blocks and optimizing for different budget types (e.g.
FLOPs, number of parameters, inference time) various architecture were
produced. EfficientNets~\cite{EfficientnetRTanM2019, Efficientnetv2TanM2021}
are a well-known example of such approach that make use of
Squeeze-and-Excitation module~\cite{SqueezeAndExcHuJi2017} as basic component.

Stacks of convolution blocks followed by a fully connected layers start to
reach the limit regarding classification accuracy. CNNs were equipped with an
attention mechanism be able to effectively process global features in an image
defining hybrid architecture~\cite{SpatialTransfoJaderb2015,
LookAndThinkCaoC2015, ShowAttendAnXuKe2015, ScaCnnSpatiaChen2016}. Recently
interest shifted towards Vision Transformers
(ViT)~\cite{AnImageIsWorDosovi2020}, architecture that use solely attention
mechanism for features extraction which are able to achieve state-of-the-art
results in some well-established image classification
problems~\cite{TransformersInKhan2021, ASurveyOnVisHanK2023}. ViT required to
be trained of huge datasets to achieve competitive performance and, at the
moment of the writing, cannot run on consumer and low power devices due to high
computational cost.

\section{Semantic Information Representations}
\label{sec:semantic-information-sources}
Image classification consists in assigning a label to an image given a fixed
set of labels. Without any additional information about labels they are just a
set of symbols with no structure nor relationship between them. On the other
hand having richer information on labels turns out to be beneficial for model
interpretability~\cite{ImprovingInterDong2017}, image
summarization~\cite{SemanticImagePasini2022} and image classification as
well~\cite{MakingBetterMBertin2019}. Assuming that label a label of an image is
an English word indicating what is depicted in the image, we can obtain extra
information about a label by various sources which produce different representation.

\paragraph{Hierarchies (trees)}
\label{par:hierarchies-tree}
WordNet~\cite{WordnetMi1995} is a lexical database for the English language
where words are linked together by semantic relations (synonyms, hyponyms and
meronyms) resulting in a graph structure. The aforementioned ImageNet database
was build using WordNet as a source of labels, so information from WordNet
could be exploited when training a model on ImageNet. Other examples of
semantically enriched datasets include
iNaturalist~\cite{TheInaturalistHorn2017} and
CIFAR-100~\cite{LearningMultipKrizhe2009}. Given a graph structure, it can be
pruned in such a way to obtain a rooted tree, that is a connected acyclic
undirected graph with one vertex as root. Moreover, we can assign a natural
orientation to edges towards (or away from) the root. The resulting data
structure can be used to represent hierarchical organised knowledge such
ontologies or taxonomies.

\paragraph{Embeddings (vectors)}
\label{par:embeddings-vectors}
The majority of times labels are just a set of words with no structure between
so semantic structure has to be constructed from bare words. A flexible
approach is to resort to internal representation of a pre-trained language
models (LM) commonly knows as word embeddings~\cite{BeyondWordEmbIncitt2023}.
The extracted semantic structure is organised as a vector space where words are
represented as real-valued vectors. The input to a LM can be the raw label or a
detailed description of the label. Moreover, such a description can be written
by human experts, scrape from the web or generated by a LM itself.\\

These two approaches produce different representations of semantic information
(tree vs vectors) so exist different ways to inject it into a model. Moreover
they can be injected in differnt part of a model: directly into labels, in the
loss function or by designing a custom architecture.

\section{Semantic Information Injection (Hierarchies)}

\section{Semantic Information Injection (Embeddings)}

% Custom Labels Encoding
Adding semantic information directly at labels level requires to find an
encoding capable of representing the meaningful relationship described by
hierarchy tree. We use the term \emph{hierarchical encoding} to refer to
an encoding of labels that is able to represent the hierarchical relationship.
The most straightforward way to produce a hierarchical encoding is to define a
notion of similarity in the tree structure and port it to the encoding. That is
two labels similar in the hierarchical tree have similar encoding. The various
proposed approaches differs by the definition of similarity and the derivation
of the encoding. \cite{HierarchyBasedBarz2018} solve systems of equations
requiring that the dot product of two encoding is proportional to the
similarity in the hierarchical tree. \cite{MakingBetterMBertin2019} apply the
softmax function to the rows of pairwise similarity matrix derived from the
hierarchy. \cite{BeyondOneHotPerott2023} followed similar strategy but
rescale intervals instead of using softmax and combine the result with the
standard one-hot encoding.

% Custom Loss
Another approach consists in defining a custom loss function that takes into
account the relationship between labels. As previously, it can be employed an

In a series of works~\cite{ComplementObjeChen2019, ImprovingAdverChen2019,
LearningWithHChen2019}, Chen et al. introduce complement entropy loss function
and its hierarchical version. They test models robusteness against various
adversarial attacks.

% Custom Architecture

\section{Why Semantic Information Injection}
\label{sec:why-semantic-information-injection}

Make use of semantic information in computer vision is not a new idea. In fact,
it was an active area of research before the advent of deep learning.
Researchers employed it to improve models classification
accuracy~\cite{ScalableRecognNister2006, SemanticHierarMarsza2007,
ExploitingObjeZweig2007, LearningMultipKrizhe2009} and to cope with a lot of
classes~\cite{WhatDoesClassDeng2010}. Accuracy was dramatically improved with
the introduction of CNNs and the interest in semantic information injection
faded. The recent revamped interest is concerned with different challenges:
Zero-Shot-Learning, fine-grained classification, model explainability and
quality of errors.

% Zero-Shot-Learning
In image classification, Zero-Shot-Learning (ZSL) is a collection of techniques
where a model is trained to recognize classes that were not present during
training~\cite{AnIntroductionSoysal2020}. This can be achieved by leveraging
the capability of other pre-trained models and instilling such knowledge into
the image classification model. DeViSE~\cite{DeviseADeepFrome2013} use latent
information extracted from big text corpus, Wikipedia, to build capable ZSL
model. The key idea was the use of the recently introduced learnable words
embeddings~\cite{EfficientEstimMikolo2013, DistributedRepMikolo2013}, later
patented by Google as Word2Vec~\cite{ComputingNumerMikolo2013}.

% TODO: ZSL (Hybrid approach: hierarchy + word embeddings)
Lie et al.~\cite{HyperbolicVisuLiuS2020} improved over DeViSE by proposing a
hybrid approach that make use of word embeddings through GloVe and hierarchical
information from WordNet. All of which defined in hyperbolic space that has
been shown to be particularly suitable for modeling hierarchical
data\cite{PoincareEmbeddNickel2017, HyperbolicEntaGanea2018}
% Fine grained classification

% Recently xAI

% Quality of errors



% TODO:
% Loss function
% Labels
% Architecture

% TODO: Fine Grtained Classification / Large number of classes
% Your Flamingo is my bird:
%   - Fine Grained Visual Classification
%   - CNN as feature extractor
%   - Multi-head for different granularity levels
%   - Disentanglement of classifier and use of gradient blocking gate


% Hyperbolic Visual Embedding Learning for Zero-Shot Recognition:
%   - Zero Shot Learning context
%   - represent embeddengs in hyperbolic space
%   - use WordNet and Wikipedia corpus as semantic information
%   - Similar but improve Devise
%


% Talk a bit about cutting edge research and multimodal learning
Transformer architecture~\cite{AttentionIsAlVaswan2017} was introduced in the
context of Natural Language Processing. They proved to be a general-purpose
differentiable computer: expressive (in the forward pass), optimizable (via
backpropagation and gradient descent) and efficient (highly
parallelizable)\footnote{Paraphrasing a post on $\mathbb{X}$ by Andrej
Karpathy, a prominent figure in the field.}. Then they were adapted for
processing images~\cite{ImageTransformParmar2018, EndToEndObjeCarion2020,
AnImageIsWorDosovi2020}, videos~\cite{VivitAVideoArnab2021,
TemporalContexShao2020}, audio~\cite{NeuralSpeechSLiNa2018,
SpeechTransforDong2018, ConformerConvGulati2020}. Now the cutting edge research
focus on processing different inputs types coming from different sources trying
to exploit the correlation between them. Those model are called multimodal
models~\cite{ASurveyOnMulYinS2023}.

\section{Adversarial Attacks}
\label{sec:adversarial-attacks}

% TODO:

% GoogLeNet:
% - presented at ILSVRC 2014 outperform competitors
% - start to keep an eye on efficiency (# params and FLOPs)
% - focus on block design (inception)
% - v2 add batch normalization
% - v3 imporoved inception block
% - v4 add residual connections
% VGGNet Deep is better
% GoogLeNet and VGGNet popularized the idea of going deeper using fix block (e.g. inceptions)
% ResNet introduce the idea of skip connections
% EfficientNet formalized scaling direcitons width vs depth for Squeeze-Extitaion block (SENet)
% RegNet improve skip connections by introduciong regulator module hybrid approach.

% Tweet by Andrej Karpathy
% The Transformer is a magnificient neural network architecture because it is a
% general-purpose differentiable computer. It is simultaneously:
% 1) expressive (in the forward pass)
% 2) optimizable (via backpropagation+gradient descent)
% 3) efficient (high parallelism compute graph)




