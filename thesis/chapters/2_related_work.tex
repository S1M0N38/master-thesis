\chapter{Related Work}
\label{ch:related-work}

\section{Image Classification with Neural Network}
\label{sec:image-classification-with-neural-network}

One of the first instances where Neural Network were employed for image
classification was in 1998 by Lecun et al.~\cite{GradientBasedLecun1998}. Their
model, LeNet-5, was able to indentify handwritten digit with higher accuracy
than previous methods which relyed on manual fetures extraction such edges
detection and hardcoded pattern recognition. A raw image of a digit is passed
as input to LeNet and the model spits out a number from 0 to 9. Some
foundational ideas were at the core of the architecture (convolutional
operations, gradient based optimization, pooling layers) stood the test of time
and proved to be highly sucessful.

Fastforwanding to 2012. AlexNet~\cite{ImagenetClassiKrizhe2017}, a Convolution
Neural Network (CNN) similar in spirit to LeNet-5, annihilated competitions at
ImageNet Large Scale Visual Recognition Challenge~\cite{ImagenetALarDeng2009}
(ILSVRC) with a whopping 10.8 \% gap on the runner up in top-5 error metric.
This result proved the potential of CNNs to process bigger color images (224 x
224 x 3) and this sparked interest in the field which abbandoned the idea of
manual featueres extraction in favour of the end-to-end approach.

Researchers explored various moving part of CNNs architecture: depth of the
model, optimal kernel size, number of filters, pooling operations.
\cite{VeryDeepConvoSimony2014,GoingDeeperWiSzeged2014} focus on increasing the
depth of the models and \cite{GoingDeeperWiSzeged2014} was able to do so by the
introduction of a highly efficient Inception module which were able to increase
the depth while keepnig the FLOPs undercontrol. Efficiency at inference time is
crucial for embedding models into resource constrained device such smartphone or
IOTs~\cite{SqueezenetAleIandol2016, MnasnetPlatfoTanM2018,
MobilenetsEffHoward2017}.

A problem that arose with the increase in depth was the so called the
``vanishing gradient''. The gradient of the loss function with respect to the
model's parameters in the early layers tends to zero and this affect the
abitily of the optimizer to update the weights towaerds a local minimum.
Residual connections, introduced in ResNet~\cite{DeepResidualLHeKa2015}, are
shortcuts in the network architecture that circumvent the problem allowing the
gradient to flow backwards effectively so deeper models can be trained.

Neural Architecture Search was employed to witdh and depth of a model given a
basic building block. Using different building blocks and optimizing for
different budget types (e.g. FLOPs, number of parameters, inference time)
various architecture were produced. EfficientNets~\cite{EfficientnetRTanM2019,
Efficientnetv2TanM2021} are a well-known example of such approach that make use
of Squeeze-Extitaion module~\cite{SqueezeAndExcHuJi2017} as basic component.

Stacked of convolution blocks followed by a fully connected layers start to
reach the limit regarding classification accuracy. CNN were equipted with an
attention mechanism be able to effectively process global fetures in a image
defining hybrid architecture (TODO add refs). Recently interest shifted towards
Vision Transformers (ViT), architecture that use soly attention mechanism for
features extraction which were able to achive state of the art results in some
well-established image classification problems. ViT required to be trained of
huge datasets to achive competitive performance and, at the moment of the
writing cannot run on consumer and low power devices due to the high
computational cost.


\section{Semantic Information Sources}
\label{sec:semantic-information-sources}

% TODO:


\section{Semantic Information Injection}
\label{sec:semantic-information-injection}

% TODO:

% Talk a bit about cutting edge research and multimodal learning
Transformer architecture~\cite{AttentionIsAlVaswan2017} introduce in the context
of Natural Language Processing. They proved to be a general-purpose
differentiable computer: expressive (in the forward pass), optimizable (via
backpropagation and gradient descent) and efficient (high
parallelism)~\footnote{Paraphrasing a post on $\mathbb{X}$ by Andrej Karpathy,
a prominet figure in the field.}. Then they where adapted for image processing
(ViT) and now the cutting edge reseach focus on processing inputs of different
type (e.g. images, videos, audio, text) exploting shared information.

\section{Adversarial Attacks}
\label{sec:adversarial-attacks}

% TODO:

% GoogLeNet:
% - presented at ILSVRC 2014 outperform competitors
% - start to keep an eye on efficiency (# params and FLOPs)
% - focus on block design (inception)
% - v2 add batch normalization
% - v3 imporoved inception block
% - v4 add residual connections
% VGGNet Deep is better
% GoogLeNet and VGGNet popularized the idea of going deeper using fix block (e.g. inceptions)
% ResNet introduce the idea of skip connections
% EfficientNet formalized scaling direcitons width vs depth for Squeeze-Extitaion block (SENet)
% RegNet improve skip connections by introduciong regulator module hybrid approach.

% Tweet by Andrej Karpathy
% The Transformer is a magnificient neural network architecture because it is a
% general-purpose differentiable computer. It is simultaneously:
% 1) expressive (in the forward pass)
% 2) optimizable (via backpropagation+gradient descent)
% 3) efficient (high parallelism compute graph)

