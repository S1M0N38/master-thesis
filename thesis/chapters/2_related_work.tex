\chapter{Related Work}
\label{ch:related-work}

\section{Image Classification with Neural Network}
\label{sec:image-classification-with-neural-network}

One of the first instances where Neural Network were employed for image
classification was in 1998 by Lecun et al.~\cite{GradientBasedLecun1998}. Their
model, LeNet-5, was able to identify handwritten digit with higher accuracy
than previous methods which relied on manual features extraction such edges
detection and hard-coded pattern recognition. A raw image of a digit is passed
as input to LeNet and the model spits out a number from 0 to 9. Some
foundational ideas were at the core of the architecture (convolutional
operations, gradient based optimization, pooling layers) stood the test of time
and proved to be highly successful.

Fast forwarding to 2012. AlexNet~\cite{ImagenetClassiKrizhe2017}, a Convolution
Neural Network (CNN) similar in spirit to LeNet-5, annihilated competitions at
ImageNet Large Scale Visual Recognition Challenge~\cite{ImagenetALarDeng2009}
(ILSVRC) with a whopping 10.8 \% gap on the runner-up in top-5 error metric.
This result proved the potential of CNNs to process bigger color images (224 ×
224 × 3) and this sparked interest in the field which abandoned the idea of
manual features extraction in favour of the end-to-end approach.

Researchers explored various moving part of CNNs architecture: depth of the
model, optimal kernel size, number of filters, pooling operations.
\cite{VeryDeepConvoSimony2014,GoingDeeperWiSzeged2014} focus on increasing the
depth of the models and \cite{GoingDeeperWiSzeged2014} was able to do so by the
introduction of a highly efficient Inception module which were able to increase
the depth while keeping the FLOPs under control. Efficiency at inference time is
crucial for embedding models into resource constrained device such smartphone or
IOTs~\cite{SqueezenetAleIandol2016, MnasnetPlatfoTanM2018,
MobilenetsEffHoward2017}.

A problem that arose with the increase in depth was the so-called ``vanishing
gradient''. The gradient of the loss function with respect to the model's
parameters in the early layers tends to zero and this affect the ability of the
optimizer to update the weights towards a local minimum. Residual connections,
introduced in ResNet~\cite{DeepResidualLHeKa2015, IdentityMappinHeKa2016}, are
shortcuts in the network architecture that circumvent the problem allowing the
gradient to flow backwards effectively, so deeper models can be trained.
Connections between layers, i.e. the topology of the network, was further
explored by~\cite{DualPathNetwoChen2017, DenselyConnectHuang2016} and prior
using evolutionary methods by~\cite{DesigningNeuraMiller1989,
EvolvingNeuralStanle2002}.

Neural Architecture Search was employed to search for the width and the depth
of a model given a basic building block~\cite{ProgressiveNeuLiuC2017,
GeneticCnnXieL2017, LargeScaleEvoReal2017, RegularizedEvoReal2018}. Using
different building blocks and optimizing for different budget types (e.g.
FLOPs, number of parameters, inference time) various architecture were
produced. EfficientNets~\cite{EfficientnetRTanM2019, Efficientnetv2TanM2021}
are a well-known example of such approach that make use of
Squeeze-and-Excitation module~\cite{SqueezeAndExcHuJi2017} as basic component.

Stacks of convolution blocks followed by a fully connected layers start to
reach the limit regarding classification accuracy. CNNs were equipped with an
attention mechanism be able to effectively process global features in an image
defining hybrid architecture~\cite{SpatialTransfoJaderb2015,
LookAndThinkCaoC2015, ShowAttendAnXuKe2015, ScaCnnSpatiaChen2016}. Recently
interest shifted towards Vision Transformers
(ViT)~\cite{AnImageIsWorDosovi2020}, architecture that use solely attention
mechanism for features extraction which are able to achieve state-of-the-art
results in some well-established image classification
problems~\cite{TransformersInKhan2021, ASurveyOnVisHanK2023}. ViT required to
be trained of huge datasets to achieve competitive performance and, at the
moment of the writing, cannot run on consumer and low power devices due to high
computational cost.

\section{Semantic Information Sources}
\label{sec:semantic-information-sources}

Image classification consists in assigning a label to an image given a fixed
set of labels. Without any additional information about labels they are just a
set of symbols with no structure nor relationship between them. On the other
hand having richer information on labels turns out to be beneficial for model
interpretability~\cite{ImprovingInterDong2017}, image
summarization~\cite{SemanticImagePasini2022} and image classification as
well~\cite{MakingBetterMBertin2019}. Assuming that label a label of an image is
an English word indicating what is depicted in the image, we can obtain extra
information about a label by various sources.

WordNet~\cite{WordnetMi1995} is a lexical database for the English language
where words are linked together by semantic relations (synonyms, hyponyms and
meronyms) resulting in a graph structure. The aforementioned ImageNet database
was build using WordNet as a source of labels, so information from WordNet could
be exploited when training a model on ImageNet. Other examples of semantically
enriched datasets include iNaturalist~\cite{TheInaturalistHorn2017} and
CIFAR-100~\cite{LearningMultipKrizhe2009}.

The majority of times labels are just a set of words with no structure between
so semantic structure has to be constructed from bare words. A flexible approach
is to resort to internal representation of a pre-trained language models (LM)
commonly knows as word embeddings~\cite{}. The extracted semantic structure is
organised as a vector space where words are represented as real-valued vectors.
The input to a LM can be the raw label or a detailed description of the label.
Moreover, such a description can be written by human experts, scraper from the
web or generated by a LM itself.

\section{Semantic Information Injection}
\label{sec:semantic-information-injection}

Depending on the representation of the semantic information (graph, vector
spaces, others) there are different ways to inject it into a model. 

% TODO:

% Talk a bit about cutting edge research and multimodal learning
Transformer architecture~\cite{AttentionIsAlVaswan2017} was introduced in the
context of Natural Language Processing. They proved to be a general-purpose
differentiable computer: expressive (in the forward pass), optimizable (via
backpropagation and gradient descent) and efficient (highly
parallelizable)\footnote{Paraphrasing a post on $\mathbb{X}$ by Andrej
Karpathy, a prominet figure in the field.}. Then they were adapted for
processing images~\cite{ImageTransformParmar2018, EndToEndObjeCarion2020,
AnImageIsWorDosovi2020}, videos~\cite{VivitAVideoArnab2021,
TemporalContexShao2020}, audio~\cite{NeuralSpeechSLiNa2018,
SpeechTransforDong2018, ConformerConvGulati2020}.
Now the cutting edge reseach focus on processing different inputs types coming
from different sources trying to exploting the correlation between them. Those
model are called multimodal models~\cite{ASurveyOnMulYinS2023}.

\section{Adversarial Attacks}
\label{sec:adversarial-attacks}

% TODO:

% GoogLeNet:
% - presented at ILSVRC 2014 outperform competitors
% - start to keep an eye on efficiency (# params and FLOPs)
% - focus on block design (inception)
% - v2 add batch normalization
% - v3 imporoved inception block
% - v4 add residual connections
% VGGNet Deep is better
% GoogLeNet and VGGNet popularized the idea of going deeper using fix block (e.g. inceptions)
% ResNet introduce the idea of skip connections
% EfficientNet formalized scaling direcitons width vs depth for Squeeze-Extitaion block (SENet)
% RegNet improve skip connections by introduciong regulator module hybrid approach.

% Tweet by Andrej Karpathy
% The Transformer is a magnificient neural network architecture because it is a
% general-purpose differentiable computer. It is simultaneously:
% 1) expressive (in the forward pass)
% 2) optimizable (via backpropagation+gradient descent)
% 3) efficient (high parallelism compute graph)

