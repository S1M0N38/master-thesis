\chapter{Injecting semantic information}
\label{ch:injecting-semantic-information}

Our goal is to inject semantic information into image classification models without significantly altering a well-established model architecture that has demonstrated its efficacy over time.
Upon dissecting a model into its constituent components, we identify which parts could be enhanced by the addition of extra information. Consequently, the techniques introduced can be seamlessly incorporated into other model architectures.

\section{Framework}
\label{sec:framework}

% HACK: How to properly reference the glossary
In order to establish a common ground for the following sections, in \Cref{fig:03/framework} we present a framework for an image classification model and define its building blocks.
Additionally, we introduce the notation employed in the mathematical expressions in the Notation section at the end.
\begin{figure}[htbp]
  \ctikzfig{03/framework}
  \caption{Image classification framework}
  \label{fig:03/framework}
\end{figure}

\begin{itemize}
\item \textbf{Input} (\gls{input}). Let \gls{input} be an image represented as a real-valued tensor\footnote{The deep learning community uses the term ``tensor'' as a synonym for ``multidimensional array''; it is not equipped with the additional structure found in physics or mathematics. Throughout this thesis, we use the deep learning jargon.} of shape \gls{channel_size} $\times$ \gls{height_size} $\times$ \gls{width_size}, where \gls{channel_size} is the number of channels, \gls{height_size} is the height, and \gls{width_size} is the width. Its entries are the intensity values of the colors associated with the RGB channels.

\item \textbf{Model} (\gls{model}). Let \gls{model} be a function that takes an image as input and returns a vector, i.e.\
\begin{equation}
  \psi_\theta: \mathcal{X} \to \mathcal{Y}
  : x \mapsto \hat{y} \gls{define_lhs_from_rhs} \psi_\theta \left(x\right)
  \label{eq:model}
\end{equation}
where \gls{input_set} $\subseteq \mathbb{R}^{C \times H \times W}$ is the set of images and \gls{output_set} $\subseteq \mathbb{R}^{D}$ is the set of the model's outputs. At the highest level of abstraction, a deep learning model can be seen as a deterministic function of several real variables that outputs a real-valued vector. Internally, it is a composition of functions such as matrix multiplications, convolutions, non-linearities, etc. Sometimes the term ``architecture'' is used synonymously with ``model''.

\item \textbf{Output} (\gls{output}). Let \gls{output} denote the output of the model, that is, the outcome of the application of \gls{model} on \gls{input}. This is a \gls{output_size}-dimensional real-valued vector. We use the more specific term ``model's predictions'' when \gls{output_size} is equal to the number of classes and the vector is a \acrfull{pmf} over the classes.

\item \textbf{Parameters} (\gls{parameters}). Using \gls{parameters} as a subscript of \gls{model}
reminds us that the functions constituting the model are parameterized by a set of real numbers. These parameters are initially randomly initialized and then adjusted during the training phase, steering the model in such a way as to solve the given task. Sometimes the term ``weights'' is used synonymously with ``parameters''.

\item \textbf{Class} (\gls{class}, \gls{class_i}). We use the terms ``class'' and ``label'' interchangeably to refer to the text associated with a given image. In the context of single-label image classification, the class \gls{class} is a word that is an element of \gls{classes_set}, the set of possible classes. Sometimes it is handy to promote the set of classes to a list-like structure so that every class \gls{class} is associated with a numeric index $i$ such that $\gls{class_i} \to i$.

\item \textbf{Encodings} (\gls{encoding}, \gls{encoding_i}). An encoding of a class \gls{class} is a \gls{output_size}-dimensional real-valued vector and is denoted by \gls{encoding}. The notation \gls{encoding_i} is used to indicate the encoding of class \gls{class_i}. The term encoding should not be confused with the term embedding, which can be regarded as a specific type of encoding. An example of encoding is one-hot encoding.

\item \textbf{Encoder} (\gls{encoder}). Let \gls{encoder} be a function that takes a class \gls{class} as input and returns its encoding \gls{encoding}, i.e.\
\begin{equation}
  \phi: \mathcal{C} \to \mathcal{Y} : c \mapsto y := \phi(c).
  \label{eq:encoder}
\end{equation}
Typically, the conversion from class text to numerical representation is not explicitly included in the descriptions of deep learning models. However, in this work, we devote attention to this aspect because it is crucial in the subsequent discussions as a means to inject semantic information. Furthermore, it must not be confused with the ``encoder block'' found in some \acrshort{lm} architectures.

\item \textbf{Loss} (\gls{loss}). The loss or cost function, denoted as \gls{loss}, is a function that takes as input the output of the model \gls{output} and the encoding of the corresponding class \gls{encoding} and returns a scalar,
\begin{equation}
  \Loss: \mathcal{Y} \times \mathcal{Y} \to \mathbb{R}
       : (\hat{y}, y) \mapsto \Loss(\hat{y}, y).
  \label{eq:loss}
\end{equation}
It measures the discrepancy between the model's output and the encodings; thus, the lower its value, the better the model's performance. Depending on the task and the encoding, different functional forms can be used. An example of a loss function is the cross-entropy loss.

\end{itemize}

Assembling these components, we obtain an end-to-end trainable model for image classification. Initially, the model's output carries no information about the classification of the input image. During training, numerous images and their corresponding labels are fed into the system so that the parameters \gls{parameters} are tuned in such a way that the model's outputs increasingly resemble the encodings of the corresponding class, thus facilitating the emergence of classification capabilities.

Taking advantage of \acrshort{gpu}s, the computation can be parallelized along the batch dimension \gls{batch_size}, i.e.\ multiple images are processed concurrently as input to the model. Therefore, at the implementation level, \gls{input} will be a tensor of dimensions \gls{batch_size} $\times$ \gls{channel_size} $\times$ \gls{height_size} $\times$ \gls{width_size}, and consequently, the output and the encoding will be tensors of dimensions \gls{batch_size}$\times$\gls{output_size}. The \gls{model}, \gls{encoder}, and \gls{loss} must be extended accordingly.


\section{Toy Dataset}
\label{sec:toy-dataset}

It is easier to explain and understand hierarchical concepts with a concrete example. Therefore, we introduce a minimalistic example based on a toy dataset that will be useful in the following discussions. The classes are $\mathcal{C} = [\ \texttt{lemon},\ \texttt{pear},\ \texttt{apple},\ \texttt{dog},\ \texttt{cat},\ \texttt{car}\ ]$ and are arranged in a list-like structure so that an index can be associated with each class, i.e.\ $\gls{class_i} \to i$.
For example, using zero-based indexing, we have $\texttt{lemon} \to 0,\ \texttt{pear} \to 1,\ \ldots$. Their hierarchical relationships are given by the tree in~\Cref{fig:03/toy-dataset}. At the first level of the hierarchy, we have the following partition: \texttt{lemon}, \texttt{pear}, and \texttt{apple} are fruits; \texttt{dog} and \texttt{cat} are animals, while \texttt{car} is a vehicle. At the second level, we have a separation between Natural and Artificial objects. The root node includes all the classes. The number of levels in this hierarchy is $\gls{hierarchy_levels} = 4$. Moreover, suppose that this dataset contains 10 samples per class for a total of $\gls{dataset_size} = 60$ samples.
\begin{figure}[htbp]
  \ctikzfig{03/toy-dataset}
  \caption{Hierarchical tree of the toy dataset}
  \label{fig:03/toy-dataset}
\end{figure}

\section{Encodings}
\label{sec:encodings}

Encodings are deterministic functions that do not change during training; thus, we can compute the encoding for each label of each image in the dataset before training. Starting from a dataset $\{ \left(\gls{input}, \gls{class}\right)_n \}_{n = 1, \ldots, \gls{dataset_size}}$ of \gls{dataset_size} images and their corresponding classes, we obtain $\{ \left(x, y\right)_n \}_{n = 1, \ldots, \gls{dataset_size}}$, a set of images and their corresponding encodings by applying an encoder \gls{encoder} to all \gls{class}.
Detaching the encoding procedure from training not only reduces the computational cost but also allows for the inspection of the encoding before training (\Cref{subsec:encodings-projections}).

\subsection{One-hot Encoding}
\label{subsec:one-hot-encoding}

One-hot encoding is a straightforward encoding typically paired with the cross-entropy loss function (see \Cref{subsec:cross-entropy-loss}). We consider this combination as our baseline, and the following encoding-loss pairs are developed to improve on various aspects while maintaining comparable performance.
The encoder function \gls{encoder} that produces one-hot encoding is
\begin{equation}
  \phi: \mathcal{C} \to \{0, \,1\}^{D}
      : c_i \mapsto y_i \equiv \phi \left(c_i\right)
  \quad \text{where} \quad
  \phi{\left(c_i\right)}_j = \gls{kronecker_delta}.
  \label{eq:one-hot-encoding}
\end{equation}
Referring to the toy dataset, we have that the one-hot encoding for \texttt{apple} is $\left[0, 0, 1, 0, 0, 0\right]$. The resulting encodings are sparse orthogonal binary vectors. Their orthogonality translates into a well-separated encoding space, facilitating model training. Speed and simplicity are the key factors for its widespread use.

A significant limitation of one-hot encoding is that it discards all hierarchical information, accounting only for level zero of the hierarchy tree (\Cref{fig:03/toy-dataset}). This means that, encoding-wise, \texttt{lemon} and \texttt{pear} are as similar as \texttt{lemon} and \texttt{car}, even though the former pair shares the same ancestor in the tree, while the latter do not share any node besides the root.

\paragraph{Cosine similarity} The notion of encoding similarity employed throughout this document is the \emph{cosine similarity}. Given two encodings $y_1$ and $y_2$, the cosine similarity (or simply their similarity) is defined as
\begin{equation}
  \gls{cosine_similarity}: \mathcal{Y} \times \mathcal{Y} \to \left[-1, +1\right]
  : (y_1, y_2) \mapsto \gls{cosine_similarity} \,(y_1, y_2) :=
  \frac{y_1 \cdot y_2}{\|y_1\| \, \|y_2\|}
  \label{eq:cosine-similarity}
\end{equation}
where \gls{dot_product} is the dot product and \gls{l2_norm} is the $L^2$ norm.
If $y_1$ and $y_2$ have already been normalized, i.e.\ $\|y_1\| = \|y_2\| = 1$, then the cosine similarity reduces to the dot product and is proportional to the Euclidean distance. Each one-hot encoding has a cosine similarity equal to zero with other encodings except for itself.


\subsection{Hierarchical Encodings}
\label{subsec:hierarchical-encodings}

We use the term \emph{hierarchical encodings} to refer to all the encodings that utilize semantic information derived from a hierarchy. As mentioned in~\cref{par:hierarchies-tree}, a hierarchy can be represented as a tree data structure, an example of which is provided by the toy dataset in~\Cref{fig:03/toy-dataset}. The leaves of the tree are the classes, while the ancestors at increasingly deeper depths encapsulate broader concepts.

First, we must select a \emph{metric on the tree} to calculate pairwise distances between classes and organize the results in a $|\mathcal{C}| \times |\mathcal{C}|$ matrix. After applying a series of \emph{operations on the rows} of this matrix, we can directly extract encodings. Different hierarchical encodings result from various choices of metrics and operations.

\paragraph{Lowest Common Ancestor}
The height of the \acrfull{lca} can serve as a measure of distance between two classes in the hierarchy tree. The \acrshort{lca} of $c_i$ and $c_j$ is the node that is the nearest common ancestor of both $c_i$ and $c_j$, i.e.\
\begin{equation}
  \gls{lca_fn} : \mathcal{C} \times \mathcal{C} \to \mathcal{V}:
  \left(c_i, c_j\right) \mapsto v_{ij} \gls{equiv}
  \gls{lca_fn} \, \left(c_i, c_j\right)
  \label{eq:lca}
\end{equation}
where \gls{vertices_set} denotes the set of vertices of the tree. The height of a node is equal to its level in the hierarchy.
Thus, the height of the \acrshort{lca} is
\begin{equation}
  \gls{height_fn} \gls{compose} \gls{lca_fn} : \left(c_i, c_j\right) \mapsto h_{ij}
  \label{eq:lca-height}
\end{equation}
From the \acrshort{lca} heights, a similarity measure between classes can be derived, which is
\begin{equation}
  s_{ij} := 1 - \frac{h_{ij}}{L}
\end{equation}
where \gls{hierarchy_levels} is the number of levels in the hierarchy tree.
Referring to the toy dataset, we have the pairwise \acrshort{lca} heights~(\Cref{fig:03/lca-height-matrix}) and \acrshort{lca} similarities~(\Cref{fig:03/lca-similarity-matrix}).
\begin{figure}[htbp]
  \begin{subfigure}{0.45\textwidth}
    \ctikzfig{03/lca-heights}
    \caption{LCA height matrix: $h_{ij}$}
    \label{fig:03/lca-height-matrix}
  \end{subfigure}
  \begin{subfigure}{0.45\textwidth}
    \ctikzfig{03/lca-similarities}
    \caption{LCA similarity matrix: $s_{ij}$}
    \label{fig:03/lca-similarity-matrix}
  \end{subfigure}
  \caption{Matrices derived from the \acrlong{lca}}
\end{figure}

\paragraph{Bertinetto et al.~\cite{MakingBetterMBertin2019}}\label{par:encoding-mbm} To produce the encoding, the authors apply a softmax function row-wise to a negative rescaling of the \acrshort{lca} heights matrix, that is
\begin{equation}
  f : h_i \mapsto y_i \equiv f(h_i)
  \quad \textrm{where} \quad
  f (h_i)_j := \frac{\exp \left({-\alpha \, h_{ij}}\right)}
  {\sum_j \exp\left({-\alpha \, h_{ij}}\right)}
  \label{eq:mbm-hierarchical-encoding}
\end{equation}
They use cross-entropy as a loss function, so the encoding must be a \acrshort{pmf} over classes.
Moreover, they introduce a hyper-parameter $\alpha \in [0, +\infty)$ that sharpens the distribution around the index corresponding to the encoded class. Thus, for a small value of $\alpha$, the resulting encoding will be a flat \acrshort{pmf}, while for larger values of $\alpha$, it approaches the one-hot encoding.

\paragraph{Perotti et al.~\cite{BeyondOneHotPerott2023}}\label{par:encoding-b3p} It is a similar approach to the previous one but differs in the normalization function and in the role played by the hyper-parameter:
\begin{equation}
  f :  s_i \mapsto y_i \equiv f(s_i)
  \quad \textrm{where} \quad
  f (s_i)_j := \beta \, \delta_{ij} +
  \left(1 - \beta \right) \, \frac{\max \left(s_{ij},\,0\right)}
  {\sum_j \max \left(s_{ij},\,0\right)}.
  \label{eq:b3p-hierarchical-encoding}
\end{equation}
They start from the \acrshort{lca} similarities matrix, then clip at zeros and normalize row-wise.
Finally, a certain amount of one-hot encoding is added by weighting terms with $\beta \in [0, 1]$. For $\beta = 1$, we revert to one-hot encoding.

\paragraph{Barz and Denzler~\cite{HierarchyBasedBarz2018}}\label{par:encoding-bd} This approach is distinct from the previous two. Instead of operating directly on the \acrshort{lca} matrix, they propose an algorithm to calculate encodings $\phi(c_i)$ such that
\begin{equation}
  \forall \, c_i, c_j \in \mathcal{C} \qquad
  \phi(c_i) \cdot \phi(c_j) = s_{ij}.
  \label{eq:bd-hierarchical-encoding}
\end{equation}
It begins by choosing a normalized encoding $\phi(c_0)$ and then recursively solves systems of linear equations to calculate the other encodings.
Applied to the toy dataset, Barz and Denzler encodings are calculated as follows:
\begin{equation*}
  \phi{\left(c_0\right)} := \left[1, 0, 0, 0, 0, 0\right]
  \quad \rightarrow \quad
  \begin{cases}
    \phi{\left(c_0\right)} \cdot \phi{\left(c_1\right)} = \sfrac{2}{3} \\
    \phi{\left(c_1\right)} \cdot \phi{\left(c_1\right)} = 1 \\
    \Rightarrow  \phi{\left(c_1\right)}
  \end{cases}
  \quad \rightarrow \quad
  \begin{cases}
    \phi{\left(c_0\right)} \cdot \phi{\left(c_2\right)} = \sfrac{2}{3} \\
    \phi{\left(c_1\right)} \cdot \phi{\left(c_2\right)} = \sfrac{2}{3} \\
    \phi{\left(c_2\right)} \cdot \phi{\left(c_2\right)} = 1 \\
    \Rightarrow \phi{\left(c_2\right)}
  \end{cases}
  \quad \rightarrow \quad \ldots
\end{equation*}

\subsection{Word Encodings}
\label{subsec:word-encoding}

For many datasets, hierarchies do not exist or cannot be easily created. However, it is quite common for the classes to be words or short phrases. This is the information that \emph{word encodings} attempt to exploit. Even without an explicit hierarchy, as provided in the toy dataset, every English speaker intuitively knows that \texttt{lemon} and \texttt{pear} are more similar than \texttt{lemon} and \texttt{car}.
However, it is not clear what ``similar'' means in this context: Is \texttt{lemon} more similar to \texttt{pear} than to \texttt{apple}? To pinpoint such a measure, we resort to \emph{word embeddings}, i.e.\ latent word representations from pre-trained \acrshort{lm}s. These are real-valued vectors of dimension $\gls{output_size}$, on which we can apply the previously defined notion of cosine similarity.
\begin{equation}
  g : \gls{classes_set} \to \mathbb{R}^{\gls{output_size}} :
  c \mapsto y \equiv g(c)
  \label{eq:word-encoding}
\end{equation}
where $g$ is the embedding function of the pre-trained \acrshort{lm}.
In this case, the output of $g$, i.e.\ the embedding, is what we define as a word encoding. However, in the following, ``embeddings'' and ``encodings'' are not synonymous; the latter is derived from the former.

Many of the works mentioned in \Cref{subsec:embeddings} relate to \acrshort{zsl} and train a word embedder alongside the image classifier.
Perotti et al.~\cite{BeyondOneHotPerott2023} use GloVe~\cite{GloveGlobalVPennin2014} as a pre-trained word embedder, seeking model performance improvements in terms of the ``quality and quantity'' of errors. The pre-trained GloVe model can be thought of as a large dense matrix where each row corresponds to a word in the training dictionary. They then read out the corresponding rows and calculate pairwise cosine similarity, obtaining $s_{ij}$. From $s_{ij}$, they again use \Cref{eq:b3p-hierarchical-encoding} to obtain the encoding.

However, upon closer examination, the word embeddings approach has two obvious flaws.
What if the dataset classes' words are not in the GloVe vocabulary?
Is \texttt{apple} the fruit or the company?
The first problem can sometimes be patched by the use of a synonym available in the GloVe dictionary. Other times, the words are so specific (e.g. scientific names in iNaturalist~\cite{TheInaturalistHorn2017}) that it makes no sense to substitute them with similar words. In the worst case, a new embedder $g$ can be trained from scratch, ensuring that the training text corpus contains the relevant words.
The second problem is more deeply rooted in the word embeddings themselves. The same word can have different meanings (homographs), and the single vector returned by a GloVe-like model does not make any distinction. The only way to overcome this issue is to provide more context about the embedded word, which is why we introduced description encodings.

\subsection{Description Encodings}
\label{subsec:description-encodings}

Description encodings offer improvements over word encodings by overcoming out-of-vocabulary and homograph issues. While the former can be addressed by using a different embedder architecture (discussed later), the latter requires the provision of additional context rather than relying on a single isolated word. 
The idea is to embed a description of the class, which includes distinctive features that aid in differentiating it from other classes within the dataset. The underlying principle remains: similar classes possess similar descriptions, leading to similar encodings.
The general pipeline is as follows:
\begin{enumerate}
  \item Produce a \emph{description} from the \emph{word}, e.g.~\eqref{eq:desc-writing}
  \item Generate the \emph{embedding} from the \emph{description}, ~\eqref{eq:desc-embedding}
  \item Produce the \emph{encodings} from the \emph{embedding}, ~\eqref{eq:embedding-projection}
\end{enumerate}

% From the word produce the description
If the classes in the dataset are represented by words, there are three potential methods to generate their corresponding descriptions: (1) manually writing them, possibly with the assistance of a domain expert, (2) scraping them from the Web, or (3) generating them with a \acrshort{lm}.
The first method can yield high-quality, individual descriptions but is neither fast nor scalable. Sometimes, domain experts are required, making the cost of enriching dataset descriptions non-negligible. For instance, iNaturalist19~(\Cref{subsec:inaturalist19}) has 1010 classes with scientific names as words.
The second method is more rapid and scalable but may be inadequate when class names are non-standard or ambiguous, which revives the homograph issue. This approach could be suitable for iNaturalist where each class has a detailed and precise description on Wikipedia, but not for ImageNet~\cite{ImagenetALarDeng2009} and CIFAR100~(\Cref{subsec:cifar100}).
Utilizing cutting-edge generative \acrshort{lm}s to write descriptions is a more versatile approach: it is quick, scalable, and can be fine-tuned with prompt engineering.
Naturally, different strategies may be appropriate for various datasets and can be applied iteratively or in combination (e.g. a human expert could review generated/scraped descriptions, or handwritten/scraped descriptions could be used for in-context learning, etc.).
The process of writing descriptions can be thought as a function from the set of classes \gls{classes_set} to the set of descriptions \gls{descriptions_set}:
\begin{equation}
  w : \mathcal{C} \to \mathcal{W}:
  c \mapsto w_c \equiv w(c)
  \label{eq:desc-writing}
\end{equation}
\Cref{fig:descriptions} presents some example descriptions for the toy dataset.

\begin{figure}[htbp]
  \begin{minipage}{\textwidth}
    \begin{itemize}
      \item \texttt{lemon}: \emph{\small``Lemons are \alert{oval-shaped} fruits known for their \alert{bright yellow} color and acidic juice.''}
      \item \texttt{pear}: \emph{\small``Pears are fruits with a \alert{rounded bottom} and a narrower, \alert{elongated top}.''}
      \item \texttt{apple}: \emph{\small``Apples are \alert{round} fruits that come in a variety of colors, including \alert{red}, \alert{green}, and \alert{yellow}.''}
    \end{itemize}
    \caption{Examples of descriptions for the toy dataset}
    \label{fig:descriptions}
  \end{minipage}
\end{figure}

% From a description generate the embedding
Once the descriptions are obtained, they must be converted into real-valued vectors where the notion of similarity is well-defined. To achieve this, we require embedders capable of processing sentences as inputs and outputting vector representations such that intuitively similar descriptions are mapped to mathematically similar vectors. These embedders are successors to GloVe-like models and do not depend on a fixed English word vocabulary, thus avoiding the out-of-vocabulary issue. The vocabulary of these embedders consists of tokens, which are high-frequency recurring sequences of characters (somewhat akin to syllables), allowing the description to be decomposed into a sequence of tokens (the tokenization process).
Similar to~\Cref{eq:word-encoding}
\begin{equation}
  g : \mathcal{W} \to \mathbb{R}^{\tilde{D}} :
  w_c \mapsto e_c \equiv g(w_c)
  \label{eq:desc-embedding}
\end{equation}
where \gls{embedding_size} denotes the size of the embedding.\\

% From an embedding generate the encodings
Embedding models employed in \Cref{eq:desc-embedding} are a powerful idea that is not limited to \acrshort{nlp} but is also used in image and audio processing.
They can be regarded as machineries capable of lossy compression of information into a fixed-size real-valued vector. Due to their versatility, their output size $\tilde{D}$ can be on the order of thousands, but it is reasonable to assume that class descriptions coming from a dataset lie on a low-dimensional manifold.\footnote{The low-dimensional manifold hypothesis was empirically verified in the experiments in~\Cref{ch:experimental-setup} by varying the output dimension $D$: reducing from $D'$ to $D$ does not affect the model's performance (on the contrary, performance improved because of the reduction in the number of parameters).}
In order to zoom in on the ``relevant'' subspace, we project the embedding into a lower-dimensional space using a dimensionality reduction algorithm $r$:
\begin{equation}
  r : \mathbb{R}^{\tilde{D}} \to \mathbb{R}^{D} :
  e_c \mapsto y \equiv r(e_c)
  \label{eq:embedding-projection}
\end{equation}
The outcome of this projection is what we call \emph{description encodings}.\\

% Using function composition
Using function composition, description encodings can be written as
\begin{equation}
  \phi := r \circ g \circ w : \mathcal{C} \to \mathbb{R}^{D} :
  c \mapsto y \equiv \phi(c)
  \label{eq:desc-encoding}
\end{equation}
Following this recipe, diverse description encodings differ from the choice of $w$, $g$, $r$ functions and the hyper-parameter $D$.

\paragraph{PCA Description Encoding}\label{par:encoding-desc} As a representative of the description encodings family, we chose to generate and embed descriptions using pretrained \acrshort{lm} and project them with the \acrfull{pca} algorithm.
A very simple prompt was used as input to OpenAI \emph{gpt-3.5-turbo}:

\vspace{10pt}

\begin{minipage}{0.9\linewidth}
\texttt{\small
You are a helpful assistant that has to provide the description of a
\textquotesingle\textcolor{gray}{\texttt{class}}\textquotesingle.\\
- What a \textquotesingle\textcolor{gray}{\texttt{class}}\textquotesingle\ is.\\
- What a \textquotesingle\textcolor{gray}{\texttt{class}}\textquotesingle\ looks like (for example color,
texture, shape, ...).\\
- In what context \textquotesingle\textcolor{gray}{\texttt{class}}\textquotesingle\ is used or can be
found.\\
Focus on the visual characteristics of a \textquotesingle\textcolor{gray}{\texttt{class}}\textquotesingle.\\
Write 7 short sentences to describe a \textquotesingle\textcolor{gray}{\texttt{class}}\textquotesingle\ in
encyclopedic style.
}
\end{minipage}

\vspace{10pt}

Then, each description is embedded using OpenAI \emph{text-embedding-ada-002} model, obtaining a 1536-dimensional embedding for each class. Finally, embeddings are passed through \acrshort{pca} projecting them to $D$-dimensional vectors. Different encodings were produced with different values for $D$.
Using the previously introduced notation
\begin{equation}
  \phi := \textrm{PCA} \circ \textrm{text-embedding-ada-002} \circ
  \textrm{gpt-3.5-turbo}
  \label{eq:desc-pca}
\end{equation}


\section{Loss Functions}
\label{sec:losses}

As mentioned in~\Cref{subsec:hierarchies}, using custom loss functions is a viable method to inject semantic information during training. However, in this work, we do not explore this path, opting instead for standard loss functions due to the following compelling reason.
We believe that the standard-loss/custom-encoding approach, when feasible, is preferable because it transfers some computational cost to the pre-training phase (embeddings are computed before training) and allows for the use of well-implemented and highly optimized loss functions available in deep learning libraries.

The loss functions that we employ in conjunction with custom embeddings are \emph{cross entropy} and \emph{cosine distance}.


\subsection{Cross Entropy Loss}
\label{subsec:cross-entropy-loss}

\acrfull{xe} is widely used in classification problems. It can be derived from the principle of maximum likelihood estimation, demonstrating that maximizing the likelihood is equivalent to minimizing cross entropy. Its mathematical foundation, simplicity, and effectiveness in practice make it the de facto standard loss function for classification tasks~\cite{ImagenetClassiKrizhe2017, VeryDeepConvoSimony2014, GoingDeeperWiSzeged2014, DeepResidualLHeKa2015, DenselyConnectHuang2016, EfficientnetRTanM2019, Efficientnetv2TanM2021, AnImageIsWorDosovi2020}

Given two probability vectors $p$ and $q$, whose components represent probability distributions over the same underlying set of events $\mathcal{C}$, the cross entropy is defined as:
\begin{equation}
  \textrm{XE} \, (p, q) := - q \cdot \log p
  \label{eq:cross-entropy}
  \qquad \textrm{where} \qquad
  \begin{cases}
    p := \psi_\theta(x) \\
    q := \phi(c)
  \end{cases}
\end{equation}

\paragraph{Cross entropy with one-hot encoding} When computing cross entropy between the model output and its corresponding one-hot encoded class, the expression simplifies to
\begin{equation*}
  \textrm{XE} \, (\psi(c_i),\phi_\theta(x))
  = - \phi(c_i) \cdot \log \psi_\theta(x)
  = \sum_{j = 1}^D \delta_{ij} \, \log \psi_\theta(x)_j
  = - \log \psi_\theta(x)_i
  \label{eq:cross-entropy-one-hot-encoding}
\end{equation*}
where only one component from the model output vector is directly involved in the optimization: the one corresponding to the encoded class is maximized.~\footnote{Furthermore, some optimization techniques can be employed to prevent overflow and underflow issues when computing the exponential followed by the logarithm. These techniques also enhance gradient calculation~\cite{SoftmaxWithCrPeters2017, UnderstandingSMirand2017, DeepLearningGoodfe2016}.}
Starting with an untrained model, we ask it to classify an image containing an apple from the toy dataset, resulting in a probability vector with random entries as the model's output~(\Cref{fig:03/xe-onehot-before}). Training with cross-entropy loss aims to adjust the model's parameters $\theta$ so that the probability distribution of the output's entries resembles the distribution of the components of the corresponding encoding; a \emph{one} with $|\mathcal{C}| - 1$ \emph{zeros} in the case of one-hot encoding~(\Cref{fig:03/xe-onehot-after}).
\begin{figure}[htbp]
  \centering
  \begin{subfigure}{0.45\textwidth}
    \centering
    % TODO: optimize the plot for thesis
    \begin{minipage}{\textwidth}
      \resizebox{\linewidth}{!}{\input{figures/03/xe-onehot-before.pgf}}
    \end{minipage}
    \caption{Before training}
    \label{fig:03/xe-onehot-before}
  \end{subfigure}
  \begin{subfigure}{0.45\textwidth}
    \centering
    % TODO: optimize the plot for thesis
    \begin{minipage}{\textwidth}
      \resizebox{\linewidth}{!}{\input{figures/03/xe-onehot-after.pgf}}
    \end{minipage}
    \caption{After training}
    \label{fig:03/xe-onehot-after}
  \end{subfigure}
  \caption{Optimization objectives of cross entropy with one-hot encoding}
\end{figure}

While cross entropy pulls up the probability of the correct class, the presence of normalization in the softmax function pushes down the probability related to the other classes. This all-or-nothing, black-and-white approach is the limit of ordinary classification models that do not exploit relationships between classes.

In the following, this combination (cross-entropy with one-hot encoding) will be considered the \emph{baseline} against which the other encoding-loss pairs will be compared.

\paragraph{Cross entropy with hierarchical encoding} Cross entropy can also be used as a loss function in the case of hierarchical encoding. Bertinetto et al.~(\ref{par:encoding-mbm}) and Perotti et al.~(\ref{par:encoding-b3p}) indeed utilize standard cross entropy loss with their respective hierarchical encoding implementations. In this case, the expression for the cross entropy loss cannot be simplified as in~\Cref{eq:cross-entropy-one-hot-encoding} and all components of the encoding play a role in the model optimization.
A predominant class should emerge during training, but the predicted probabilities for the other classes should not be zero. The more a class is similar to the ground truth class, the higher its predicted probability should be.
\begin{figure}[htbp]
  \centering
  \begin{subfigure}{0.45\textwidth}
    \centering
    % TODO: optimize the plot for thesis
    \begin{minipage}{\textwidth}
      \resizebox{\linewidth}{!}{\input{figures/03/xe-hier-before.pgf}}
    \end{minipage}
    \caption{Before training}
    \label{fig:03/xe-hier-before}
  \end{subfigure}
  \begin{subfigure}{0.45\textwidth}
    \centering
    % TODO: optimize the plot for thesis
    \begin{minipage}{\textwidth}
      \resizebox{\linewidth}{!}{\input{figures/03/xe-hier-after.pgf}}
    \end{minipage}
    \caption{After training}
    \label{fig:03/xe-hier-after}
  \end{subfigure}
  \caption{Optimization objectives of cross entropy with hierarchical encoding}
\end{figure}

\subsection{Cosine Distance Loss}
\label{subsec:cosine-distance-loss}

A different type of models are the ones that do not produce a probability distribution as their output, but instead return a ``vector representation of their answer''. To determine the predicted class, the output vector is compared to the encodings of the classes, and the class with the highest similarity is selected as the predicted class (\Cref{sec:decoding}).

In such cases, we aim to adjust the model's parameters so that the output vector of a specific image becomes increasingly similar to the encoding vector of the corresponding class. To achieve this, we require a loss function that accounts for the maximization of similarity (or minimization of dissimilarity).
Starting from cosine similarity~\eqref{eq:cosine-similarity}, we define the \acrfull{cd} as
\begin{equation}
  \textrm{CD} \, (p, q) := 1 - \cos (p, q)
  \label{eq:cosine-distance}
  \quad \textrm{where} \quad
  \begin{cases}
    p := \psi_\theta(x) \\
    q := \phi(c)
  \end{cases}
\end{equation}

\paragraph{Cosine Distance with Barz \& Denzler's Encodings} Barz and Denzler~(\ref{par:encoding-bd}) utilized cosine distance as loss function in~\cite{HierarchyBasedBarz2018}, which has been shown to be particularly effective with small datasets~\cite{DeepLearningOBarz2019}. In their research, they experimented with custom architectures featuring multiple output heads. One head aligns its output with their custom encoding using cosine distance, while another uses the standard cross-entropy loss with one-hot encoding. The total loss function is simply the sum of the two.
We refrain from using custom architectures so that the effectiveness of Barz and Denzler's encoding can be directly compared with PCA Description Encoding~(\ref{par:encoding-desc}).

\paragraph{Cosine Distance with Descriptions Encoding} Assuming that we have generated class descriptions as shown in~\Cref{fig:descriptions}, and following the procedure described in~\Cref{par:encoding-desc}, we obtain the description encodings.
To visually comprehend how the cosine distance loss optimizes the model, we further project the description encodings and the model's output into a two-dimensional space (see~\Cref{subsec:encodings-projections} for details). Optimization with cosine distance essentially involves minimizing the angle between the output vector and the encoding vector of the corresponding class.
\begin{figure}[htbp] % TODO:make these figures smaller
  \begin{subfigure}{0.45\textwidth}
    \ctikzfig{03/cd-desc-before}
    \caption{Before training}
    \label{fig:03/cd-desc-before}
  \end{subfigure}
  \begin{subfigure}{0.45\textwidth}
    \ctikzfig{03/cd-desc-after}
    \caption{After training}
    \label{fig:03/cd-desc-after}
  \end{subfigure}
  \caption{Optimization objectives of cosine distance with description encodings optimize for}
\end{figure}

\section{Encodings - Losses pairs}
\label{sec:encodings-losses-pairs}

In the preceding sections, we introduced various encodings and loss functions.
~\Cref{tab:encodings-losses} collects all encoding-loss combinations that we have focused on, and the results of which will be analyzed in subsequent chapters.
\begin{table}[htbp]
  \centering
  \begin{tabular}{lllcc}
    \toprule
    Name       & Encoding       & Loss & Parameter & Requires Hierarchy? \\
    \midrule
    XE One-hot & One-hot~(\ref{subsec:one-hot-encoding})
               & XE & None    & No   \\
    XE MBM     & Bertinetto et al.~(\ref{par:encoding-mbm})
               & XE & $\alpha$ & Yes  \\
    XE B3P     & Perotti et al.~(\ref{par:encoding-b3p})
               & XE & $\beta$ & Yes  \\
    CD BD      & Barz \& Denzler~(\ref{par:encoding-bd})
               & CD & None    & Yes  \\
    CD Desc.   & Description~(\ref{par:encoding-desc})
               & CD & $D$     & No   \\
    \bottomrule
  \end{tabular}
  \caption{Encoding-Loss Pairs}
  \label{tab:encodings-losses}
\end{table}
The first part of the naming convention uses the initial two letters to denote the loss (\texttt{XE} for cross-entropy and \texttt{CD} for cosine distance) while the second part indicates the encoding.
The acronym \texttt{MBM} refers to the paper by Bertinetto et al.~\cite{MakingBetterMBertin2019}, titled ``Making Better Mistakes''.
\texttt{B3P} and \texttt{BD} are acronyms for the authors of the respective papers: Perotti, Bertolotto, Pastor, Panisson, and Barz, Denzler. Some encodings necessitate the tuning of a hyper-parameter, whereas others do not.
Finally, the only encodings that do not require a hierarchy are the one-hot and description encodings, which, in theory, can be applied to any dataset.
