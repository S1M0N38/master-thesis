\chapter{Model Evaluation}
\label{ch:model-evaluation}

In this chapter, we will discuss all the components involved in evaluating a model's performance, i.e.\ how to derive the predicted class from the model's output (\Cref{sec:decoding}), various metrics (\Cref{sec:metrics}), projections (\Cref{sec:projections}), and adversarial attacks (\Cref{sec:adversarial-attacks-model-evaluation}).

\section{Decoding}
\label{sec:decoding}

As stated in \Cref{subsec:one-hot-encoding}, when using one-hot encoding and optimizing with cross-entropy loss, we compel the model's output $\hat{y}$ to amplify the probability associated with the correct class while disregarding the others. Thus, the focus is solely on a single component, and if the model is well-trained, its index corresponds to the ground truth class. In this case, it is sufficient to take the $\argmax$ over the model's output to obtain the predicted class.
Due to the trivial nature of this operation, it is often overlooked, but the process differs slightly when using alternative encodings (\Cref{subsec:hierarchical-encodings,subsec:description-encodings}).\medskip

Formally, obtaining the class from the output can be considered as the inverse of the encoding function $\phi$, that is
\begin{equation}
  \gls{decoder} : \mathcal{Y} \to \mathcal{C}
  : \hat{y} \mapsto \gls{predicted_class} := \gls{decoder}(\hat{y})
  \label{eq:decoding}
\end{equation}
hence the term \emph{decoding}.
For one-hot encoding, $\phi^{-1} := \argmax$, i.e.\ a function that returns the index of the maximum value of the vector given as input. In contrast, others encodings -- hierarchical and description encodings -- utilize multiple components in the model's output during training, so taking the $\argmax$ indiscriminately discards potentially useful information. Nevertheless, both loss functions employed -- cross-entropy loss and cosine distance -- aim to make the model's output resemble the ground truth encoding, so it is reasonable to consider the predicted class to be the one corresponding to the encoding most similar to the model's output.
To this end, we choose cosine similarity as the measure for such a quantity, i.e.\
\begin{equation}
  \phi^{-1}(\hat{y}) := \argmax_{c \in \mathcal{C}}
  \left( \gls{cosine_similarity} \left(\hat{y}, \phi(c) \right) \right)
  \label{eq:decoding-cos-sim}
\end{equation}
If $\phi$ is one-hot encoding, \Cref{eq:decoding-cos-sim} is equivalent to simply taking the $\argmax$ over the output vector.

An advantage of defining a decoding function based on encoding similarity is that it is compatible with different encoding schemes. For instance, selecting the highest component in a model's output trained using description encodings is nonsensical: the vector dimensions do not represent classes but are coordinates of a point in a high-dimensional space. What is significant is the relative distance of that point (class encoding) to other points (other class encodings).
Conversely, if components directly represent a class (e.g. hierarchical encodings), the provided decoding function operates as expected.
Another benefit is the ability to leverage information from all the output's components: we can not only attempt to identify the class present in the provided image but also discern what the model is confident the image is not (e.g. the class with the encoding that differs the most from $\hat{y}$).\medskip

The concept of decoding could be straightforwardly extended to the top-\gls{number_predicted_classes} predicted classes. Instead of returning the class associated with the most similar encoding, we simply return the top-\gls{number_predicted_classes} most similar classes.

\section{Metrics}
\label{sec:metrics}
Russakovsky et al.~\cite{ImagenetLargeRussak2014} introduced the \acrfull{ilsvrc}, which became the de facto benchmark to evaluate image classification models.
Even though they propose the use of top-\gls{number_predicted_classes} and hierarchical measures to assess a model's performance, they conclude ``[...] all three measures of error (top-5, top-1, and hierarchical) produced the same ordering of results. Thus, since \acrshort{ilsvrc} 2012 we have been exclusively using the top-5 metric, which is the simplest and most suitable to the dataset.''
The deep learning community prioritizes achieving high top-\gls{number_predicted_classes} accuracies as these are easier to compute for every dataset.

\paragraph{Quantity \& Quality}
\label{par:metrics-quantity-quality}
However, this work focuses on developing models that not only produce fewer errors but also less severe ones. While top-\gls{number_predicted_classes} accuracies are useful to capture the \emph{quantity} aspect, they do not provide any information about their \emph{quality}.
For example, confusing a \texttt{dog} for a \texttt{cat} can be considered a milder error compared to a \texttt{dog} - \texttt{car} misclassification; both are mammals and, as such, share some distinguishing features like fur, a head, a pair of eyes, four paws, etc. Cars present straight lines, a uniform coated surface, and an absence of typical traits of living things, etc.
This notion of visual similarity is somewhat encoded in the classes' hierarchy, so it makes sense to use distances on a hierarchical tree to quantify the severity of mistakes.\footnote{The assumption that a hierarchy carries information about visual characteristics is indeed strong and must be empirically checked for dataset-hierarchy pairs considered.
Datasets used in this work satisfy the ansatz, so hierarchical similarity is a good proxy for visual similarity.} In the following sections, we introduce some performance metrics that enable this quantitative/qualitative error assessment.

\paragraph{Levels}
\label{par:metrics-levels}
Having access to datasets equipped with a hierarchy introduces a new dimension to evaluate models across: their performance at different levels of the hierarchy.
With reference to the toy dataset in~\Cref{fig:03/toy-dataset}, suppose we are interested in a coarser-grained classification that distinguishes between Fruits, Animals, and Vehicles (level 1). In this case, after applying the decoding to \gls{output} to get \gls{predicted_class}, we need to map the predicted class to its ancestor in the hierarchy, obtaining the \emph{level-1 predicted class}. We can produce an even coarser classification discriminating between Natural vs.\ Artificial instances by mapping the level-1 predicted class to its ancestor, obtaining the \emph{level-2 predicted class}, and so on.
We denote the predicted class at the l-th level as $\hat{c}^l$ and $c^l$ as its corresponding ground truth; the index $l$ is omitted at level zero.

\subsection{Error Rate}
\label{subsec:error-rate}
There is no need to reinvent the wheel to give an estimate of the amount of error: top-1 accuracy (or simply accuracy) provides such an estimation.
However, for easier comparison with other metrics, we use the \emph{top-1 error rate} (or simply error rate), which is defined as $1 - \,$accuracy. The usage of a \emph{confusion matrix} makes the definition of the error rate, and the subsequent metrics, straightforward at the cost of extensibility to top-\gls{number_predicted_classes} versions.

\paragraph{Confusion Matrix}
\label{par:confusion-matrix}
It is a square matrix $|\mathcal{C}| \times |\mathcal{C}|$ whose rows are ground-truth classes \gls{class} and columns are predicted classes \gls{predicted_class}. Its entries $m_{ij}$ count the number of times a model produces the pair $(c_i, \hat{c}_j)$ when evaluated on a dataset.
For example, a model applied to the example dataset can produce confusion matrices at
different hierarchical levels as in~\Cref{fig:04/confusion-matrices}.
\begin{figure}[htbp]
  \centering
  \begin{subfigure}{0.35\textwidth}
    \ctikzfig{04/confusion-matrix-level-0}
  \end{subfigure}
  \begin{subfigure}{0.30\textwidth}
    \ctikzfig{04/confusion-matrix-level-1}
  \end{subfigure}
  \begin{subfigure}{0.25\textwidth}
    \ctikzfig{04/confusion-matrix-level-2}
  \end{subfigure}
  \caption{Example of confusion matrices: $m_{ij}^l$}
  \label{fig:04/confusion-matrices}
\end{figure}
\medskip

Using the confusion matrix, the error rate at the l-th level is the sum of the off-diagonal elements divided by the sum of all elements, i.e.\
\begin{equation}
  \textrm{error rate} :=
  \sum_{i \ne j} m_{ij}^l \bigg/ \sum_{i, \,j} m_{ij}^l.
  \label{eq:error-rate}
\end{equation}


\subsection{Hierarchical Distance Mistake}
\label{subsec:hierarchical-distance-mistake}
\emph{Hierarchical distance mistake} is a metric that takes into account the hierarchy to quantify the severity of a mistake, i.e.\ it makes use of the \acrshort{lca} height $h_{ij}$ as a weighting factor (see example in \Cref{fig:03/lca-height-matrix}).
The Hierarchical distance mistake is defined to be
\begin{equation}
  \textrm{hier.\ dist.\ mistake} :=
  \sum_{i, \, j} m_{ij}^l \, h_{ij}^l \bigg/ \sum_{i \ne j} m_{ij}^l.
  \label{eq:hierarchical-distance-mistake}
\end{equation}
Thus, it is the average \acrshort{lca} height between predicted and ground-truth classes.
Note that $h_{ii}$ are all zeros, so we are effectively summing only the weighted errors in the numerator, while the denominator is the total number of errors. Due to its definition, it is not related to the error rate; i.e.\ it is possible to have a low error rate (few errors) but a high hierarchical distance mistake (those errors are severe).

\paragraph{Error rate -- Hier.\ dist.\ mistake}
\label{par:error-rate-hier-dist-mistake}
In order to compare models at a glance, we can plot the error rate versus hierarchical distance mistake on a two-dimensional scatter plot.
The x-axis represents the amount of error (error rate), while the y-axis represents the severity of errors. Each point on the scatter plot corresponds to a trained model, and the overall best model is the one in the bottom-left corner.
Moreover, we can draw this plot for different levels in the hierarchy, highlighting the fact that milder errors at lower levels usually result in fewer errors at higher ones.
\begin{figure}[htbp]
  \ctikzfig{04/error-rate-hier-dist-mistake}
  \caption{Quadrants in error rate vs hier.\ dist.\ mistake plot}
  \label{fig:04/error-rate-hier-dist-mistake}
\end{figure}

\subsection{Hierarchical Distance}
\label{subsec:hierarchical-distance}
\emph{Hierarchical distance} is somewhat hybrid: it accounts for both the quality and quantity of errors. Its definition is almost identical to that of hierarchical distance mistake but instead of dividing by the number of errors, we divide by the total number of samples in the dataset, i.e.\
\begin{equation}
  \textrm{hier.\ dist.\ } :=
  \sum_{i, \, j} m_{ij}^l \, h_{ij}^l \bigg/ \sum_{i, \, j} m_{ij}^l.
  \label{eq:hierarchical-distance}
\end{equation}
If the model produces few errors, the off-diagonal values in the confusion matrix will be smaller, which are the only ones multiplied by non-zero values, hence the numerator is smaller. The denominator always stays the same, resulting in a lower value for hierarchical distance.

\section{Projections}
\label{sec:projections}
% TODO: Features vector, clustering, plots ...

\subsection{Encodings projections}
\label{subsec:encodings-projections}
\begin{itemize}
  \item introduce encodings projections
  \item are important for description encodings
  \item briefly explain UMAP
  \item  We use UMAP because it scale better for large number of points so we
    can reuse it in the next subsection
\end{itemize}

\subsection{Features projections}
\label{subsec:features-projections}
\begin{itemize}
  \item Visual inspections of model's features vectors (the last layer before
    output)
  \item Clustering metrics as measure for internal structured representation
\end{itemize}


\section{Adversarial Attacks}
\label{sec:adversarial-attacks-model-evaluation}

As briefly mentioned in~\Cref{sec:adversarial-attacks-related-work}, the term \emph{adversarial attack} refers to a manipulation by an adversarial party of inputs to a machine learning model with the purpose of controlling its outputs.

It is outside the scope of this work to develop models that can withstand sophisticated adversarial attacks as a defense mechanism. Instead, we focus on how models trained with different encodings and loss functions behave against the well-known adversarial attack: \acrfull{fgsm}. The key idea is to exploit this adversarial attack's searching capabilities in the image space to look for challenging examples and reevaluate the models on the modified inputs. In this view, the ability to resist an adversarial attack can be seen as a proxy for a ``good'' and structured internal representation of the model. In this regard, it is similar to the clustering metrics on feature projections in \Cref{subsec:features-projections}.

After a short digression on the types of adversarial attacks in \Cref{subsec:types-adversarial-attacks}, we will describe the \acrshort{fgsm} attack in \Cref{subsec:fgsm-attack}.

\subsection{Types of Adversarial Attacks}
\label{subsec:types-adversarial-attacks}
Regarding image classification, there are various non-mutually exclusive ways to classify types of adversarial attacks based on the context, technique, and goal of the attack.

Based on the adversary's knowledge, we can distinguish between \emph{white-box}, \emph{black-box}, and \emph{gray-box} attacks.
A \emph{white-box} attack is one in which the adversary has ``read permission'' to all components of the framework~(\Cref{fig:03/framework}) and ``write permission'' on the inputs of a model. Thus, the adversarial party can make use of the model's parameters, architecture, training data, etc., to craft the adversarial examples.
On the opposite end of the spectrum, a \emph{black-box} attack is one in which the adversary can only modify the inputs of the model and can read out the corresponding outputs; all other components of the framework are hidden.
Between these two extremes, there are various ``shades of gray'' in which the adversary has access to a proper subset of the components of the machine learning pipeline -- e.g.\ test and training datasets, model's architecture, loss function, etc. -- excluding the model's parameters.

Another important classification of adversarial attacks is based on the goal of the attack, one of which is the distinction between \emph{targeted} and \emph{untargeted} attacks.
A \emph{targeted} attack is said to be successful if the model misclassifies the adversarial example as a specific class chosen by the adversary. There is also the possibility that the model incorrectly predicts the class of the adversarial example but not the one chosen by the adversary; we refer to this as a \emph{misclassification}.
An \emph{untargeted} attack is said to be successful if the model misclassifies the adversarial example, independently of the predicted class.

Adversarial attacks can be \emph{poisoning attacks} or \emph{evasion attacks}.
The former is a type of attack performed during training in which the training dataset is manipulated to make the model learn an incorrect mapping between inputs and outputs. This implies that the adversarial party has access to or can redirect the training pipeline to a malicious dataset. These techniques are generally known as \emph{data poisoning} and are outside the scope of this work.
\emph{Evasion attacks} refer to adversarial attacks performed at inference time in which the manipulated input is fed to the model, and the corresponding output is observed.

\subsection{FGSM Attack}
\label{subsec:fgsm-attack}

The \acrfull{fgsm} is a \emph{white-box evasion} adversarial attack introduced by Goodfellow et al.~\cite{ExplainingAndGoodfe2014}. Like previous white-box adversarial attacks, it is fundamentally an optimization problem; specifically, finding the stationary point of a given objective with a constraint on the magnitude of the perturbation.

Let \gls{input} be the original image represented by a tensor of \gls{channel_size} $\times$ \gls{height_size} $\times$ \gls{width_size}.
The perturbation \gls{perturbation} is a tensor with the same dimensions as \gls{input}. The adversarial example \gls{perturbed-input} is the sum of the original image and the perturbation, i.e.
\begin{equation}
  \gls{perturbed-input} := \gls{input} + \gls{perturbation}
  \label{eq:perturbed-input}
\end{equation}
Different adversarial attacks have different methods for producing \gls{perturbation}.

\paragraph{Untargeted \acrshort{fgsm}} In the case of untargeted \acrshort{fgsm}, the perturbation \gls{perturbation} is defined as
\begin{equation}
  \gls{perturbation} :=
  \gls{signum} \, \left[\gls{grad} \Loss\left(\hat{y}, y\right)\right] \, \epsilon
  \label{eq:untargeted-perturbation}
\end{equation}
which is the sign of the gradient of the loss between the model's output \gls{output} and the encoding of the corresponding ground truth \gls{encoding} with respect to the input \gls{input}, rescaled by $\epsilon$. In other words, we are looking for a perturbation that increases or decreases each pixel value in the original image by a quantity $\epsilon$ such that the $\Loss\left(\hat{y}, y\right)$ is maximized. This effectively steers the model away from the correct classification.

\paragraph{Targeted \acrshort{fgsm}} A targeted attack aims to steer all model predictions towards a given class, called the \emph{target class}. Let \gls{class_i} be the target class and \gls{encoding_i} its corresponding encoding. The perturbation \gls{perturbation_i} is given by
\begin{equation}
  \gls{perturbation_i} :=
  - \gls{signum} \, \left[\gls{grad} \Loss\left(\hat{y}, y_i\right)\right] \, \epsilon.
  \label{eq:targeted-perturbation}
\end{equation}
The difference between \Cref{eq:targeted-perturbation} and \Cref{eq:untargeted-perturbation} is twofold. The minus sign turns the maximization problem into a minimization one. The quantity we try to minimize is the loss between the model's output \gls{output} and the encoding of the target class \gls{encoding_i}. This procedure effectively steers the model towards the target class, making it more likely that all inputs are classified as the chosen target class.\medskip

The parameter $\epsilon$ is fixed and controls the magnitude of the perturbation. Smaller values of $\epsilon$ produce a smaller perturbation, hence \gls{perturbed-input} will closely resemble the original \gls{input}, but the attack will be less effective. If a perturbation is sufficiently small, a \gls{perturbed-input} cannot be distinguished from \gls{input} by the naked eye but can still deceive the classifier. Conversely, if $\epsilon$ is too large, the corrupted input could be easily spotted by a human observer, and in the extreme case, make the content of \gls{input} barely recognizable. Various experiments for targeted and untargeted attacks with different values for $\epsilon$ are presented in \Cref{ch:results-and-discussion}.\medskip

Despite its simplicity, the \gls{fgsm} attack is quite effective, especially against models that do not implement a defense mechanism (see \Cref{sec:adversarial-attacks-results-and-discussion}), even for ``undetectable values'' of $\epsilon$. We reiterate that the \gls{fgsm} attack is used merely as an evaluation tool for probing the model's internal representation.
