\chapter{Injecting semantic information}
\label{ch:injecting-semantic-information}

% TODO: Subections on various encodings.
% TODO: Equivalence of losses and encodings.
%
Our goal is to inject semantic information into image classification models
without drastically disrupt a well-run system architecture which has proven
itself over time. After dissect a model in its components, we identify which
parts could be enhanced by the addition of extra information. In that way the
presented technique can be applied to other model architectures in a
straightforward manner.

\section{Framework}
\label{sec:framework}

In order to have a common ground for the following sections, we first need to
introduce a framework for model for image classification and define its building
blocks. Referring to \Cref{fig:1_framework} here are the components.

\begin{figure}[htbp]
  \ctikzfig{1_framework}
  \caption{Image classification framework}
  \label{fig:1_framework}
\end{figure}

\paragraph{input} Let \gls{input} be an image represented as a real-valued
tensor\footnote{The deep learning community use the term ``tensor'' as a
synonym for ``multidimensional array'', it is not equipped with additional
structure founded in physics or in mathematics. Throughout this thesis we use
the deep learning jargon} of shape \gls{channel_size} $\times$ \gls{height_size}
$\times$ \gls{width_size} where \gls{channel_size} is the number of channels,
\gls{height_size} is the height and \gls{width_size} is the width. Its entries
are the intensity value of colors associated to a RGB channels.

\paragraph{output} Let \gls{output} denote the output of the model, that is the
outcome of the application of \gls{model} on \gls{input}. This is
\gls{output_size}-dimensional real-valued vector. We use the more specific term
``model's predictions'' when \gls{output_size} is equal to the number of
classes and the vector is a \acrlong{pmf} over classes.

\paragraph{model} Let \gls{model} be a function that takes as input an image
and return a vector, i.e.
\begin{equation}
  \psi_\theta: \mathcal{X} \to \mathcal{Y}
     : x \mapsto \hat{y} := \psi_\theta \left(x\right)
  \label{eq:model}
\end{equation}
where \gls{input_set} $\subseteq \mathbb{R}^{C \times H \times W}$ is the set of
images and \gls{output_set} $\subseteq \mathbb{R}^{D}$ is the set of model's
output. At the higher level of abstraction a deep learning model can be seen as
a deterministic function of several real variables that output a real-valued
vector. Internally it is a composition of functions like matrix multiplications,
convolutions, non-linearities, etc. regulated by parameters $\theta$. Sometimes
the term ``architecture'' is used as synonym of ``model''.

\paragraph{parameters} Using \gls{parameters} as subscript of \gls{model}
reminds that the functions that constitute the model are parametrized by a set
of real number. These parameters are initially randomly initialized and then
adjusted during the training phase steering the model in such a way to solve
the given task.

\paragraph{class} We use interchangeably the terms ``class'' and ``label'' to
referring to the text associated to a given image. In the context of single
label image classification the class \gls{class} is word which is an element of
\gls{classes_set}, the set of possible classes.

\paragraph{encodings} Let \gls{encoding} denote a numerical representation of
the class. It is a \gls{output_size}-dimensional real-valued vector. The term
encoding must not be confused with the term embedding which can be regarded as
a specific type of encoding. An example of encoding is the one-hot encoding.

\paragraph{encoder} Let \gls{encoder} be a function that takes as input a class
\gls{class} and return its encoding \gls{encoding}, i.e.
\begin{equation}
  \phi: \mathcal{C} \to \mathcal{Y} : c \mapsto y := \phi \left(x\right).
  \label{eq:encoder}
\end{equation}
Usually the conversion from class text to numerical representation is not
explicitly included in the descriptions of deep learning models. However, here
we devote some attention to this aspect because it will be crucial in the
following as place to inject semantic information. Moreover, it must not be
confuse with the ``encoder block'' that can be found in some \acrshort{lm}
architectures.

\paragraph{loss} The loss or cost function, denote as \gls{loss}, is a function
that takes as input the output of the model and the encoding of the
corresponding class and return a scalar,
\begin{equation}
  \Loss: \mathcal{Y} \times \mathcal{Y} \to \mathbb{R}
       : \left(\hat{y}, y\right) \mapsto \Loss \left(\hat{y}, y\right).
  \label{eq:loss}
\end{equation}
It is a measure of how much the models' output and encodings differ, so lower
is its value better is the model's performance. Depending on the task and the
encoding different functional from can be used. An example of loss function is
the cross-entropy loss.

\medskip Assembling these components we obtain an end to end trainable model
for image classification. At first model's output carry no information about
classification for the input image. During training many images and their
corresponding labels are fed into the system so that the parameters
\gls{parameters} are tuned in such a way that the model's outputs become more
and more similar to the encodings of the corresponding class, hence the
emergence of classification capabilities. % TODO: explain a bit better
optimization Taking advantage of \acrshort{gpu}s, the calculation can be
parallelize along the batch dimension $B$, i.e. multiple images are
concurrently pass as input to the model so, at the implementation level,
\gls{input} will be a tensor \gls{batch_size} $\times$ \gls{channel_size}
$\times$ \gls{height_size} $\times$ \gls{width_size} and consequently the
output and the encoding will be tensors $B \times D$. \gls{model},
\gls{encoder} and \gls{loss} must be extended accordingly.


\section{Toy Dataset}
\label{sec:toy-dataset}
It is easier to explain and to understand hierarchical concepts with concrete
example. So here we introduce a minimalistic example based on toy dataset that
will come handy in the following. The classes are $\mathcal{C} = [\
\texttt{lemon},\ \texttt{pear},\ \texttt{apple},\ \texttt{dog},\ \texttt{cat},\
\texttt{car}\ ]$ and are arranged is a list-like structure so an index can be
associated to each class, i.e. $c_i \to i$. For example, using a zero-based
indexing, we have $\texttt{lemon} \to 0,\ \texttt{pear} \to 1,\ \ldots$. Theirs
hierarchical relationships are given by the tree in~\Cref{fig:2_toy-dataset}. At
the first level of the hierarchy we have the following partition:
\texttt{Lemon}, \texttt{pear} and \texttt{apple} are fruits, \texttt{dog} and
\texttt{cat} are animals while \texttt{car} is a vehicle. At the second level we
have separation between Natural and Artificial objects. The root node include all
the classes. The number of levels in this hierarchy is
\gls{hierarchy_levels}$\,=4$. Moreover suppose that this dataset contains 10
samples per class for a total of $N = 60$ samples.
\begin{figure}[htbp]
  \ctikzfig{2_toy-dataset}
  \caption{Toy dataset hierarchical tree}
  \label{fig:2_toy-dataset}
\end{figure}

\section{Encodings}
\label{sec:encodings}

Encodings are deterministic function that does not change during training, so
we can compute the encoding for each label of each image in the dataset before
training. So starting from a dataset $\mathcal{D} = \{ \left(x, c\right)_i
\}_{i = 1, \ldots, N}$ of \gls{dataset_size} images and their corresponding
classes, we obtain $\mathcal{D}' = \{ \left(x, y\right)_i \}_{i = 1, \ldots,
N}$, a set of images and their corresponding encodings. Detaching the encoding
procedure from training not only reduce the computational cost but also allow to
inspect the encoding before training.
%TODO: add reference to future section about encoding projection and evaluation


\subsection{One-hot Encoding}
\label{subsec:one-hot-encoding}

The one-hot encoding is a simple encoding that is usually coupled with the
cross-entropy loss function (see later~\cref{subsec:cross-entropy-loss}). We
consider this binomial our baseline and the following encoding-losses pairs are
develop in order to improve on various aspects while keeping comparable
performances. The encoder function \gls{encoder} that produce one-hot encoding is
\begin{equation}
  \phi: \mathcal{C} \to \{0, \,1\}^{D}
      : c_i \mapsto y_i \equiv \phi \left(c_i\right)
  \quad \text{where} \quad
  \phi{\left(c_i\right)}_j = \gls{kronecker_delta}_{ij}.
  \label{eq:one-hot-encoding}
\end{equation}
Referring to the toy dataset we have that one-hot encoding for \texttt{apple} is
$\left[0, 0, 1, 0, 0, 0\right]$. The resulting encodings will be sparse
orthogonal binary vectors. Theirs orthogonality translates into a well-separated
encoding space facilitating model training. Moreover, if it used in conjunction
with the cross-entropy loss, neat optimization tricks can be employed. Speed,
simplicity and effectiveness are the ingredients of its success.

The elephant in the room is that one-hot encoding discard all hierarchical
information accounting only for level zero of the hierarchy tree
(\Cref{fig:2_toy-dataset}). This means that encoding-wise \texttt{lemon} and
\texttt{pear} are as similar as \texttt{lemon} and \texttt{car} even though the
former have the same ancestor in the tree while the latter do not share any
node beside the root.

\paragraph{cosine similarity} The notion of encoding similarity employed
throughout this document is the \emph{cosine similarity}. Given two encoding
$y_1$ and $y_2$, the cosine similarity (or simply their similarity) is defined
as
\begin{equation}
  \gls{cosine_similarity}: \mathcal{Y} \times \mathcal{Y} \to \left[-1, +1\right]
  : (y_1, y_2) \mapsto \gls{cosine_similarity} \,(y_1, y_2) :=
  \frac{y_1 \cdot y_2}{\|y_1\| \, \|y_2\|}
  \label{eq:cosine-simiarity}
\end{equation}
where \gls{dot_product} is the dot product and \gls{l2_norm} is the $L^2$ norm.
If $y_1$ and $y_2$ have already been normalized, i.e. $\|y_1\| = \|y_2\| = 1$,
then the cosine similarity reduce the dot product and it is proportional to
Euclidean distance. Every one-hot encoding has cosine similarity equal to zero
with other encodings except for itself (which has it equal to one).


\subsection{Hierarchical Encodings}
\label{subsec:hierarchical-encodings}
We use the term \emph{hierarchical encodings} to designate all the encodings
that make use of semantic information coming from an hierarchy. As mentioned
in~\cref{par:hierarchies-tree}, a hierarchy can be represented as a tree data
structure, an example of which is given by the toy dataset
in~\Cref{fig:2_toy-dataset}. Tree's leaves are the classes while the ancestors
at various depth pack broader and broader concepts.

First we need to choose a \emph{metric on the tree} so we can calculate
pairwise distances between classes and organize results in a $|\mathcal{C}|
\times |\mathcal{C}|$ matrix. After apply a series of \emph{operations on the
rows} of this matrix we can directly read out encodings. Different hierarchical
encodings boils down to different choices of metric and operations.

\paragraph{Lower Common Ancestor}
The height of \acrfull{lca} can be used as a measure of distance between two
class on the hierarchy tree. The \acrshort{lca} of $c_i$ and $c_j$ is the node
that is the closest common ancestor of both $c_i$ and $c_j$, i.e.
\begin{equation}
  \gls{lca_fn} : \mathcal{C} \times \mathcal{C} \to \mathcal{V}:
  \left(c_i, c_j\right) \mapsto v_{ij} \equiv
  \gls{lca_fn} \, \left(c_i, c_j\right)
  \label{eq:lca}
\end{equation}
where \gls{vertices_set} is the set of vertices of the tree. The height of a
node is equal to its hierarchy level. So the height of the \acrshort{lca} is
\begin{equation}
  \gls{height_fn} \circ \gls{lca_fn} : \left(c_i, c_j\right) \mapsto h_{ij}
  \label{eq:lca-height}
\end{equation}
From \acrshort{lca} heights it can be derived a similarity measure between
classes, that is $s_{ij} := 1 - \sfrac{h_{ij}}{L}$. Referring to the toy
dataset, we have the pairwise lca heights~(\Cref{fig:lca-height-matrix}) and
lca similarities~(\Cref{fig:lca-similarity-matrix}).
\begin{figure}[htbp]
  \begin{subfigure}{0.45\textwidth}
    \ctikzfig{3_lca-heights}
    \caption{LCA height matrix: $h_{ij}$}
    \label{fig:lca-height-matrix}
  \end{subfigure}
  \begin{subfigure}{0.45\textwidth}
    \ctikzfig{4_lca-similarities}
    \caption{LCA similarity matrix: $s_{ij}$}
    \label{fig:lca-similarity-matrix}
  \end{subfigure}
  \caption{Matrix derived from \acrlong{lca}}
\end{figure}

\paragraph{Bertinetto et al.~\cite{MakingBetterMBertin2019}}
To produce the encoding, the authors apply a softmax function row-wise to a
negative rescaling of the lca heights matrix, that is hi
\begin{equation}
  f : h_i \mapsto y_i \equiv f(h_i)
  \quad \textrm{where} \quad
  f (h_i)_j := \frac{\exp \left({-\alpha \, h_{ij}}\right)}
  {\sum_j \exp\left({-\alpha \, h_{ij}}\right)}
  \label{eq:mbm-hierarchical-encoding}
\end{equation}
They used cross-entropy as loss function so the encoding must be a
\acrshort{pmf} over classes. Moreover they introduce an hyper-parameter $\alpha
\in [0, +\infty)$ that peaks the distribution around the index corresponding to
the encoded class. So, for small value of $\alpha$ the resulting encoding will
be a flat \acrshort{pmf}, while for bigger values it approaches one-hot
encoding.

\paragraph{Perotti et al.~\cite{BeyondOneHotPerott2023}} It's a similar
approach to the previous one but it differs in the normalization function and
in the role played by the hyper-parameter:
\begin{equation}
  f :  s_i \mapsto y_i \equiv f(s_i)
  \quad \textrm{where} \quad
  f (s_i)_j := \beta \, \delta_{ij} +
  \left(1 - \beta \right) \, \frac{\max \left(s_{ij},\,0\right)}
  {\sum_j \max \left(s_{ij},\,0\right)}.
  \label{eq:b3p-hierarchical-encoding}
\end{equation}
They started from \acrshort{lca} similarities matrix, then clip at zeros and
normalize row-wise. Finally a certain amount of one-hot encoding is added by
weighting terms with $\beta \in [0, 1]$. For $\beta = 1$ we get back to one-hot
encoding.

\paragraph{Barz and Denzler~\cite{HierarchyBasedBarz2018}} This approach is
different from the previous two. Instead of operating directly on the lca
matrix, they propose an algorithm to calculate embeddings $\phi(c_i)$ such that
\begin{equation}
  \forall \, c_i, c_j \in \mathcal{C} \qquad
  \phi(c_i) \cdot \phi(c_j) = s_{ij}.
  \label{eq:bd-hierarchical-encoding}
\end{equation}
It starts by choosing a normalized embedding $\phi(c_0)$ and then recursively
solve systems of linear equations to calculate the other embeddings.
Applied on the toy dataset, Barz and Denzler embeddings are calculate in the
following way:
\begin{equation*}
  \phi{\left(c_0\right)} := \left[1, 0, 0, 0, 0, 0\right]
  \quad \rightarrow \quad
  \begin{cases}
    \phi{\left(c_0\right)} \cdot \phi{\left(c_1\right)} = \sfrac{2}{3} \\
    \phi{\left(c_1\right)} \cdot \phi{\left(c_1\right)} = 1 \\
    \Rightarrow  \phi{\left(c_1\right)}
  \end{cases}
  \quad \rightarrow \quad
  \begin{cases}
    \phi{\left(c_0\right)} \cdot \phi{\left(c_2\right)} = \sfrac{2}{3} \\
    \phi{\left(c_1\right)} \cdot \phi{\left(c_2\right)} = \sfrac{2}{3} \\
    \phi{\left(c_2\right)} \cdot \phi{\left(c_2\right)} = 1 \\
    \Rightarrow \phi{\left(c_2\right)}
  \end{cases}
  \quad \rightarrow \quad \ldots
\end{equation*}

\subsection{Word Encodings}
\label{subsec:word-encoding}
For many datasets don't exist nor can be easily created hierarchies. However it
is really common that the classes are words or short sentences. That is the
information that \emph{word encodings} try to exploit. Even without the explicit
hierarchy provided in the toy dataset, every English speaker intuitively knows
that \texttt{lemon} and \texttt{pear} are more similar than \texttt{lemon} and
\texttt{car}. However, it is not clear what similar means in this context: is
\texttt{lemon} more similar to \texttt{pear} than to \texttt{apple}? To pin
point such measure we resort to \emph{word embeddings}, i.e. latent words
representations of pre-trained \acrshort{lm}. These are real-valued
$\gls{output_size}$-dimensional vectors on which we can applied the previously
defined notion of cosine similarity.
\begin{equation}
  g : \gls{classes_set} \to \mathbb{R}^{\gls{output_size}} :
  c_i \mapsto e_i \equiv g(c_i)
  \label{eq:word-encoding}
\end{equation}
where the $g$ is the embedding function of the pre-trained \acrshort{lm}.

Many of the works mentioned in the~\Cref{subsec:embeddings} relate to
\acrshort{zsl} and train a word embedder alongside the image classifier. Perotti
et al.~\cite{BeyondOneHotPerott2023} use GloVe~\cite{GloveGlobalVPennin2014} and
as a pre-trained word embedder looking for model performance improvements
regarding ``quality and quantity'' of errors. The pre-trained GloVe model
can be though as a big dense matrix where each row correspond to a word in the
training dictionary. Then they read out the corresponding rows and finally
calculate pairwise cosine similarity obtaining $s_{ij}$. From $s_{ij}$, they
again use~\Cref{eq:b3p-hierarchical-encoding} to get the encoding.

However, upon closer examination, words embeddings approach has two obvious
flaws. What if the dataset classes' words that are not in the GloVe dictionary?
Is \texttt{apple} the fruit or the company? The first problem can sometimes be
patch by the use of a synonym available in the GloVe dictionary. Other times,
the words are so specific (e.g. scientific names in iNaturalist) that makes no
sense substitute with similar words. In the worst case a new embedder $g$ can be
trained from scratch making sure that the training text corpora contain the
relevant words. The second problem is far more rooted in the word embeddings
themselves. Same word can have different meanings (homographs) and the single
vector return by GloVe-like model does not make any distinction. They only way
to overcome this issue is to provided more context about the embedded word,
hence description encodings.

\subsection{Description Encodings}
\label{subsec:description-encodings}
% Overcome homographs problem
% Overcome out-of-dictionary problem
% Exploit next gen language model aka LLM
% Recently really good for compressing information
% We have fine control on what to inject
% Steerable

\section{Loss Functions}
\label{sec:losses}

\subsection{Cross-entropy Loss}
\label{subsec:cross-entropy-loss}

\subsection{Cosine distance Loss}
\label{subsec:cosine-distance-loss}
