\begin{frame}{Hierarchical Encondings}
  Hierarchical encodings are derived from \alert{hierarchical trees}.

  \metroset{block=fill}
  \begin{block}{Example}
    \begin{align*}
      \mathcal{C} &= [
        &\texttt{lemon} &,  &\texttt{pear} &,  &\texttt{apple} &,
        &\texttt{dog} &, &\texttt{cat} &, &\texttt{car} &\
      ]
    \end{align*}
    \tikzfig{figures/example_hierarchical_tree}
  \end{block}

  \note[item]{Una gererchia tra le classi è un'informazione che possiamo
  sfruttare per costruire una funzione di encoding che tenga conto delle
  relazioni tra queste.}
  \note[item]{Un modo di rappresentare una gerachia è con una struttura ad
  albero.}
  \note[item]{Limone, pera e mela sono frutti, cane e gatto animali mentre l'auto è
  un veicolo. Frutti e animali sono presenti in natura, i veicoli sono
  oggetti artificiali.}
  \note[item]{Questo è un esempio di gerarchia dalla quale possiamo
  derivare un embedding gerarchico.}
\end{frame}

\begin{frame}{Hierarchical Encondings}
  \begin{columns}
    \column{0.7\textwidth}
    \only<1>{\tikzfig{figures/example_lca}}
    \only<2>{\tikzfig{figures/example_similarity}}
    \only<3>{\tikzfig{figures/example_probability}}
    \only<4>{\tikzfig{figures/example_probability_apple}}

    \column{0.3\textwidth}
    \begin{enumerate}[<+- | alert@+>]
      \item distance
      \item similarity
      \item probability
    \end{enumerate}
  \end{columns}

  \note[item]{Iniziamo costruendo una matrice di distanza tra le classi per
  esempio usando l'altezza dell'ultimo antenato comune. Ogni classe ha distanza
  0 da se stessa. I frutti hanno distanza 1 tra loro, 2 con gli animali e 3,
  distanza massima con i veicoli.}
  \note[item]{Dalla distanza construiamo una matrice di similarità. 1 -
  distanza normalizzata. Le classi avranno similarità 1 con se stesse e 0 con
  quelle con cui non hanno niente in comune.}
  \note[item]{Infine, otteniamo una probability mass function sulle classi
  applicando la softmax lungo le righe. E ora ogni righa corrisponde
  all'encoding di una classe.}
  \note[item]{NEXT}
  \note[item]{Consideriamo l'encoding per la classe "mela".}
\end{frame}

\begin{frame}{Hierarchical Encondings}
  \only<1>{
    \centering
    similarity $\longrightarrow$ probability
    \vspace{-0.0cm}
    \input{figures/hier_cross-entropy_1.pgf}
  }
  \only<2>{
    \centering
    similarity + hyperparam $\longrightarrow$ probability
    \vspace{-0.0cm}
    \input{figures/hier_cross-entropy_2.pgf}
  }
  \only<3>{
    \centering
    $\mathcal{L} = - \phi\left(\texttt{apple}\right) \cdot \log \psi_\theta\left(x\right)$
    \vspace{-0.0cm}
    \input{figures/hier_cross-entropy_3.pgf}
  }
  \only<4>{
    \centering
    $\mathcal{L} = - \phi\left(\texttt{apple}\right) \cdot \log \psi_\theta\left(x\right)$
    \vspace{-0.0cm}
    \input{figures/hier_cross-entropy_4.pgf}
  }
 
  \note[item]{Il risulato della softmax è una distruibuzione di probabilità in
    cui non è particolarmente evidente la classe di partenza e ciò ostacola il
    training.}
  \note[item]{NEXT}
  \note[item]{Per ovviare a questo problema possiamo introdurre un
    iperparamentro che riscalando le similarià procude un encoding che risulta
    più efficiace nel training.}
  \note[item]{NEXT}
  \note[item]{Come prima partiamo da una inizializzazione casuale del modello e
    usiamo la cross entropy come funzione di loss.}
  \note[item]{NEXT}
  \note[item]{Stavolta, tutte le componenti di $\psi$ contribuiscono e, agendo
    i paramentri $\theta$, sono aumentate o ridotte per allinersi all'encoding.}
\end{frame}

\begin{frame}{Hierarchical Encondings}
  \begin{figure}
    \centering
    \includegraphics[width=.8\linewidth]{figures/paper/title.pdf}
  \end{figure}
  \begin{figure}
    \centering
    \includegraphics[width=.8\linewidth]{figures/paper/errors.pdf}
  \end{figure}
  \begin{figure}
    \centering
    \includegraphics[width=.8\linewidth]{figures/paper/features.pdf}
  \end{figure}

  \note{\scriptsize{Questa tecnica per sfruttare le reazioni tra classi usando un
    esplicita gerarchia, è stata già descritta in alcune pubblicazioni.
    Una di queste è un articolo che sarà presentato a xAI 2023.}}
  \note[item]{Abbiamo applicato una simile tecnica per produrre un embedding
    gerarchico e confrontato le performance dei modelli con quelle ottenute
    che sfruttano e non la gerachica tra le classi.}
  \note[item]{Per performance intendo la quatità di errori, asse orizzontale, e
    la qualità degli errori, asse verticale, a diversi livelli della gerarchica.
    Questi plots saranno ripresi in seguito.}
  \note[item]{Abbiamo inoltre studiato la disposizione spaziale dei features
    vectors, ovvero come sono organizzati nello spazio i vettori dal penultimo
    livello del modello associati alle varie immaginni ottenuti}
  \note[item]{Proiettarli nel piano dà un'indicazione su quali immagini il
    modello pensa siano simili. Ciò che ci attendiamo è la comparsa di cluster
    relativi alle varie classi.}
  \note[item]{Abbiamo usato delle metriche di clustering per quantificare il
    raggruppamento dei features vectors e capire quali modelli producevano una
    rappresentazione interna più strutturata.}
  \note[item]{Un limite di tale approccio è la necessità di avere o poter
    costruire una gerarchica tra le classi. Uno degli approcci presentati supera
    tale limite ricavando gli encoding dai word embeddings delle classi. Ma ciò
    introduce un altro problema, non tutte le parole hanno un'embedding e una
    parola può avere diversi un significati.}
  \note[item]{Da qui l'idea per i descriptions encodings.}
\end{frame}
