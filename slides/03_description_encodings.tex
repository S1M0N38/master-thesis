\begin{frame}{Description Encodings}
  Recipe:
  \begin{enumerate}
    \item Use Language Model (e.g. chatGPT) to \alert{generate descriptions}\\
      for each class.
    \item Use Embedding Model (e.g. BERT-like) to \alert{generate embeddings}\\
      for each description.
    \item Use Dimensionality Reduction (e.g. t-SNE) to \alert{generate encodings}\\
      for each embedding.
  \end{enumerate}

    \note[item]{La ricetta per generarli è la seguente.}
  \note[item]{"Uno": usare modelli generativi di testo per produrre
    delle descrizioni delle classi.}
  \note[item]{"Due": usate modelli di embeddings che convertano le descrizioni
    testuali in un vettore di numeri reali detto embeddings. Descrizioni simili
    avranno embeddings simili.}
  \note[item]{"Tre": usare un algoritmo di riduzione della dimensionalità
    mappare gli embeddings in uno spazio di dimensionalità inferiore. Inoltre
    agendo sugli iperparametri di tali algoritmi e possibile modificare la
    distiribuzione spaziale, preservando le relazioni di similarità.}
  \note[item]{Il risulato sono i Description Encodings.}
\end{frame}

\begin{frame}{Description Encodings}
  \begin{itemize}
    \item \texttt{lemon}: \emph{\small``Lemons are \alert{oval-shaped} fruits
      known for their \alert{bright yellow} color and acidic juice.''}
    \item \texttt{pear}: \emph{\small``Pears are fruits a with \alert{rounded
      bottom} and a narrower, \alert{elongated top}.''}
    \item \texttt{apple}: \emph{\small``Apples are \alert{round} fruits that
      come in a variety of colors, including \alert{red}, \alert{green}, and
      \alert{yellow}.''}
  \end{itemize}
  \note[item]{Queste sono esempi di parti di descrizioni di alcune delle
    classi ottenute da un modello generativo.}
  \note[item]{I limoni sono frutti ovali di un giallo brillante, le pere
    hanno la base rotonda e la parte superiore allungata, le mele sono
    tondeggianti rosse, verdi e gialle. Tutte carattiristiche che aiutano a
    distingure visivamente le diverse classi.}
\end{frame}

\begin{frame}{Description Encodings}
  \begin{columns}
    \column{0.65\textwidth}
    \tikzfig{figures/example_embeddings}

    \column{0.35\textwidth}
    \textbf{\textsc{Desideratum}}\\
    \vspace{0.2cm}
    \emph{Distinctive features in the descriptions translate to
    a hierarchy-like structure in the encodings space}
  \end{columns}

  \note[item]{Dalle descrizioni passiamo agli embedding e da questi agli
    encoding. Questa è una rappresentazione 2D semplificata degli encoding.}
  \note[item]{La speranza è che le caratteristiche distintive presenti nelle
    descrizioni si traducano in una struttura gerarchica nello spazio degli
    encoding. Ciò è una conferma che i descriptions encoding riescono a
    ricavare una gerarchia tra le classi dal significato delle descrizioni.}
  \note[item]{Tuttavia gerarchia e similarità tra classi sono concetti
    soggettivi e non necessariamente collegati.}
  \note[item]{Come misura di similarità tra due encodings utilizziamo l'angolo
    tra i due vettori. Mela e pera sono più simili di mela e cane.}
\end{frame}

\begin{frame}{Description Encodings}
  \begin{columns}
    \column{0.65\textwidth}
    \only<1>{\tikzfig{figures/example_embeddings_apple1}}
    \only<2>{\tikzfig{figures/example_embeddings_apple2}}

    \column{0.35\textwidth}
    Align $\psi_\theta$ to $\phi$ with
    \alert{Cosine Distance}.
    \begin{align*}
      \phi &:
      \mathcal{C} \rightarrow \mathbb{R}^{d} \\
      \psi_\theta &:
      \mathcal{X} \rightarrow \mathbb{R}^{d}
    \end{align*}
    \begin{equation*}
      \mathcal{L} := 1 -
      \frac{
        \psi_\theta \cdot \phi
      }{
        \left|\psi_\theta\right|_2
        \left|\phi\right|_2
      }
    \end{equation*}
  \end{columns}

  \note[item]{Consideriamo l'encoding per mela (in blu) e l'output del
    modello non addestrato (arancione).}
  \note[item]{Ora $\phi$ mappa dall'inisieme delle classi a $\mathbb{R}^d$ dove
    la dimensione $d$ e stabilita dall'algoritmo di riduzione di
    dimensionalità. $\mathbb{R}^d$ è anche lo spazio degli output del modello
    che non sono più distribuzioni di probabilità ma semplici vettori con
    componeti reali.}
  \note[item]{Il training consiste nel modificare i parametri $\theta$ in modo
    tale da allineare l'output all'encoding, quindi minimizzare l'angolo compreso.}
  \note[item]{NEXT}
  \note[item]{In questo caso la funzione di loss è la Cosine Distance.}
  \note[item]{La classe predetta dal modello sarà quindi quella associata al
    vettore di encoding più vicino all'output del modello.}
\end{frame}

\begin{frame}{Encondings Comparison}
  \begin{block}{\textsc{One-hot Encoding + Cross Entropy Loss}}
    \begin{itemize}
      \item[\cmark] Battle tested
      \item[\xmark] Ignore classes similarities
    \end{itemize}
  \end{block}
  \begin{block}{\textsc{Hierarchical Encoding + Cross Entropy Loss}}
    \begin{itemize}
      \item[\cmark] Exploit classes similarities
      \item[\xmark] Require a hierarchy
    \end{itemize}
  \end{block}
  \begin{block}{\textsc{Description Encoding + Cosine Distance Loss}}
    \begin{itemize}
      \item[\cmark] Exploit classes similarities
      \item[\cmark] Do not require a hierarchy
    \end{itemize}
  \end{block}

  \note[item]{Riassumendo. One-hot encoding è largamente utilizzato e dà buoni
  risulati. È semplice da implementare e non necessita di informazione extra.
  Tuttavia non tiene conto delle relazioni tra le cassi con la conseguenza che
  scambiare una mela con una pera è equivalente a scambiare una mela con un
  automobile.}
  \note[item]{Hierarchical encoding è una codifica che sfrutta le relazioni tra
  classi basandosi su un'esplicita gerarchia. È quindi applicaile a quei dataset
  per i quali esite o è possibile costruire una gerarchia. Sperimentalmente si
  verifica che è sensibile agli iperparametri conivolti e alla scelta delle
  funzioni che costruiscono le distanze e le probabilità.}
  \note[item]{Infine i modelli che fanno uso di Description Encodings sfruttano
  la relazioni tra classi senza richedere una gerarchica. Costruiscono tali
  relazioni basandosi le conoscenze semantiche apprese dai modelli generativi e
  di embeddings.}
\end{frame}
