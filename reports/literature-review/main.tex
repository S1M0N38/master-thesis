\documentclass[a4paper, 10pt]{article}

\usepackage{titlesec}

% Math related packages
\usepackage{amsfonts}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{bm}

% Cite from bibliography
\usepackage[backend=biber, style=alphabetic]{biblatex}
\addbibresource{bibliography.bib}


\usepackage{algorithm}
\usepackage{algpseudocode}

\titleformat{\section}
{\vspace{2.5em}\titlerule\vspace{1em}\bfseries\centering}
{}{0em}{}

\addtolength{\topmargin}{-2.5cm}
\addtolength{\textheight}{+2.5cm}
\addtolength{\footskip}{+0.5cm}


% use := for "is defined to be equal to"
% http://projekte.dante.de/DanteFAQ/Symbole#20
\mathchardef\ordinarycolon\mathcode`\:
\mathcode`\:=\string"8000
\begingroup \catcode`\:=\active
  \gdef:{\mathrel{\mathop\ordinarycolon}}
\endgroup

% Custom Math Symbol
\newcommand{\loss}{\ell}
\newcommand{\Loss}{\mathcal{L}}
\newcommand{\transpose}{\intercal}

\begin{document}

\section{Notation}

\paragraph{Inputs} What are we feeding the model with. A single input is denote
with $\bm{x}$, a batch of inputs with $\bm{X}$. For example in CIFAR-10
$\bm{x}$ is single image $32 \times 32$ with 3 color channel (a tensor $3
\times 32 \times 32$, channel $\times$ height $\times$ width). A batch of 64
images will be denoted $\bm{X}$ and is a tensor $64 \times 3 \times 32 \times
32$. $|\bm{X}|$ is the size of batch dimension.

\paragraph{Classes} We are concern with classification task. Let $K$ be the set
of classes and $|K|$ its cardinality. For example in CIFAR-10 $K = \{
  \textrm{airplane}, \ldots, \textrm{truck} \} = \{k_1, \ldots, k_{10}\}$ and
$|K| = 10$. Let $g_{(\bm{x})} \in K$ be the class associated to the input
$\bm{x}$ (ground--truth class). When the $\bm{x}$ is clear from the context the
index for $g$ will be dropped.
%TODO be more precise about ordering in a set. g is used as a index

\paragraph{Labels} Labels are the numerical encoding of classes. Each $\bm{x}$
belong to only one class $g$. In this setting one popular encoding scheme is
\emph{one--hot encoding}.
\begin{equation*}
  \textrm{one--hot}: \,
  K \longrightarrow \{0, 1\}^{|K|} \, : \,
  g_{(\bm{x})} \longmapsto \bm{y}_{(\bm{x})} \quad \textrm{with components}
  \quad \bm{y}_j := \delta_{g,j}\
\end{equation*}
Applying an encoding scheme to the batch of labels produce $\bm{Y}_{(\bm{X})}$.

\paragraph{Outputs} What model return. Let $\bm{z}_{(\bm{x})} \in
\mathbb{R}^{|K|}$ be the outputs of the model given the input $\bm{x}$. To
output prediction as probability vector, usually denoted by $\hat{\bm{y}}$,
$\bm{z}$ must be normalized.
\begin{equation*}
  P_K : \, \mathbb{R}^{|K|} \rightarrow [0, 1]^{|K|} \, : \,
  \bm{z}_{(\bm{x})} \mapsto \hat{\bm{y}}_{(\bm{x})} = P_K(\bm{z}_{(\bm{x})})
  \quad \textrm{with} \quad
  P_K(\bm{z})_j := \frac{e^{\bm{z}_j}}{\sum_{k \in K} e^{\bm{z}_k}}
\end{equation*}
$P_K(.)$ is an alternative notation for the usual \emph{softmax} function. Be
more explicit about the underlying classes $K$ give us more flexibility for
constructing custom loss function.

\paragraph{Losses} In the following losses are define as a pointwise operation
that take $\bm{x}$ and its corresponding label $\bm{y}$ as input and return a
scalar. Pointwise function apply to a batch can be combined with a reduction
operator $\bigoplus$ (e.g. sum or mean).

\begin{equation*}
  \loss : (\bm{x}, \bm{y}) \longmapsto \loss(\bm{x}, \bm{y}) \qquad
  \Loss : (\bm{X}, \bm{Y}) \longmapsto \Loss(\bm{X}, \bm{Y}) :=
  \bigoplus_{(\bm{x}, \bm{y}) \in (\bm{X}, \bm{Y})} \loss(\bm{x}, \bm{y})
\end{equation*}


\section{Cross Entropy Loss (XE)}
Standard Cross--Entropy only uses the output vector component corresponding to
the ground--truth label (i.e. the $g$ component of the output vector). Training
process force $\hat{\bm{y}}_g$ to increase and, due to normalization, other
components shrinks.
\begin{equation}
  \loss_{\textrm{XE}} := - \bm{y}^{\transpose} \log{\hat{\bm{y}}}
  = - \log{\hat{\bm{y}}_g}
  \quad \longrightarrow \quad
  \Loss_{\textrm{XE}} := \frac{1}{|\bm{X}|} \sum \loss_{\textrm{XE}}
\end{equation}

\section{Complement Entropy Loss (CE)}
\cite{1903.01182v2} introduce \emph{Complement Entropy Loss}. CE take as input the
output vector $\bm{z}$, remove its $g$ component and renormalise it  obtaining
$P_{K\setminus\{g\}}(\bm{z})$. CE is minus the Shannon Entropy $H(.)$ of this
new vector.
\begin{equation}
  \loss_{\textrm{CE}} := - H(P_{K\setminus\{g\}}(\bm{z}))
  \qquad \longrightarrow \qquad
  \Loss_{\textrm{CE}} := \frac{1}{|\bm{X}|} \sum \loss_{\textrm{CE}}
\end{equation}
Maximise the entropy of a distribution force it towards a flatter one (in these
setting the distribution with maximum entropy is the uniform distribution). CE
is paired with XE using a custom training loop:

\begin{algorithm}
\caption{Custom Training Loop for CE}
\label{alg:CE}
\begin{algorithmic}[1]
  \For{$step$ in $steps$}
    \State Update parameters of the model using $\Loss_{\textrm{XE}}$
    \State Update parameters of the model using $\Loss_{\textrm{CE}}$
  \EndFor
\end{algorithmic}
\end{algorithm}

They also proposed an empirical modification to balance the contribution that
came from CE~\footnote{In the paper they propose a rescaling factor of $|K| -
1$ but in the source code they use $|K|$. The similar inconsistencies appear in
others normalized loss functions.}
\begin{equation*}
  \loss_{\textrm{CE}}' := \frac{\loss_{\textrm{CE}}}{|K|-1}
  \qquad \longrightarrow \qquad
  \Loss_{\textrm{CE}}' := \frac{1}{|\bm{X}|} \sum \loss_{\textrm{CE}}'
\end{equation*}

\section{Guided Complement Entropy Loss (GCE)}
\cite{1903.09799v3} improved upon CE by adding an additional term in the
$\loss_{\text{CE}}$ that leverage the variation in model confidence during the
training phase, i.e.
\begin{equation}
  \loss_{\textrm{GCE}} := 
  {\left[ \hat{\bm{y}}_g \right]}^{\alpha} \, \loss_{\textrm{CE}}
  \qquad \longrightarrow \qquad
  \Loss_{\textrm{GCE}} := \frac{1}{|\bm{X}|} \sum \loss_{\textrm{GCE}}
\end{equation}
The new factor ${\left[ \hat{\bm{y}}_g \right]}^{\alpha}$ is called the
\emph{guiding factor}. At the beginning of training it is small (the model
outputs low probability for the g-component of $\hat{\bm{y}}$) and then it
increase with training (the model get better and the g-component of
$\hat{\bm{y}}$ will be higher). $\alpha$ is a fixed hyperparameter ($\alpha =
0.2$ works reasonably well). \\

GCE is the only loss used in a standard training loop, i.e.
\begin{algorithm}
\caption{Standard Training Loop for GCE}
\label{alg:GCE}
\begin{algorithmic}[1]
  \For{$step$ in $steps$}
    \State Update parameters of the model using $\Loss_{\textrm{GCE}}$
  \EndFor
\end{algorithmic}
\end{algorithm}

Similar to \cite{1903.01182v2}, they modify $\Loss_{\textrm{GCE}}$ to account
for the number of classes
\begin{equation*}
  \loss_{\textrm{GCE}}' := \frac{\loss_{\textrm{GCE}}}{\log(|K|-1)}
  \qquad \longrightarrow \qquad
  \Loss_{\textrm{GCE}}' := \frac{1}{|\bm{X}|} \sum \loss_{\textrm{GCE}}'
\end{equation*}

\section{Hierarchical Complement Entropy (HCE)}
\cite{1911.07257v1} try to exploit hierarchical labels in CIFAR-100. Let $G$
be a set that contains the siblings classes that belong to the same parental
class of the ground--truth class, that is $g \in G$ and $G \subseteq K$. The
\emph{Hierarchical Complement Entropy} is
\begin{equation}\label{eq:HCE}
  \loss_{\textrm{HCE}} :=
  - H(P_{G\setminus\{g\}}(\bm{z}))
  - H(P_{K\setminus\{G\}}(\bm{z}))
  \quad \longrightarrow \quad
  \Loss_{\textrm{HCE}} := \frac{1}{|\bm{X}|} \sum \loss_{\textrm{HCE}}
\end{equation}

\cite{1911.07257v1} empolyed HCE in two training loops: the classic single step
training loop and in the double steps training loop for direct comparison with
\cite{1903.01182v2}
\begin{algorithm}
\caption{Standard Training Loop for XE + HCE}
\label{alg:XE+HCE}
\begin{algorithmic}[1]
  \For{$step$ in $steps$}
    \State Update parameters of the model using
    $\Loss_{\textrm{XE}} + \Loss_{\textrm{HCE}}$
  \EndFor
\end{algorithmic}
\end{algorithm}
\begin{algorithm}
\caption{Custom Training Loop for HCE}
\label{alg:XE.HCE}
\begin{algorithmic}[1]
  \For{$step$ in $steps$}
    \State Update parameters of the model using $\Loss_{\textrm{XE}}$
    \State Update parameters of the model using $\Loss_{\textrm{HCE}}$
  \EndFor
\end{algorithmic}
\end{algorithm}

Normalized version of $\loss_{\textrm{HCE}}$ can be obtained dividing each
$H(.)$ in \eqref{eq:HCE} by the number of classes involved
\begin{equation*}
  \loss_{\textrm{HCE}}' :=
  - \frac{H(P_{G\setminus\{g\}}(\bm{z}))}{|G| - 1}
  - \frac{H(P_{K\setminus\{G\}}(\bm{z}))}{|K| - |G|}
  \qquad \longrightarrow \qquad
  \Loss_{\textrm{HCE}}' := \frac{1}{|\bm{X}|} \sum \loss_{\textrm{HCE}}'
\end{equation*}

\section{Hierarchical Guided Complement Entropy Loss (HGCE)}
Following the same reasoning in \cite{1903.09799v3}, we take HCE and add
the guiding factor ${\left[ \hat{\bm{y}}_g \right]}^{\alpha}$.
\begin{equation}
  \loss_{\textrm{HGCE}} := 
  {\left[ \hat{\bm{y}}_g \right]}^{\alpha} \, \loss_{\textrm{HCE}}
  \quad \longrightarrow \quad
  \Loss_{\textrm{HGCE}} := \frac{1}{|\bm{X}|} \sum \loss_{\textrm{HGCE}}
\end{equation}

Standard (single step) training loop is employed using HGCE as the only criterion
\begin{algorithm}
\caption{Standard Training Loop for HGCE}
\label{alg:HGCE}
\begin{algorithmic}[1]
  \For{$step$ in $steps$}
    \State Update parameters of the model using $\Loss_{\textrm{HGCE}}$
  \EndFor
\end{algorithmic}
\end{algorithm}

Combining ideas in GCE and HCE the normalized version will be
\begin{equation*}
  \Loss_{\textrm{HGCE}}' := \frac{1}{|\bm{X}|} \sum
  {\left[ \hat{\bm{y}}_g \right]}^{\alpha} \,
  \left(
  - \frac{H(P_{G\setminus\{g\}}(\bm{z}))}{\log(|G| - 1)}
  - \frac{H(P_{K\setminus\{G\}}(\bm{z}))}{\log(|K| - |G|)}
  \right)
\end{equation*}

\section{Results}
Missing results are not available at the moment due to a bug in the training script.

\begin{table}[!h]
  \centering
  \begin{tabular}{|l|l|c|c|c|c|}
  \hline
  \textbf{Ref} & \textbf{Loss} & \textbf{Baseline} & \textbf{FGSM} & \textbf{I-FGSM} & \textbf{PGD} \\ \hline
  Che+19a & XE & 0.9176/1.0000 & 0.1044/1.0000 & 0.0795/1.0000 & 0.0173/1.0000 \\
  Che+19a & XE,CE & - & - & - & - \\ \hline
  \end{tabular}
  \label{tab:ResNet110_CIFAR-10}
  \caption{ResNet110 on CIFAR-10, Accuracy/LCA.}
\end{table}
\begin{table}[!h]
  \centering
  \begin{tabular}{|l|l|c|c|c|c|}
  \hline
  \textbf{Ref} & \textbf{Loss} & \textbf{Baseline} & \textbf{FGSM} & \textbf{I-FGSM} & \textbf{PGD} \\ \hline
  Che+19a & XE & 0.7011/1.6577 & 0.0112/1.9649 & 0.0260/1.9175 & 0.0077/1.9438 \\
  Che+19a & XE,CE & - & - & - & - \\ \hline
  \end{tabular}
  \label{tab:ResNet110_CIFAR-100}
  \caption{ResNet110 on CIFAR-100, Accuracy/LCA.}
\end{table}
\begin{table}[!h]
  \centering
  \begin{tabular}{|l|l|c|c|c|c|}
  \hline
  \textbf{Ref} & \textbf{Loss} & \textbf{Baseline} & \textbf{FGSM} & \textbf{I-FGSM} & \textbf{PGD} \\ \hline
  Che+19b & XE & 0.9269/1.0000 & 0.0991/1.0000 & 0.0426/1.0000 & 0.0038/1.0000 \\
  Che+19b & GCE & 0.9297/1.0000 & 0.1195/1.0000 & 0.1306/1.0000 & 0.0959/1.0000 \\ \hline
  \end{tabular}
  \label{tab:ResNet56_CIFAR-10}
  \caption{ResNet56 on CIFAR-10, Accuracy/LCA.}
\end{table}
\begin{table}[!h]
  \centering
  \begin{tabular}{|l|l|c|c|c|c|}
  \hline
  \textbf{Ref} & \textbf{Loss} & \textbf{Baseline} & \textbf{FGSM} & \textbf{I-FGSM} & \textbf{PGD} \\ \hline
  Che+19b & XE & 0.6677/1.6759 & 0.0128/1.9599 & 0.0259/1.9255 & 0.0097/1.9555 \\
  Che+19c & XE,CE & 0.6866/1.6643 & 0.0102/1.9580 & 0.0258/1.9431 & 0.0102/1.9592 \\
  Che+19b & GCE & 0.6749/1.6626 & 0.0153/1.9503 & 0.0669/1.9130 & 0.0315/1.9394 \\
  Che+19c & XE,HCE & 0.6971/1.6418 & 0.0098/1.9655 & 0.0278/1.9355 & 0.0126/1.9600 \\
  Che+19c & HGCE & 0.6777/1.6131 & 0.0165/1.9514 & 0.0741/1.9155 & 0.0313/1.9270 \\ \hline
  \end{tabular}
  \label{tab:ResNet56_CIFAR-100}
  \caption{ResNet56 on CIFAR-100, Accuracy/LCA.}
\end{table}

\newpage
\printbibliography

\end{document}
