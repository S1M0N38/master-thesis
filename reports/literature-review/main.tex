\documentclass[a4paper, 10pt]{article}

\usepackage{titlesec}

% Math related packages
\usepackage{amsfonts}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{bm}

% Cite from bibliography
\usepackage[backend=biber, style=alphabetic]{biblatex}
\addbibresource{bibliography.bib}


\usepackage{algorithm}
\usepackage{algpseudocode}

\titleformat{\section}
{\vspace{2.5em}\titlerule\vspace{1em}\bfseries\centering}
{}{0em}{}

\addtolength{\topmargin}{-2.5cm}
\addtolength{\textheight}{+2.5cm}
\addtolength{\footskip}{+0.5cm}


% use := for "is defined to be equal to"
% http://projekte.dante.de/DanteFAQ/Symbole#20
\mathchardef\ordinarycolon\mathcode`\:
\mathcode`\:=\string"8000
\begingroup \catcode`\:=\active
  \gdef:{\mathrel{\mathop\ordinarycolon}}
\endgroup

% Custom Math Symbol
\newcommand{\loss}{\ell}
\newcommand{\Loss}{\mathcal{L}}
\newcommand{\transpose}{\intercal}

\begin{document}

\section{Notation}

\paragraph{Inputs} What are we feeding the model with. A single input is denote
with $\bm{x}$, a batch of inputs with $\bm{X}$. For example in CIFAR-10
$\bm{x}$ is single image $32 \times 32$ with 3 color channel (a tensor $3
\times 32 \times 32$, channel $\times$ height $\times$ width). A batch of 64
images will be denoted $\bm{X}$ and is a tensor $64 \times 3 \times 32 \times
32$. $|\bm{X}|$ is the size of batch dimension.

\paragraph{Classes} We are concern with classification task. Let $K$ be the set
of classes and $|K|$ its cardinality. For example in CIFAR-10 $K = \{
  \textrm{airplane}, \ldots, \textrm{truck} \} = \{k_1, \ldots, k_{10}\}$ and
$|K| = 10$. Let $g_{(\bm{x})} \in K$ be the class associated to the input
$\bm{x}$ (ground--truth class). When the $\bm{x}$ is clear from the context the
index for $g$ will be dropped.
%TODO be more precise about ordering in a set. g is used as a index

\paragraph{Labels} Labels are the numerical encoding of classes. Each $\bm{x}$
belong to only one class $g$. In this setting one popular encoding scheme is
\emph{one--hot encoding}.
\begin{equation*}
  \textrm{one--hot}: \,
  K \longrightarrow \{0, 1\}^{|K|} \, : \,
  g_{(\bm{x})} \longmapsto \bm{y}_{(\bm{x})} \quad \textrm{with components}
  \quad \bm{y}_j := \delta_{g,j}\
\end{equation*}
Applying an encoding scheme to the batch of labels produce $\bm{Y}_{(\bm{X})}$.

\paragraph{Outputs} What model return. Let $\bm{z}_{(\bm{x})} \in
\mathbb{R}^{|K|}$ be the outputs of the model given the input $\bm{x}$. To
output prediction as probability vector, usually denoted by $\hat{\bm{y}}$,
$\bm{z}$ must be normalized.
\begin{equation*}
  P_K : \, \mathbb{R}^{|K|} \rightarrow [0, 1]^{|K|} \, : \,
  \bm{z}_{(\bm{x})} \mapsto \hat{\bm{y}}_{(\bm{x})} = P_K(\bm{z}_{(\bm{x})})
  \quad \textrm{with} \quad
  P_K(\bm{z})_j := \frac{e^{\bm{z}_j}}{\sum_{k \in K} e^{\bm{z}_k}}
\end{equation*}
$P_K(.)$ is an alternative notation for the usual \emph{softmax} function. Be
more explicit about the underlying classes $K$ give us more flexibility for
constructing custom loss function.

\paragraph{Losses} In the following losses are define as a pointwise operation
that take $\bm{x}$ and its corresponding label $\bm{y}$ as input and return a
scalar. Pointwise function apply to a batch can be combined with a reduction
operator $\bigoplus$ (e.g. sum or mean).

\begin{equation*}
  \loss : (\bm{x}, \bm{y}) \longmapsto \loss(\bm{x}, \bm{y}) \qquad
  \Loss : (\bm{X}, \bm{Y}) \longmapsto \Loss(\bm{X}, \bm{Y}) :=
  \bigoplus_{(\bm{x}, \bm{y}) \in (\bm{X}, \bm{Y})} \loss(\bm{x}, \bm{y})
\end{equation*}


\section{Cross Entropy Loss (XE)}
Standard Cross--Entropy only uses the output vector component corresponding to
the ground--truth label (i.e. the $g$ component of the output vector). Training
process force $\hat{\bm{y}}_g$ to increase and, due to normalization, other
components shrinks.
\begin{equation}
  \loss_{\textrm{XE}} := - \bm{y}^{\transpose} \log{\hat{\bm{y}}}
  = - \log{\hat{\bm{y}}_g}
  \quad \longrightarrow \quad
  \Loss_{\textrm{XE}} := \frac{1}{|\bm{X}|} \sum \loss_{\textrm{XE}}
\end{equation}

\section{Complement Entropy Loss (CE)}
\cite{1903.01182v2} introduce \emph{Complement Entropy Loss}. CE take as input the
output vector $\bm{z}$, remove its $g$ component and renormalise it  obtaining
$P_{K\setminus\{g\}}(\bm{z})$. CE is minus the Shannon Entropy $H(.)$ of this
new vector.
\begin{equation}
  \loss_{\textrm{CE}} := - H(P_{K\setminus\{g\}}(\bm{z}))
  \qquad \longrightarrow \qquad
  \Loss_{\textrm{CE}} := \frac{1}{|\bm{X}|} \sum \loss_{\textrm{CE}}
\end{equation}
Maximise the entropy of a distribution force it towards a flatter one (in these
setting the distribution with maximum entropy is the uniform distribution). CE
is paired with XE using a custom training loop:

\begin{algorithm}
\caption{Custom Training Loop for CE}
\label{alg:CE}
\begin{algorithmic}[1]
  \For{$step$ in $steps$}
    \State Update parameters of the model using $\Loss_{\textrm{XE}}$
    \State Update parameters of the model using $\Loss_{\textrm{CE}}$
  \EndFor
\end{algorithmic}
\end{algorithm}

They also proposed an empirical modification to balance the contribution that
came from CE~\footnote{In the paper they propose a rescaling factor of $|K| -
1$ but in the source code they use $|K|$. The same inconsistency appers for
$\Loss_{\textrm{GCE}}'.$}.
\begin{equation*}
  \loss_{\textrm{CE}}' := \frac{\loss_{\textrm{CE}}}{|K|-1}
  \qquad \longrightarrow \qquad
  \Loss_{\textrm{CE}}' := \frac{1}{|\bm{X}|} \sum \loss_{\textrm{CE}}'
\end{equation*}

\section{Guided Complement Entropy Loss (GCE)}
\cite{1903.09799v3} improved upon CE by adding an additional term in the
$\loss_{\text{CE}}$ that leverage the variation in model confidence during the
training phase, i.e.
\begin{equation}
  \loss_{\textrm{GCE}} := 
  {\left[ \hat{\bm{y}}_g \right]}^{\alpha} \, \loss_{\textrm{CE}}
  \qquad \longrightarrow \qquad
  \Loss_{\textrm{GCE}} := \frac{1}{|\bm{X}|} \sum \loss_{\textrm{GCE}}
\end{equation}
The new factor ${\left[ \hat{\bm{y}}_g \right]}^{\alpha}$ is called the
\emph{guiding factor}. At the beginning of training it is small (the model
outputs low probability for the g-component of $\hat{\bm{y}}$) and then it
increase with training (the model get better and the g-component of
$\hat{\bm{y}}$ will be higher). $\alpha$ is a fixed hyperparameter ($\alpha =
0.2$ works reasonably well). \\

GCE is the only loss used in a standard training loop, i.e.
\begin{algorithm}
\caption{Standard Training Loop for GCE}
\label{alg:GCE}
\begin{algorithmic}[1]
  \For{$step$ in $steps$}
    \State Update parameters of the model using $\Loss_{\textrm{GCE}}$
  \EndFor
\end{algorithmic}
\end{algorithm}

Similar to \cite{1903.01182v2}, they modify $\Loss_{\textrm{GCE}}$ to account
for the number of classes
\begin{equation*}
  \loss_{\textrm{GCE}}' := \frac{\loss_{\textrm{GCE}}}{\log(|K|-1)}
  \qquad \longrightarrow \qquad
  \Loss_{\textrm{GCE}}' := \frac{1}{|\bm{X}|} \sum \loss_{\textrm{GCE}}'
\end{equation*}

\section{Hierarchical Complement Entropy (HCE)}
\cite{1911.07257v1} try to exploit hierarchical labels in CIFAR-100. Let $G$
be a set that contains the siblings classes that belong to the same parental
class of the ground--truth class, that is $g \in G$ and $G \subseteq K$. The
\emph{Hierarchical Complement Entropy} is
\begin{equation}
  \loss_{\textrm{HCE}} :=
  - H(P_{G\setminus\{g\}}(\bm{z}))
  - H(P_{K\setminus\{G\}}(\bm{z}))
  \quad \longrightarrow \quad
  \Loss_{\textrm{HCE}} := \frac{1}{|\bm{X}|} \sum \loss_{\textrm{HCE}}
\end{equation}

The sum of HCE and XE is employed is a standard training loop:
\begin{algorithm}
\caption{Standard Training Loop for XE + HCE}
\label{alg:GCE}
\begin{algorithmic}[1]
  \For{$step$ in $steps$}
    \State Update parameters of the model using 
    $\Loss_{\textrm{XE}} + \Loss_{\textrm{HCE}}$
  \EndFor
\end{algorithmic}
\end{algorithm}

\section{Results}
\begin{table}[h!]
  \centering
  \begin{tabular}{|l|r|r|}
    \hline
    \textbf{Model} & \textbf{Val Acc} & \textbf{Val Adv Acc} \\ \hline\hline
    Che+19a\_ResNet110\_CIFAR10\_XE & 0.8698 & 0.5571 \\ \hline
    Che+19a\_ResNet110\_CIFAR10\_XE,CE & \textbf{0.8836} & \textbf{0.5923} \\ \hline\hline
    Che+19b\_ResNet56\_CIFAR10\_XE & 0.8983 & 0.5333 \\ \hline
    Che+19b\_ResNet56\_CIFAR10\_GCE & \textbf{0.9005} & \textbf{0.6213} \\ \hline
  \end{tabular}
  \caption{CIFAR-10 experiments. Accuracy are calculate on validation dataset.
  FGSM based on XE is used to produce adversarial inputs.}
  \label{tab:CIFAR-10}
\end{table}
\begin{table}[h!]
  \centering
  \begin{tabular}{|l|r|r|}
    \hline
    \textbf{Model} & \textbf{Val Acc} & \textbf{Val Adv Acc} \\ \hline\hline
    Che+19a\_ResNet110\_CIFAR100\_XE & 0.6497 & 0.3128 \\ \hline
    Che+19a\_ResNet110\_CIFAR100\_XE,CE & \textbf{0.6516} & \textbf{0.3214} \\ \hline\hline
    Che+19b\_ResNet56\_CIFAR100\_XE & 0.6001 & 0.2744 \\ \hline
    Che+19b\_ResNet56\_CIFAR100\_GCE & 0.6246 & \textbf{0.3674} \\ \hline
    Che+19c\_ResNet56\_CIFAR100\_XE,CE & 0.6164 & 0.2674 \\ \hline
    Che+19c\_ResNet56\_CIFAR100\_XE,HCE & 0.6164 & 0.3013 \\ \hline
    Ours\_ResNet56\_CIFAR100\_HGCE & \textbf{0.6356} & 0.3603 \\ \hline
  \end{tabular}
  \caption{CIFAR-100 experiments. Accuracy are calculate on validation dataset.
  FGSM based on XE is used to produce adversarial inputs.}
  \label{tab:CIFAR-100}
\end{table}

\newpage
\printbibliography

\end{document}
