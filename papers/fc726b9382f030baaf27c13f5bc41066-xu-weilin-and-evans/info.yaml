abstract: 'Although deep neural networks (DNNs) have achieved great success in many
  tasks, they can often be fooled by \emph{adversarial examples} that are generated
  by adding small but purposeful distortions to natural examples. Previous studies
  to defend against adversarial examples mostly focused on refining the DNN models,
  but have either shown limited success or required expensive computation. We propose
  a new strategy, \emph{feature squeezing}, that can be used to harden DNN models
  by detecting adversarial examples. Feature squeezing reduces the search space available
  to an adversary by coalescing samples that correspond to many different feature
  vectors in the original space into a single sample. By comparing a DNN model''s
  prediction on the original input with that on squeezed inputs, feature squeezing
  detects adversarial examples with high accuracy and few false positives. This paper
  explores two feature squeezing methods: reducing the color bit depth of each pixel
  and spatial smoothing. These simple strategies are inexpensive and complementary
  to other defenses, and can be combined in a joint detection framework to achieve
  high detection rates against state-of-the-art attacks.'
archiveprefix: arXiv
author: Xu, Weilin and Evans, David and Qi, Yanjun
author_list:
- family: Xu
  given: Weilin
- family: Evans
  given: David
- family: Qi
  given: Yanjun
doi: 10.14722/ndss.2018.23198
eprint: 1704.01155v2
file: 1704.01155v2.pdf
files:
- tmpdm0ba-dq.pdf
month: Apr
papis_id: 4f8f9aa691817c7dc1ae0b5a5d7ac0a9
primaryclass: cs.CV
ref: FeatureSqueeziXuWe2017
time-added: 2023-11-21-16:55:47
title: 'Feature Squeezing: Detecting Adversarial Examples in Deep Neural   Networks'
type: article
url: http://arxiv.org/abs/1704.01155v2
year: '2017'
