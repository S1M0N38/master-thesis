abstract: While the Transformer architecture has become the de-facto standard for
  natural language processing tasks, its applications to computer vision remain limited.
  In vision, attention is either applied in conjunction with convolutional networks,
  or used to replace certain components of convolutional networks while keeping their
  overall structure in place. We show that this reliance on CNNs is not necessary
  and a pure transformer applied directly to sequences of image patches can perform
  very well on image classification tasks. When pre-trained on large amounts of data
  and transferred to multiple mid-sized or small image recognition benchmarks (ImageNet,
  CIFAR-100, VTAB, etc.), Vision Transformer (ViT) attains excellent results compared
  to state-of-the-art convolutional networks while requiring substantially fewer computational
  resources to train.
archiveprefix: arXiv
author: Dosovitskiy, Alexey and Beyer, Lucas and Kolesnikov, Alexander and Weissenborn,
  Dirk and Zhai, Xiaohua and Unterthiner, Thomas and Dehghani, Mostafa and Minderer,
  Matthias and Heigold, Georg and Gelly, Sylvain and Uszkoreit, Jakob and Houlsby,
  Neil
author_list:
- family: Dosovitskiy
  given: Alexey
- family: Beyer
  given: Lucas
- family: Kolesnikov
  given: Alexander
- family: Weissenborn
  given: Dirk
- family: Zhai
  given: Xiaohua
- family: Unterthiner
  given: Thomas
- family: Dehghani
  given: Mostafa
- family: Minderer
  given: Matthias
- family: Heigold
  given: Georg
- family: Gelly
  given: Sylvain
- family: Uszkoreit
  given: Jakob
- family: Houlsby
  given: Neil
eprint: 2010.11929v2
file: 2010.11929v2.pdf
files:
- tmpjinzrm3z.pdf
month: 10
papis_id: 236af8ee57d2d4687b897e6e38431cc9
primaryclass: cs.CV
ref: AnImageIsWorDosovi2020
time-added: 2023-08-08-10:56:21
title: 'An Image is Worth 16x16 Words: Transformers for Image Recognition at   Scale'
type: article
url: http://arxiv.org/abs/2010.11929v2
year: '2020'
