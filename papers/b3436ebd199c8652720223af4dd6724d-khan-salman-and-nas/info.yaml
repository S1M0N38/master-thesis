abstract: Astounding results from Transformer models on natural language tasks have
  intrigued the vision community to study their application to computer vision problems.
  Among their salient benefits, Transformers enable modeling long dependencies between
  input sequence elements and support parallel processing of sequence as compared
  to recurrent networks e.g., Long short-term memory (LSTM). Different from convolutional
  networks, Transformers require minimal inductive biases for their design and are
  naturally suited as set-functions. Furthermore, the straightforward design of Transformers
  allows processing multiple modalities (e.g., images, videos, text and speech) using
  similar processing blocks and demonstrates excellent scalability to very large capacity
  networks and huge datasets. These strengths have led to exciting progress on a number
  of vision tasks using Transformer networks. This survey aims to provide a comprehensive
  overview of the Transformer models in the computer vision discipline. We start with
  an introduction to fundamental concepts behind the success of Transformers i.e.,
  self-attention, large-scale pre-training, and bidirectional encoding. We then cover
  extensive applications of transformers in vision including popular recognition tasks
  (e.g., image classification, object detection, action recognition, and segmentation),
  generative modeling, multi-modal tasks (e.g., visual-question answering, visual
  reasoning, and visual grounding), video processing (e.g., activity recognition,
  video forecasting), low-level vision (e.g., image super-resolution, image enhancement,
  and colorization) and 3D analysis (e.g., point cloud classification and segmentation).
  We compare the respective advantages and limitations of popular techniques both
  in terms of architectural design and their experimental value. Finally, we provide
  an analysis on open research directions and possible future works.
archiveprefix: arXiv
author: Khan, Salman and Naseer, Muzammal and Hayat, Munawar and Zamir, Syed Waqas
  and Khan, Fahad Shahbaz and Shah, Mubarak
author_list:
- family: Khan
  given: Salman
- family: Naseer
  given: Muzammal
- family: Hayat
  given: Munawar
- family: Zamir
  given: Syed Waqas
- family: Khan
  given: Fahad Shahbaz
- family: Shah
  given: Mubarak
doi: 10.1145/3505244
eprint: 2101.01169v5
file: 2101.01169v5.pdf
files:
- tmpces62tu3.pdf
month: 1
papis_id: f4ee6f86019fbeb361b49d926508aeac
primaryclass: cs.CV
ref: TransformersInKhan2021
time-added: 2023-08-08-11:01:42
title: 'Transformers in Vision: A Survey'
type: article
url: http://arxiv.org/abs/2101.01169v5
year: '2021'
