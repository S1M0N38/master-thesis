abstract: Neural networks have been proven to be vulnerable to a variety of adversarial
  attacks. From a safety perspective, highly sparse adversarial attacks are particularly
  dangerous. On the other hand the pixelwise perturbations of sparse attacks are typically
  large and thus can be potentially detected. We propose a new black-box technique
  to craft adversarial examples aiming at minimizing $l_0$-distance to the original
  image. Extensive experiments show that our attack is better or competitive to the
  state of the art. Moreover, we can integrate additional bounds on the componentwise
  perturbation. Allowing pixels to change only in region of high variation and avoiding
  changes along axis-aligned edges makes our adversarial examples almost non-perceivable.
  Moreover, we adapt the Projected Gradient Descent attack to the $l_0$-norm integrating
  componentwise constraints. This allows us to do adversarial training to enhance
  the robustness of classifiers against sparse and imperceivable adversarial manipulations.
archiveprefix: arXiv
author: Croce, Francesco and Hein, Matthias
author_list:
- family: Croce
  given: Francesco
- family: Hein
  given: Matthias
eprint: 1909.05040v1
file: 1909.05040v1.pdf
files:
- tmpkd4kn4or.pdf
month: Sep
primaryclass: cs.LG
ref: 1909.05040v1
tags: ICCV2019 attack
time-added: 2023-03-07-11:40:09
title: Sparse and Imperceivable Adversarial Attacks
type: article
url: http://arxiv.org/abs/1909.05040v1
year: '2019'
notes: 'Propose a new black-box technique to craft adversarial examples aiming
  at minimizing l0-distance to the original image. Allowing pixels to change
  only in region of high variation and avoiding changes along axis-aligned
  edges makes our adversarial examples almost non-perceivable.'
code: 'https://github.com/fra31/sparse-imperceivable-attacks'
