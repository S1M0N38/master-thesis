abstract: The recently introduced continuous Skip-gram model is an efficient method
  for learning high-quality distributed vector representations that capture a large
  number of precise syntactic and semantic word relationships. In this paper we present
  several extensions that improve both the quality of the vectors and the training
  speed. By subsampling of the frequent words we obtain significant speedup and also
  learn more regular word representations. We also describe a simple alternative to
  the hierarchical softmax called negative sampling. An inherent limitation of word
  representations is their indifference to word order and their inability to represent
  idiomatic phrases. For example, the meanings of "Canada" and "Air" cannot be easily
  combined to obtain "Air Canada". Motivated by this example, we present a simple
  method for finding phrases in text, and show that learning good vector representations
  for millions of phrases is possible.
archiveprefix: arXiv
author: Mikolov, Tomas and Sutskever, Ilya and Chen, Kai and Corrado, Greg and Dean,
  Jeffrey
author_list:
- family: Mikolov
  given: Tomas
- family: Sutskever
  given: Ilya
- family: Chen
  given: Kai
- family: Corrado
  given: Greg
- family: Dean
  given: Jeffrey
eprint: 1310.4546v1
file: 1310.4546v1.pdf
files:
- tmpvq5fqoz9.pdf
month: 10
papis_id: 3bb165358c1066620ca90af8c2a9dabf
primaryclass: cs.CL
ref: DistributedRepMikolo2013
time-added: 2023-08-10-10:12:48
title: Distributed Representations of Words and Phrases and their   Compositionality
type: article
url: http://arxiv.org/abs/1310.4546v1
year: '2013'
