abstract: 'Although end-to-end neural text-to-speech (TTS) methods (such as Tacotron2)
  are proposed and achieve state-of-the-art performance, they still suffer from two
  problems: 1) low efficiency during training and inference; 2) hard to model long
  dependency using current recurrent neural networks (RNNs). Inspired by the success
  of Transformer network in neural machine translation (NMT), in this paper, we introduce
  and adapt the multi-head attention mechanism to replace the RNN structures and also
  the original attention mechanism in Tacotron2. With the help of multi-head self-attention,
  the hidden states in the encoder and decoder are constructed in parallel, which
  improves the training efficiency. Meanwhile, any two inputs at different times are
  connected directly by self-attention mechanism, which solves the long range dependency
  problem effectively. Using phoneme sequences as input, our Transformer TTS network
  generates mel spectrograms, followed by a WaveNet vocoder to output the final audio
  results. Experiments are conducted to test the efficiency and performance of our
  new network. For the efficiency, our Transformer TTS network can speed up the training
  about 4.25 times faster compared with Tacotron2. For the performance, rigorous human
  tests show that our proposed model achieves state-of-the-art performance (outperforms
  Tacotron2 with a gap of 0.048) and is very close to human quality (4.39 vs 4.44
  in MOS).'
archiveprefix: arXiv
author: Li, Naihan and Liu, Shujie and Liu, Yanqing and Zhao, Sheng and Liu, Ming
  and Zhou, Ming
author_list:
- family: Li
  given: Naihan
- family: Liu
  given: Shujie
- family: Liu
  given: Yanqing
- family: Zhao
  given: Sheng
- family: Liu
  given: Ming
- family: Zhou
  given: Ming
eprint: 1809.08895v3
file: 1809.08895v3.pdf
files:
- tmpfu8mrgcy.pdf
month: 9
papis_id: b8871beef33694e991646c3650748bd8
primaryclass: cs.CL
ref: NeuralSpeechSLiNa2018
time-added: 2023-08-08-11:36:06
title: Neural Speech Synthesis with Transformer Network
type: article
url: http://arxiv.org/abs/1809.08895v3
year: '2018'
