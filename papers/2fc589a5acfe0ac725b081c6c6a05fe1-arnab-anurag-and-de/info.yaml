abstract: We present pure-transformer based models for video classification, drawing
  upon the recent success of such models in image classification. Our model extracts
  spatio-temporal tokens from the input video, which are then encoded by a series
  of transformer layers. In order to handle the long sequences of tokens encountered
  in video, we propose several, efficient variants of our model which factorise the
  spatial- and temporal-dimensions of the input. Although transformer-based models
  are known to only be effective when large training datasets are available, we show
  how we can effectively regularise the model during training and leverage pretrained
  image models to be able to train on comparatively small datasets. We conduct thorough
  ablation studies, and achieve state-of-the-art results on multiple video classification
  benchmarks including Kinetics 400 and 600, Epic Kitchens, Something-Something v2
  and Moments in Time, outperforming prior methods based on deep 3D convolutional
  networks. To facilitate further research, we release code at https://github.com/google-research/scenic/tree/main/scenic/projects/vivit
archiveprefix: arXiv
author: Arnab, Anurag and Dehghani, Mostafa and Heigold, Georg and Sun, Chen and Lučić,
  Mario and Schmid, Cordelia
author_list:
- family: Arnab
  given: Anurag
- family: Dehghani
  given: Mostafa
- family: Heigold
  given: Georg
- family: Sun
  given: Chen
- family: Lučić
  given: Mario
- family: Schmid
  given: Cordelia
eprint: 2103.15691v2
file: 2103.15691v2.pdf
files:
- tmp3cif3wb3.pdf
month: Mar
papis_id: eaeae68bcf2ebfbd040d95a8d9e32861
primaryclass: cs.CV
ref: VivitAVideoArnab2021
time-added: 2023-08-08-11:26:06
title: 'ViViT: A Video Vision Transformer'
type: article
url: http://arxiv.org/abs/2103.15691v2
year: 2021
