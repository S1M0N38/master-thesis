abstract: 'Human can easily recognize visual objects with lost information: even losing
  most details with only contour reserved, e.g. cartoon. However, in terms of visual
  perception of Deep Neural Networks (DNNs), the ability for recognizing abstract
  objects (visual objects with lost information) is still a challenge. In this work,
  we investigate this issue from an adversarial viewpoint: will the performance of
  DNNs decrease even for the images only losing a little information? Towards this
  end, we propose a novel adversarial attack, named \textit{AdvDrop}, which crafts
  adversarial examples by dropping existing information of images. Previously, most
  adversarial attacks add extra disturbing information on clean images explicitly.
  Opposite to previous works, our proposed work explores the adversarial robustness
  of DNN models in a novel perspective by dropping imperceptible details to craft
  adversarial examples. We demonstrate the effectiveness of \textit{AdvDrop} by extensive
  experiments, and show that this new type of adversarial examples is more difficult
  to be defended by current defense systems.'
archiveprefix: arXiv
author: Duan, Ranjie and Chen, Yuefeng and Niu, Dantong and Yang, Yun and Qin, A.
  K. and He, Yuan
author_list:
- family: Duan
  given: Ranjie
- family: Chen
  given: Yuefeng
- family: Niu
  given: Dantong
- family: Yang
  given: Yun
- family: Qin
  given: A. K.
- family: He
  given: Yuan
eprint: 2108.09034v1
file: 2108.09034v1.pdf
files:
- tmptlnevq2g.pdf
month: Aug
primaryclass: cs.CV
ref: 2108.09034v1
tags: ICCV2021 attack
time-added: 2023-03-07-09:16:50
title: 'AdvDrop: Adversarial Attack to DNNs by Dropping Information'
type: article
url: http://arxiv.org/abs/2108.09034v1
year: '2021'
notes: 'Human can easily recognize visual objects with lost information: even losing
  most details with only contour reserved, e.g. cartoon. For DDNs this is still
  a challenge. Opposite to previous works, this work explores the adversarial
  robustness of DNN models in a novel perspective by dropping imperceptible
  details to craft adversarial examples. This new type of adversarial examples
  is more difficult to be defended by current defense systems.'
code: 'https://github.com/RjDuan/AdvDrop'
