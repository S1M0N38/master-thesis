abstract: 'Adversarial examples are malicious inputs designed to fool machine learning
  models. They often transfer from one model to another, allowing attackers to mount
  black box attacks without knowledge of the target model''s parameters. Adversarial
  training is the process of explicitly training a model on adversarial examples,
  in order to make it more robust to attack or to reduce its test error on clean inputs.
  So far, adversarial training has primarily been applied to small problems. In this
  research, we apply adversarial training to ImageNet. Our contributions include:
  (1) recommendations for how to succesfully scale adversarial training to large models
  and datasets, (2) the observation that adversarial training confers robustness to
  single-step attack methods, (3) the finding that multi-step attack methods are somewhat
  less transferable than single-step attack methods, so single-step attacks are the
  best for mounting black-box attacks, and (4) resolution of a "label leaking" effect
  that causes adversarially trained models to perform better on adversarial examples
  than on clean examples, because the adversarial example construction process uses
  the true label and the model can learn to exploit regularities in the construction
  process.'
archiveprefix: arXiv
author: Kurakin, Alexey and Goodfellow, Ian and Bengio, Samy
author_list:
- family: Kurakin
  given: Alexey
- family: Goodfellow
  given: Ian
- family: Bengio
  given: Samy
eprint: 1611.01236v2
file: 1611.01236v2.pdf
files:
- tmp9w4o03dl.pdf
month: Nov
papis_id: f3a0eb574876bd17b8bb5f4129a603cc
primaryclass: cs.CV
ref: AdversarialMacKuraki2016
time-added: 2023-11-21-17:11:09
title: Adversarial Machine Learning at Scale
type: article
url: http://arxiv.org/abs/1611.01236v2
year: '2016'
