abstract: Effective regularization techniques are highly desired in deep learning
  for alleviating overfitting and improving generalization. This work proposes a new
  regularization scheme, based on the understanding that the flat local minima of
  the empirical risk cause the model to generalize better. This scheme is referred
  to as adversarial model perturbation (AMP), where instead of directly minimizing
  the empirical risk, an alternative "AMP loss" is minimized via SGD. Specifically,
  the AMP loss is obtained from the empirical risk by applying the "worst" norm-bounded
  perturbation on each point in the parameter space. Comparing with most existing
  regularization schemes, AMP has strong theoretical justifications, in that minimizing
  the AMP loss can be shown theoretically to favour flat local minima of the empirical
  risk. Extensive experiments on various modern deep architectures establish AMP as
  a new state of the art among regularization schemes. Our code is available at https://github.com/hiyouga/AMP-Regularizer.
archiveprefix: arXiv
author: Zheng, Yaowei and Zhang, Richong and Mao, Yongyi
author_list:
- family: Zheng
  given: Yaowei
- family: Zhang
  given: Richong
- family: Mao
  given: Yongyi
eprint: 2010.04925v4
file: 2010.04925v4.pdf
files:
- tmp-6uaxufd.pdf
month: Oct
primaryclass: cs.LG
ref: 2010.04925v4
tags: CVPR2021 selected
time-added: 2023-03-07-15:34:53
title: Regularizing Neural Networks via Adversarial Model Perturbation
type: article
url: http://arxiv.org/abs/2010.04925v4
year: '2020'
notes: 'Proposes a new regularization scheme, based on the understanding that
  the flat local minima of the empirical risk cause the model to generalize
  better. This scheme is referred to as adversarial model perturbation (AMP),
  where instead of directly minimizing the empirical risk, an alternative "AMP
  loss" is minimized. AMP has strong theoretical justifications.'
code: 'https://github.com/hiyouga/AMP-Regularizer'
