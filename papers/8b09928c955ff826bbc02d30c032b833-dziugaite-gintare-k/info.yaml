abstract: Neural network image classifiers are known to be vulnerable to adversarial
  images, i.e., natural images which have been modified by an adversarial perturbation
  specifically designed to be imperceptible to humans yet fool the classifier. Not
  only can adversarial images be generated easily, but these images will often be
  adversarial for networks trained on disjoint subsets of data or with different architectures.
  Adversarial images represent a potential security risk as well as a serious machine
  learning challenge---it is clear that vulnerable neural networks perceive images
  very differently from humans. Noting that virtually every image classification data
  set is composed of JPG images, we evaluate the effect of JPG compression on the
  classification of adversarial images. For Fast-Gradient-Sign perturbations of small
  magnitude, we found that JPG compression often reverses the drop in classification
  accuracy to a large extent, but not always. As the magnitude of the perturbations
  increases, JPG recompression alone is insufficient to reverse the effect.
archiveprefix: arXiv
author: Dziugaite, Gintare Karolina and Ghahramani, Zoubin and Roy, Daniel M.
author_list:
- family: Dziugaite
  given: Gintare Karolina
- family: Ghahramani
  given: Zoubin
- family: Roy
  given: Daniel M.
eprint: 1608.00853v1
file: 1608.00853v1.pdf
files:
- tmpd-vdertm.pdf
month: Aug
papis_id: 97e15e0748cd73521e8944c03a1c3601
primaryclass: cs.CV
ref: AStudyOfTheDziuga2016
time-added: 2023-11-21-16:59:01
title: A study of the effect of JPG compression on adversarial images
type: article
url: http://arxiv.org/abs/1608.00853v1
year: '2016'
