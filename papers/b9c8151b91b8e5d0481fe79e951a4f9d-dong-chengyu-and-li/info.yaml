abstract: We show that label noise exists in adversarial training. Such label noise
  is due to the mismatch between the true label distribution of adversarial examples
  and the label inherited from clean examples - the true label distribution is distorted
  by the adversarial perturbation, but is neglected by the common practice that inherits
  labels from clean examples. Recognizing label noise sheds insights on the prevalence
  of robust overfitting in adversarial training, and explains its intriguing dependence
  on perturbation radius and data quality. Also, our label noise perspective aligns
  well with our observations of the epoch-wise double descent in adversarial training.
  Guided by our analyses, we proposed a method to automatically calibrate the label
  to address the label noise and robust overfitting. Our method achieves consistent
  performance improvements across various models and datasets without introducing
  new hyper-parameters or additional tuning.
archiveprefix: arXiv
author: Dong, Chengyu and Liu, Liyuan and Shang, Jingbo
author_list:
- family: Dong
  given: Chengyu
- family: Liu
  given: Liyuan
- family: Shang
  given: Jingbo
eprint: 2110.03135v2
file: 2110.03135v2.pdf
files:
- tmpcw3t6eag.pdf
month: Oct
primaryclass: cs.LG
ref: 2110.03135v2
tags: NeurIPS2022
time-added: 2023-03-08-12:01:55
title: 'Label Noise in Adversarial Training: A Novel Perspective to Study Robust   Overfitting'
type: article
url: http://arxiv.org/abs/2110.03135v2
year: '2021'
