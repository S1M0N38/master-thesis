abstract: 'Deep learning algorithms have been shown to perform extremely well on many
  classical machine learning problems. However, recent studies have shown that deep
  learning, like other machine learning techniques, is vulnerable to adversarial samples:
  inputs crafted to force a deep neural network (DNN) to provide adversary-selected
  outputs. Such attacks can seriously undermine the security of the system supported
  by the DNN, sometimes with devastating consequences. For example, autonomous vehicles
  can be crashed, illicit or illegal content can bypass content filters, or biometric
  authentication systems can be manipulated to allow improper access. In this work,
  we introduce a defensive mechanism called defensive distillation to reduce the effectiveness
  of adversarial samples on DNNs. We analytically investigate the generalizability
  and robustness properties granted by the use of defensive distillation when training
  DNNs. We also empirically study the effectiveness of our defense mechanisms on two
  DNNs placed in adversarial settings. The study shows that defensive distillation
  can reduce effectiveness of sample creation from 95% to less than 0.5% on a studied
  DNN. Such dramatic gains can be explained by the fact that distillation leads gradients
  used in adversarial sample creation to be reduced by a factor of 10^30. We also
  find that distillation increases the average minimum number of features that need
  to be modified to create adversarial samples by about 800% on one of the DNNs we
  tested.'
archiveprefix: arXiv
author: Papernot, Nicolas and McDaniel, Patrick and Wu, Xi and Jha, Somesh and Swami,
  Ananthram
author_list:
- family: Papernot
  given: Nicolas
- family: McDaniel
  given: Patrick
- family: Wu
  given: Xi
- family: Jha
  given: Somesh
- family: Swami
  given: Ananthram
eprint: 1511.04508v2
file: 1511.04508v2.pdf
files:
- tmp2cipewlj.pdf
month: Nov
papis_id: f8fb9bc9b67e493ccf2bcbd6588ff188
primaryclass: cs.CR
ref: DistillationAsPapern2015
time-added: 2023-11-21-17:14:54
title: Distillation as a Defense to Adversarial Perturbations against Deep   Neural
  Networks
type: article
url: http://arxiv.org/abs/1511.04508v2
year: '2015'
