abstract: Owing to security implications of adversarial vulnerability, adversarial
  robustness of deep metric learning models has to be improved. In order to avoid
  model collapse due to excessively hard examples, the existing defenses dismiss the
  min-max adversarial training, but instead learn from a weak adversary inefficiently.
  Conversely, we propose Hardness Manipulation to efficiently perturb the training
  triplet till a specified level of hardness for adversarial training, according to
  a harder benign triplet or a pseudo-hardness function. It is flexible since regular
  training and min-max adversarial training are its boundary cases. Besides, Gradual
  Adversary, a family of pseudo-hardness functions is proposed to gradually increase
  the specified hardness level during training for a better balance between performance
  and robustness. Additionally, an Intra-Class Structure loss term among benign and
  adversarial examples further improves model robustness and efficiency. Comprehensive
  experimental results suggest that the proposed method, although simple in its form,
  overwhelmingly outperforms the state-of-the-art defenses in terms of robustness,
  training efficiency, as well as performance on benign examples.
archiveprefix: arXiv
author: Zhou, Mo and Patel, Vishal M.
author_list:
- family: Zhou
  given: Mo
- family: Patel
  given: Vishal M.
eprint: 2203.01439v1
file: 2203.01439v1.pdf
files:
- tmpo991ztug.pdf
month: Mar
primaryclass: cs.LG
ref: 2203.01439v1
tags: CVPR2022
time-added: 2023-03-07-14:45:06
title: Enhancing Adversarial Robustness for Deep Metric Learning
type: article
url: http://arxiv.org/abs/2203.01439v1
year: '2022'
notes: 'Propose Hardness Manipulation to efficiently perturb the training
  triplet till a specified level of hardness for adversarial training. An
  Intra-Class Structure loss term among benign and adversarial examples further
  improves model robustness and efficiency. Overwhelmingly outperforms the
  state-of-the-art defenses in terms of robustness, training efficiency, as
  well as performance on benign examples.'
