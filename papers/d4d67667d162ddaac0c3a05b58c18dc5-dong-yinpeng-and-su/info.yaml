abstract: Interpretability of deep neural networks (DNNs) is essential since it enables
  users to understand the overall strengths and weaknesses of the models, conveys
  an understanding of how the models will behave in the future, and how to diagnose
  and correct potential problems. However, it is challenging to reason about what
  a DNN actually does due to its opaque or black-box nature. To address this issue,
  we propose a novel technique to improve the interpretability of DNNs by leveraging
  the rich semantic information embedded in human descriptions. By concentrating on
  the video captioning task, we first extract a set of semantically meaningful topics
  from the human descriptions that cover a wide range of visual concepts, and integrate
  them into the model with an interpretive loss. We then propose a prediction difference
  maximization algorithm to interpret the learned features of each neuron. Experimental
  results demonstrate its effectiveness in video captioning using the interpretable
  features, which can also be transferred to video action recognition. By clearly
  understanding the learned features, users can easily revise false predictions via
  a human-in-the-loop procedure.
archiveprefix: arXiv
author: Dong, Yinpeng and Su, Hang and Zhu, Jun and Zhang, Bo
author_list:
- family: Dong
  given: Yinpeng
- family: Su
  given: Hang
- family: Zhu
  given: Jun
- family: Zhang
  given: Bo
eprint: 1703.04096v2
file: 1703.04096v2.pdf
files:
- tmp7wk61rz3.pdf
month: 3
papis_id: f6a21c5c9adde67a809d31cecb2e20d8
primaryclass: cs.CV
ref: ImprovingInterDong2017
time-added: 2023-08-08-11:58:54
title: Improving Interpretability of Deep Neural Networks with Semantic   Information
type: article
url: http://arxiv.org/abs/1703.04096v2
year: '2017'
