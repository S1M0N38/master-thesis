abstract: We find that images contain intrinsic structure that enables the reversal
  of many adversarial attacks. Attack vectors cause not only image classifiers to
  fail, but also collaterally disrupt incidental structure in the image. We demonstrate
  that modifying the attacked image to restore the natural structure will reverse
  many types of attacks, providing a defense. Experiments demonstrate significantly
  improved robustness for several state-of-the-art models across the CIFAR-10, CIFAR-100,
  SVHN, and ImageNet datasets. Our results show that our defense is still effective
  even if the attacker is aware of the defense mechanism. Since our defense is deployed
  during inference instead of training, it is compatible with pre-trained networks
  as well as most other defenses. Our results suggest deep networks are vulnerable
  to adversarial examples partly because their representations do not enforce the
  natural structure of images.
archiveprefix: arXiv
author: Mao, Chengzhi and Chiquier, Mia and Wang, Hao and Yang, Junfeng and Vondrick,
  Carl
author_list:
- family: Mao
  given: Chengzhi
- family: Chiquier
  given: Mia
- family: Wang
  given: Hao
- family: Yang
  given: Junfeng
- family: Vondrick
  given: Carl
eprint: 2103.14222v3
file: 2103.14222v3.pdf
files:
- tmp2mhjhyf6.pdf
month: Mar
primaryclass: cs.CV
ref: 2103.14222v3
tags: ICCV2021 defence
time-added: 2023-03-06-23:42:29
title: Adversarial Attacks are Reversible with Natural Supervision
type: article
url: http://arxiv.org/abs/2103.14222v3
year: '2021'
notes: Attack vectors cause not only image classifiers to fail, but also
  collaterally disrupt incidental structure in the image. Restoring the natural
  structure of the attacked image will reverse many types of attacks. Improved
  robustness for several SOTA models across CIFAR-10 and CIFAR-100. This kind
  of defence holds even if the attacker is aware of it.
code: 'https://github.com/cvlab-columbia/SelfSupDefense'

