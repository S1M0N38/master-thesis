abstract: Deep neural networks (DNNs) are vulnerable to adversarial noise. Preprocessing
  based defenses could largely remove adversarial noise by processing inputs. However,
  they are typically affected by the error amplification effect, especially in the
  front of continuously evolving attacks. To solve this problem, in this paper, we
  propose to remove adversarial noise by implementing a self-supervised adversarial
  training mechanism in a class activation feature space. To be specific, we first
  maximize the disruptions to class activation features of natural examples to craft
  adversarial examples. Then, we train a denoising model to minimize the distances
  between the adversarial examples and the natural examples in the class activation
  feature space. Empirical evaluations demonstrate that our method could significantly
  enhance adversarial robustness in comparison to previous state-of-the-art approaches,
  especially against unseen adversarial attacks and adaptive attacks.
archiveprefix: arXiv
author: Zhou, Dawei and Wang, Nannan and Peng, Chunlei and Gao, Xinbo and Wang, Xiaoyu
  and Yu, Jun and Liu, Tongliang
author_list:
- family: Zhou
  given: Dawei
- family: Wang
  given: Nannan
- family: Peng
  given: Chunlei
- family: Gao
  given: Xinbo
- family: Wang
  given: Xiaoyu
- family: Yu
  given: Jun
- family: Liu
  given: Tongliang
eprint: 2104.09197v1
file: 2104.09197v1.pdf
files:
- tmpqf4nbkfh.pdf
month: Apr
primaryclass: cs.LG
ref: 2104.09197v1
tags: ICCV2021 adversarial-training defence
time-added: 2023-03-06-23:47:51
title: Removing Adversarial Noise in Class Activation Feature Space
type: article
url: http://arxiv.org/abs/2104.09197v1
year: '2021'
notes: 'Propose to remove adversarial noise by implementing a self-supervised
  adversarial training mechanism in a class activation feature space.
  Enhance adversarial robustness especially against unseen adversarial attacks.'
