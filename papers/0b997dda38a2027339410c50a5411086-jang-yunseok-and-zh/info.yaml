abstract: With the remarkable success of deep learning, Deep Neural Networks
  (DNNs) have been applied as dominant tools to various machine learning domains.
  Despite this success, however, it has been found that DNNs are surprisingly
  vulnerable to malicious attacks; adding a small, perceptually indistinguishable
  perturbations to the data can easily degrade classification performance.
  Adversarial training is an effective defense strategy to train a robust
  classifier. In this work, we propose to utilize the generator to learn how to
  create adversarial examples. Unlike the existing approaches that create a
  one-shot perturbation by a deterministic generator, we propose a recursive and
  stochastic generator that produces much stronger and diverse perturbations that
  comprehensively reveal the vulnerability of the target classifier. Our
  experiment results on MNIST and CIFAR-10 datasets show that the classifier
  adversarially trained with our method yields more robust performance over
  various white-box and black-box attacks.
author: Jang, Yunseok and Zhao, Tianchen and Hong, Seunghoon and Lee, Honglak
author_list:
- affiliation: []
  family: Jang
  given: Yunseok
- affiliation: []
  family: Zhao
  given: Tianchen
- affiliation: []
  family: Hong
  given: Seunghoon
- affiliation: []
  family: Lee
  given: Honglak
booktitle: 2019 IEEE/CVF International Conference on Computer Vision (ICCV)
citations:
- article-title: Logit Pairing Methods Can Fool Gradient-Based Attacks
  author: mosbach
  journal-title: NeurIPS SECML
  year: '2018'
- doi: 10.1109/CVPR.2018.00084
- article-title: Delving into Transferable Adversarial Examples and Black-box Attacks
  author: liu
  journal-title: ICLRE
  year: '2017'
- doi: 10.1109/ICCV.2017.615
- article-title: Visualizing the Loss Landscape of Neural Nets
  author: li
  journal-title: NeurIPS
  year: '2018'
- article-title: 'Generative Adversarial Trainer: Defense to Adversarial Perturbations
    with GAN'
  author: lee
  journal-title: arXiv l705 03387
  year: '2017'
- article-title: Playing Atari with Deep Reinforcement Learning
  author: mnih
  journal-title: NeurIPS
  year: '2013'
- article-title: On Detecting Adversarial Perturbations
  author: metzen
  journal-title: ICLRE
  year: '2017'
- doi: 10.1145/3133956.3134057
- article-title: Towards Deep Learning Models Resistant to Adversarial Attacks
  author: madry
  journal-title: ICLRE
  year: '2018'
- doi: 10.1145/3128572.3140449
- article-title: Adversarial Machine Learning at Scale
  author: kurakin
  journal-title: ICLRE
  year: '2017'
- article-title: Adversarial Examples in the Physical World
  author: kurakin
  journal-title: ICLR Workshop
  year: '2017'
- doi: 10.1109/5.726791
- article-title: 'Obfuscated Gradients Give a False Sense of Security: Circumventing
    Defenses to Adversarial Examples'
  author: athalye
  journal-title: ICML
  year: '2018'
- article-title: Towards Principled Methods for Training Generative Adversarial Networks
  author: arjovsky
  journal-title: ICLRE
  year: '2017'
- article-title: Countering Adversarial Images using Input Transformations
  author: guo
  journal-title: ICLRE
  year: '2018'
- doi: 10.1109/CVPR.2016.90
- article-title: 'Machine vs Machine: Defending Classifiers Against Learning-based
    Adversarial Attacks'
  author: hamm
  journal-title: arXiv 1711 04368
  year: '2017'
- article-title: Adversarial Logit Pairing
  author: kannan
  journal-title: arXiv 1803 06373
  year: '2018'
- article-title: Early Methods for Detecting Adversarial Images
  author: hendrycks
  journal-title: ICLR Workshop
  year: '2017'
- article-title: ImageNet Classification with Deep Convolutional Neural Networks
  author: krizhevsky
  journal-title: NeurIPS
  year: '2012'
- author: krizhevsky
  journal-title: Learning multiple layers of features from tiny images
  year: '2009'
- article-title: Certifying Some Distributional Robustness with Principled Adversarial
    Training
  author: sinha
  journal-title: ICLRE
  year: '2018'
- article-title: 'PixelDefend: Leveraging Generative Models to Understand and Defend
    against Adversarial Examples'
  author: song
  journal-title: ICLRE
  year: '2018'
- article-title: Diversity-Sensitive Conditional Generative Adversarial Networks
  author: yang
  journal-title: ICLRE
  year: '2019'
- doi: 10.1109/CVPR.2019.00059
- article-title: Mitigating Adversarial Effects Through Randomization
  author: xie
  journal-title: ICLRE
  year: '2018'
- doi: 10.24963/ijcai.2018/543
- article-title: 'MixTrain: Scalable Training of Formally Robust Neural Networks'
  author: wang
  journal-title: arXiv 181 1 02625
  year: '2018'
- article-title: A Direct Approach to Robust Deep Learning Using Adversarial Networks
  author: wang
  journal-title: ICLRE
  year: '2019'
- doi: 10.1109/TPAMI.2008.128
- article-title: Intriguing Properties of Neural Networks
  author: szegedy
  journal-title: ICLRE
  year: '2014'
- article-title: Stochastic Activation Pruning for Robust Adversarial Defense
  author: dhillon
  journal-title: ICLRE
  year: '2018'
- doi: 10.1109/CVPR.2018.00957
- article-title: Cascade Adversarial Machine Learning Regularized with a Unified Embedding
  author: na
  journal-title: ICLRE
  year: '2018'
- article-title: A Learned Representation For Artistic Style
  author: dumoulin
  journal-title: ICLRE
  year: '2017'
- article-title: Evaluating and Understanding the Robustness of Adversarial Logit
    Pairing
  author: engstrom
  journal-title: NeurIPS SECML
  year: '2018'
- article-title: Detecting Adversarial Samples from Artifacts
  author: feinman
  journal-title: arXiv l703 00410
  year: '2017'
- article-title: Adversarial and Clean Data Are Not Twins
  author: gong
  journal-title: arXiv l704 04960
  year: '2017'
- article-title: 'NIPS 2016 Tutorial: Generative Adversarial Networks'
  author: goodfellow
  journal-title: arXiv l701 00160
  year: '2016'
- article-title: Explaining and Hamessing Adversarial Examples
  author: goodfellow
  journal-title: ICLRE
  year: '2015'
- doi: 10.1109/ICASSP.2013.6638947
- article-title: On the (Statistical) Detection of Adversarial Examples
  author: grosse
  journal-title: arXiv l702 06280
  year: '2017'
- doi: 10.1145/3128572.3140444
- article-title: 'Adversarial Transformation Networks: Learning to Generate Adversarial
    Examples'
  author: baluja
  journal-title: AAAI
  year: '2018'
- doi: 10.1109/SP.2017.49
- article-title: MagNet and &#x201C;Efficient Defenses Against Adversarial Attacks&#x201D;
    are Not Robust to Adversarial Examples
  author: carlini
  journal-title: arXiv l711 08478
  year: '2017'
- article-title: Modulating Early Visual Processing by Language
  author: de vries
  journal-title: NeurIPS
  year: '2017'
- article-title: Learning to Defense by Learning to Attack
  author: chen
  journal-title: arXiv 1811 01213
  year: '2018'
- article-title: Very Deep Convolutional Networks for Large-Scale Image Recognition
  author: simonyan
  journal-title: ICLRE
  year: '2014'
- doi: 10.1109/CVPR.2016.282
- article-title: Improved Techniques for Training GANs
  author: salimans
  journal-title: NeurIPS
  year: '2016'
- article-title: 'U-Net: Convolutional Networks for Biomedical Image Segmentation'
  author: ronneberger
  journal-title: MICCAI
  year: '2015'
- doi: 10.1038/nature16961
- article-title: 'Defense-GAN: Protecting Classifiers Against Adversarial Attacks
    Using Generative Models'
  author: samangouei
  journal-title: ICLRE
  year: '2018'
- doi: 10.1145/3052973.3053009
- article-title: 'Transferability in Machine Learning: from Phenomena to Black-Box
    Attacks using Adversarial Samples'
  author: papemot
  journal-title: arXiv 1605 07277
  year: '2016'
- doi: 10.1109/CVPR.2018.00465
- doi: 10.1109/EuroSP.2016.36
doi: 10.1109/iccv.2019.00283
files:
- jang-adversarial-defense-via-learning-to-generate-diverse-attacks-iccv-2019-paper.pdf
journal: 2019 IEEE/CVF International Conference on Computer Vision (ICCV)
month: 10
publisher: IEEE
ref: AdversarialDefJang2019
tags: ICCV2019 defence
time-added: 2023-03-07-10:57:30
title: Adversarial Defense via Learning to Generate Diverse Attacks
type: inproceedings
url: https://openaccess.thecvf.com/content_ICCV_2019/papers/Jang_Adversarial_Defense_via_Learning_to_Generate_Diverse_Attacks_ICCV_2019_paper.pdf
venue: Seoul, Korea (South)
year: 2019
notes: 'Propose to utilize the generator to learn how to create adversarial
  examples. Unlike the existing approaches that create a one-shot perturbation by
  a deterministic generator, we propose a recursive and stochastic generator that
  produces much stronger and diverse perturbations. experiment results on MNIST
  and CIFAR-10.'
code: 'http://github.com/YunseokJANG/l2l-da'
