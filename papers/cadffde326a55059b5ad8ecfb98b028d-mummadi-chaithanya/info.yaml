abstract: Classifiers such as deep neural networks have been shown to be vulnerable
  against adversarial perturbations on problems with high-dimensional input space.
  While adversarial training improves the robustness of image classifiers against
  such adversarial perturbations, it leaves them sensitive to perturbations on a non-negligible
  fraction of the inputs. In this work, we show that adversarial training is more
  effective in preventing universal perturbations, where the same perturbation needs
  to fool a classifier on many inputs. Moreover, we investigate the trade-off between
  robustness against universal perturbations and performance on unperturbed data and
  propose an extension of adversarial training that handles this trade-off more gracefully.
  We present results for image classification and semantic segmentation to showcase
  that universal perturbations that fool a model hardened with adversarial training
  become clearly perceptible and show patterns of the target scene.
archiveprefix: arXiv
author: Mummadi, Chaithanya Kumar and Brox, Thomas and Metzen, Jan Hendrik
author_list:
- family: Mummadi
  given: Chaithanya Kumar
- family: Brox
  given: Thomas
- family: Metzen
  given: Jan Hendrik
eprint: 1812.03705v2
file: 1812.03705v2.pdf
files:
- tmpxxervn2p.pdf
month: Dec
primaryclass: cs.CV
ref: 1812.03705v2
tags: ICCV2019 defence
time-added: 2023-03-07-12:22:23
title: Defending Against Universal Perturbations With Shared Adversarial Training
type: article
url: http://arxiv.org/abs/1812.03705v2
year: '2018'
notes: 'Show that adversarial training is more effective in preventing
  universal perturbations, where the same perturbation needs to fool a
  classifier on many inputs. Investigate the trade-off between robustness
  against universal perturbations and performance on unperturbed data and
  propose an extension of adversarial training that handles this trade-off more
  gracefully.' 
