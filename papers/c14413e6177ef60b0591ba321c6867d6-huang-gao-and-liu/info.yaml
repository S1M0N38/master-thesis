abstract: 'Recent work has shown that convolutional networks can be substantially
  deeper, more accurate, and efficient to train if they contain shorter connections
  between layers close to the input and those close to the output. In this paper,
  we embrace this observation and introduce the Dense Convolutional Network (DenseNet),
  which connects each layer to every other layer in a feed-forward fashion. Whereas
  traditional convolutional networks with L layers have L connections - one between
  each layer and its subsequent layer - our network has L(L+1)/2 direct connections.
  For each layer, the feature-maps of all preceding layers are used as inputs, and
  its own feature-maps are used as inputs into all subsequent layers. DenseNets have
  several compelling advantages: they alleviate the vanishing-gradient problem, strengthen
  feature propagation, encourage feature reuse, and substantially reduce the number
  of parameters. We evaluate our proposed architecture on four highly competitive
  object recognition benchmark tasks (CIFAR-10, CIFAR-100, SVHN, and ImageNet). DenseNets
  obtain significant improvements over the state-of-the-art on most of them, whilst
  requiring less computation to achieve high performance. Code and pre-trained models
  are available at https://github.com/liuzhuang13/DenseNet .'
archiveprefix: arXiv
author: Huang, Gao and Liu, Zhuang and van der Maaten, Laurens and Weinberger, Kilian
  Q.
author_list:
- family: Huang
  given: Gao
- family: Liu
  given: Zhuang
- family: van der Maaten
  given: Laurens
- family: Weinberger
  given: Kilian Q.
eprint: 1608.06993v5
file: 1608.06993v5.pdf
files:
- tmpx6lpypbx.pdf
month: Aug
papis_id: 18508466a3ab82531b39e078cf1d0f72
primaryclass: cs.CV
ref: DenselyConnectHuang2016
time-added: 2023-08-08-10:03:26
title: Densely Connected Convolutional Networks
type: article
url: http://arxiv.org/abs/1608.06993v5
year: 2016
